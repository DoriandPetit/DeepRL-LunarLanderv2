{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepRL-BaselineZoo-LunarLander-v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJy9QoDC7XA7"
      },
      "source": [
        "# RL Baselines3 Zoo: Training in Colab\n",
        "\n",
        "\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo)\n",
        "\n",
        "Stable-Baselines3 Repo: [https://github.com/DLR-RM/rl-baselines3-zoo](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "# Install Dependencies\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXVDDlTn02M9",
        "outputId": "d666ad99-33b7-4ee6-8037-4634d6a9cc55"
      },
      "source": [
        "!apt-get install swig cmake ffmpeg freeglut3-dev xvfb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "freeglut3-dev is already the newest version (2.8.1-3).\n",
            "freeglut3-dev set to manually installed.\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0 xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 1,885 kB of archives.\n",
            "After this operation, 8,093 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 1,885 kB in 0s (14.7 MB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjF3qRg7oGH"
      },
      "source": [
        "## Clone RL Baselines3 Zoo Repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCjGikdT1DFy",
        "outputId": "021a3a1a-935c-40c3-e943-b8ccd12f5206"
      },
      "source": [
        "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'rl-baselines3-zoo'...\n",
            "remote: Enumerating objects: 3126, done.\u001b[K\n",
            "remote: Counting objects: 100% (186/186), done.\u001b[K\n",
            "remote: Compressing objects: 100% (122/122), done.\u001b[K\n",
            "remote: Total 3126 (delta 116), reused 106 (delta 60), pack-reused 2940\u001b[K\n",
            "Receiving objects: 100% (3126/3126), 2.12 MiB | 4.37 MiB/s, done.\n",
            "Resolving deltas: 100% (2046/2046), done.\n",
            "Submodule 'rl-trained-agents' (https://github.com/DLR-RM/rl-trained-agents) registered for path 'rl-trained-agents'\n",
            "Cloning into '/content/rl-baselines3-zoo/rl-trained-agents'...\n",
            "remote: Enumerating objects: 1567, done.        \n",
            "remote: Counting objects: 100% (152/152), done.        \n",
            "remote: Compressing objects: 100% (102/102), done.        \n",
            "remote: Total 1567 (delta 55), reused 140 (delta 50), pack-reused 1415        \n",
            "Receiving objects: 100% (1567/1567), 1.12 GiB | 27.86 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n",
            "Submodule path 'rl-trained-agents': checked out '3dd2af4cee930750016cf943dc6393bada57b89c'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REMQlh-ezyVt",
        "outputId": "5236fe52-c0e6-48b7-bfa3-56fe0cbf6ddc"
      },
      "source": [
        "%cd /content/rl-baselines3-zoo/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/rl-baselines3-zoo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmD_QTBqTMb"
      },
      "source": [
        "### Install pip dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OWIDzgJTqShY",
        "outputId": "58f46ee2-a6f1-40c3-875f-ac88dc6ef56a"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ-pAbF7zRZ"
      },
      "source": [
        "## Train an RL Agent\n",
        "\n",
        "\n",
        "The train agent can be found in the `logs/` folder.\n",
        "\n",
        "Here we will train QRDQN on LunarLander-v2 environment for 100 000 steps. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bIR_N7R11XI",
        "outputId": "de2c8ef1-3859-40ca-d5f4-a8d5050b3a50"
      },
      "source": [
        "!python train.py --algo qrdqn --env LunarLander-v2 --n-timesteps 100000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== LunarLander-v2 ==========\n",
            "Seed: 598561340\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('batch_size', 128),\n",
            "             ('buffer_size', 100000),\n",
            "             ('exploration_final_eps', 0.18),\n",
            "             ('exploration_fraction', 0.24),\n",
            "             ('gamma', 0.995),\n",
            "             ('gradient_steps', -1),\n",
            "             ('learning_rate', 'lin_1.5e-3'),\n",
            "             ('learning_starts', 10000),\n",
            "             ('n_timesteps', 100000.0),\n",
            "             ('policy', 'MlpPolicy'),\n",
            "             ('policy_kwargs', 'dict(net_arch=[256, 256], n_quantiles=170)'),\n",
            "             ('target_update_interval', 1),\n",
            "             ('train_freq', 256)])\n",
            "Using 1 environments\n",
            "Overwriting n_timesteps with n=100000\n",
            "Creating test environment\n",
            "Using cuda device\n",
            "Log path: logs/qrdqn/LunarLander-v2_1\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.8     |\n",
            "|    ep_rew_mean      | -183     |\n",
            "|    exploration rate | 0.988    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 352      |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total timesteps  | 355      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 87.9     |\n",
            "|    ep_rew_mean      | -163     |\n",
            "|    exploration rate | 0.976    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 352      |\n",
            "|    time_elapsed     | 1        |\n",
            "|    total timesteps  | 703      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 85.9     |\n",
            "|    ep_rew_mean      | -173     |\n",
            "|    exploration rate | 0.965    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 349      |\n",
            "|    time_elapsed     | 2        |\n",
            "|    total timesteps  | 1031     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 84.1     |\n",
            "|    ep_rew_mean      | -171     |\n",
            "|    exploration rate | 0.954    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 348      |\n",
            "|    time_elapsed     | 3        |\n",
            "|    total timesteps  | 1345     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 86.2     |\n",
            "|    ep_rew_mean      | -173     |\n",
            "|    exploration rate | 0.941    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 347      |\n",
            "|    time_elapsed     | 4        |\n",
            "|    total timesteps  | 1724     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 86.9     |\n",
            "|    ep_rew_mean      | -172     |\n",
            "|    exploration rate | 0.929    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 348      |\n",
            "|    time_elapsed     | 5        |\n",
            "|    total timesteps  | 2085     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 86       |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration rate | 0.918    |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 349      |\n",
            "|    time_elapsed     | 6        |\n",
            "|    total timesteps  | 2407     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.3     |\n",
            "|    ep_rew_mean      | -180     |\n",
            "|    exploration rate | 0.903    |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 349      |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total timesteps  | 2827     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.8     |\n",
            "|    ep_rew_mean      | -169     |\n",
            "|    exploration rate | 0.891    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 9        |\n",
            "|    total timesteps  | 3197     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.3     |\n",
            "|    ep_rew_mean      | -168     |\n",
            "|    exploration rate | 0.878    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 10       |\n",
            "|    total timesteps  | 3574     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 88.2     |\n",
            "|    ep_rew_mean      | -171     |\n",
            "|    exploration rate | 0.867    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 11       |\n",
            "|    total timesteps  | 3882     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.4     |\n",
            "|    ep_rew_mean      | -178     |\n",
            "|    exploration rate | 0.853    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 349      |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total timesteps  | 4289     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.3     |\n",
            "|    ep_rew_mean      | -175     |\n",
            "|    exploration rate | 0.84     |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 13       |\n",
            "|    total timesteps  | 4696     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91       |\n",
            "|    ep_rew_mean      | -181     |\n",
            "|    exploration rate | 0.826    |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 14       |\n",
            "|    total timesteps  | 5094     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91       |\n",
            "|    ep_rew_mean      | -177     |\n",
            "|    exploration rate | 0.814    |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 15       |\n",
            "|    total timesteps  | 5458     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.8     |\n",
            "|    ep_rew_mean      | -171     |\n",
            "|    exploration rate | 0.801    |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 16       |\n",
            "|    total timesteps  | 5814     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.2     |\n",
            "|    ep_rew_mean      | -168     |\n",
            "|    exploration rate | 0.79     |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 17       |\n",
            "|    total timesteps  | 6133     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91       |\n",
            "|    ep_rew_mean      | -166     |\n",
            "|    exploration rate | 0.776    |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 18       |\n",
            "|    total timesteps  | 6554     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.5     |\n",
            "|    ep_rew_mean      | -168     |\n",
            "|    exploration rate | 0.765    |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 19       |\n",
            "|    total timesteps  | 6881     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.5     |\n",
            "|    ep_rew_mean      | -164     |\n",
            "|    exploration rate | 0.753    |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 20       |\n",
            "|    total timesteps  | 7236     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.8     |\n",
            "|    ep_rew_mean      | -162     |\n",
            "|    exploration rate | 0.739    |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 21       |\n",
            "|    total timesteps  | 7628     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.3     |\n",
            "|    ep_rew_mean      | -162     |\n",
            "|    exploration rate | 0.729    |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 22       |\n",
            "|    total timesteps  | 7943     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.7     |\n",
            "|    ep_rew_mean      | -161     |\n",
            "|    exploration rate | 0.715    |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 23       |\n",
            "|    total timesteps  | 8348     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.5     |\n",
            "|    ep_rew_mean      | -164     |\n",
            "|    exploration rate | 0.703    |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total timesteps  | 8686     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.1     |\n",
            "|    ep_rew_mean      | -163     |\n",
            "|    exploration rate | 0.692    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 25       |\n",
            "|    total timesteps  | 9010     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.1     |\n",
            "|    ep_rew_mean      | -163     |\n",
            "|    exploration rate | 0.68     |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 26       |\n",
            "|    total timesteps  | 9362     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 89.8     |\n",
            "|    ep_rew_mean      | -167     |\n",
            "|    exploration rate | 0.669    |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 350      |\n",
            "|    time_elapsed     | 27       |\n",
            "|    total timesteps  | 9683     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-665.95 +/- 225.91\n",
            "Episode length: 94.00 +/- 10.68\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 94       |\n",
            "|    mean_reward      | -666     |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.658    |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 10000    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 90.1     |\n",
            "|    ep_rew_mean      | -166     |\n",
            "|    exploration rate | 0.657    |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 344      |\n",
            "|    time_elapsed     | 29       |\n",
            "|    total timesteps  | 10045    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.7     |\n",
            "|    ep_rew_mean      | -163     |\n",
            "|    exploration rate | 0.637    |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 299      |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total timesteps  | 10611    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00134  |\n",
            "|    loss             | 163      |\n",
            "|    n_updates        | 512      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.2     |\n",
            "|    ep_rew_mean      | -159     |\n",
            "|    exploration rate | 0.619    |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 268      |\n",
            "|    time_elapsed     | 41       |\n",
            "|    total timesteps  | 11143    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00133  |\n",
            "|    loss             | 228      |\n",
            "|    n_updates        | 1024     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.4     |\n",
            "|    ep_rew_mean      | -158     |\n",
            "|    exploration rate | 0.603    |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 244      |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total timesteps  | 11625    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00133  |\n",
            "|    loss             | 309      |\n",
            "|    n_updates        | 1536     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.3     |\n",
            "|    ep_rew_mean      | -150     |\n",
            "|    exploration rate | 0.579    |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 219      |\n",
            "|    time_elapsed     | 56       |\n",
            "|    total timesteps  | 12333    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00132  |\n",
            "|    loss             | 358      |\n",
            "|    n_updates        | 2304     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -144     |\n",
            "|    exploration rate | 0.562    |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 62       |\n",
            "|    total timesteps  | 12829    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00131  |\n",
            "|    loss             | 389      |\n",
            "|    n_updates        | 2816     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration rate | 0.54     |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 195      |\n",
            "|    time_elapsed     | 68       |\n",
            "|    total timesteps  | 13475    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0013   |\n",
            "|    loss             | 409      |\n",
            "|    n_updates        | 3328     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -136     |\n",
            "|    exploration rate | 0.508    |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 80       |\n",
            "|    total timesteps  | 14390    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00128  |\n",
            "|    loss             | 464      |\n",
            "|    n_updates        | 4352     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 114      |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration rate | 0.476    |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 90       |\n",
            "|    total timesteps  | 15322    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00127  |\n",
            "|    loss             | 517      |\n",
            "|    n_updates        | 5120     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 122      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration rate | 0.438    |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 155      |\n",
            "|    time_elapsed     | 105      |\n",
            "|    total timesteps  | 16445    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00125  |\n",
            "|    loss             | 560      |\n",
            "|    n_updates        | 6400     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 126      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration rate | 0.41     |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 149      |\n",
            "|    time_elapsed     | 115      |\n",
            "|    total timesteps  | 17264    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00124  |\n",
            "|    loss             | 543      |\n",
            "|    n_updates        | 7168     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 131      |\n",
            "|    ep_rew_mean      | -111     |\n",
            "|    exploration rate | 0.379    |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 145      |\n",
            "|    time_elapsed     | 125      |\n",
            "|    total timesteps  | 18162    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00123  |\n",
            "|    loss             | 536      |\n",
            "|    n_updates        | 7936     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 134      |\n",
            "|    ep_rew_mean      | -108     |\n",
            "|    exploration rate | 0.354    |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 140      |\n",
            "|    time_elapsed     | 134      |\n",
            "|    total timesteps  | 18895    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00122  |\n",
            "|    loss             | 524      |\n",
            "|    n_updates        | 8704     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 136      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration rate | 0.338    |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 138      |\n",
            "|    time_elapsed     | 140      |\n",
            "|    total timesteps  | 19381    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00121  |\n",
            "|    loss             | 532      |\n",
            "|    n_updates        | 9216     |\n",
            "----------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=88.21 +/- 76.36\n",
            "Episode length: 898.00 +/- 122.51\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 898      |\n",
            "|    mean_reward      | 88.2     |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.317    |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 20000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0012   |\n",
            "|    loss             | 516      |\n",
            "|    n_updates        | 9984     |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 156      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration rate | 0.256    |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 118      |\n",
            "|    time_elapsed     | 183      |\n",
            "|    total timesteps  | 21783    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00117  |\n",
            "|    loss             | 581      |\n",
            "|    n_updates        | 11776    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 184      |\n",
            "|    ep_rew_mean      | -95.6    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 110      |\n",
            "|    time_elapsed     | 225      |\n",
            "|    total timesteps  | 24913    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00113  |\n",
            "|    loss             | 566      |\n",
            "|    n_updates        | 14848    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 220      |\n",
            "|    ep_rew_mean      | -88.8    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 103      |\n",
            "|    time_elapsed     | 278      |\n",
            "|    total timesteps  | 28913    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00107  |\n",
            "|    loss             | 553      |\n",
            "|    n_updates        | 18688    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=-22.01 +/- 28.21\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | -22      |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 30000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00105  |\n",
            "|    loss             | 536      |\n",
            "|    n_updates        | 19968    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 257      |\n",
            "|    ep_rew_mean      | -85.9    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 94       |\n",
            "|    time_elapsed     | 348      |\n",
            "|    total timesteps  | 32913    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00101  |\n",
            "|    loss             | 500      |\n",
            "|    n_updates        | 22784    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 284      |\n",
            "|    ep_rew_mean      | -79.5    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 92       |\n",
            "|    time_elapsed     | 389      |\n",
            "|    total timesteps  | 36047    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000962 |\n",
            "|    loss             | 456      |\n",
            "|    n_updates        | 25856    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 313      |\n",
            "|    ep_rew_mean      | -73      |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 90       |\n",
            "|    time_elapsed     | 434      |\n",
            "|    total timesteps  | 39248    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000912 |\n",
            "|    loss             | 433      |\n",
            "|    n_updates        | 29184    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=13.86 +/- 5.73\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 1e+03    |\n",
            "|    mean_reward      | 13.9     |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 40000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000901 |\n",
            "|    loss             | 421      |\n",
            "|    n_updates        | 29952    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 349      |\n",
            "|    ep_rew_mean      | -66.3    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 86       |\n",
            "|    time_elapsed     | 499      |\n",
            "|    total timesteps  | 43248    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000855 |\n",
            "|    loss             | 437      |\n",
            "|    n_updates        | 33024    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 374      |\n",
            "|    ep_rew_mean      | -52.5    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 85       |\n",
            "|    time_elapsed     | 535      |\n",
            "|    total timesteps  | 46040    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000813 |\n",
            "|    loss             | 437      |\n",
            "|    n_updates        | 35840    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 395      |\n",
            "|    ep_rew_mean      | -45.8    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 85       |\n",
            "|    time_elapsed     | 569      |\n",
            "|    total timesteps  | 48531    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000774 |\n",
            "|    loss             | 451      |\n",
            "|    n_updates        | 38400    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=218.23 +/- 52.81\n",
            "Episode length: 556.00 +/- 238.16\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 556      |\n",
            "|    mean_reward      | 218      |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 50000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000751 |\n",
            "|    loss             | 440      |\n",
            "|    n_updates        | 39936    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 423      |\n",
            "|    ep_rew_mean      | -34.1    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 615      |\n",
            "|    total timesteps  | 51636    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000728 |\n",
            "|    loss             | 485      |\n",
            "|    n_updates        | 41472    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 442      |\n",
            "|    ep_rew_mean      | -24.5    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 644      |\n",
            "|    total timesteps  | 53881    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000694 |\n",
            "|    loss             | 482      |\n",
            "|    n_updates        | 43776    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 471      |\n",
            "|    ep_rew_mean      | -10.7    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 686      |\n",
            "|    total timesteps  | 57127    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000644 |\n",
            "|    loss             | 478      |\n",
            "|    n_updates        | 47104    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 492      |\n",
            "|    ep_rew_mean      | 0.403    |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 83       |\n",
            "|    time_elapsed     | 719      |\n",
            "|    total timesteps  | 59765    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000605 |\n",
            "|    loss             | 494      |\n",
            "|    n_updates        | 49664    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=229.34 +/- 26.34\n",
            "Episode length: 574.60 +/- 218.41\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 575      |\n",
            "|    mean_reward      | 229      |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 60000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000601 |\n",
            "|    loss             | 473      |\n",
            "|    n_updates        | 49920    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 508      |\n",
            "|    ep_rew_mean      | 9.26     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 82       |\n",
            "|    time_elapsed     | 751      |\n",
            "|    total timesteps  | 61920    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000575 |\n",
            "|    loss             | 457      |\n",
            "|    n_updates        | 51712    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 538      |\n",
            "|    ep_rew_mean      | 22.8     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 82       |\n",
            "|    time_elapsed     | 796      |\n",
            "|    total timesteps  | 65383    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000521 |\n",
            "|    loss             | 437      |\n",
            "|    n_updates        | 55296    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 557      |\n",
            "|    ep_rew_mean      | 33.2     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 228      |\n",
            "|    fps              | 82       |\n",
            "|    time_elapsed     | 829      |\n",
            "|    total timesteps  | 68015    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000482 |\n",
            "|    loss             | 427      |\n",
            "|    n_updates        | 57856    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=244.64 +/- 53.53\n",
            "Episode length: 386.20 +/- 307.09\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 386      |\n",
            "|    mean_reward      | 245      |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 70000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000452 |\n",
            "|    loss             | 403      |\n",
            "|    n_updates        | 59904    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 578      |\n",
            "|    ep_rew_mean      | 36.6     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 232      |\n",
            "|    fps              | 81       |\n",
            "|    time_elapsed     | 865      |\n",
            "|    total timesteps  | 70611    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000444 |\n",
            "|    loss             | 397      |\n",
            "|    n_updates        | 60416    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 579      |\n",
            "|    ep_rew_mean      | 41.9     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 236      |\n",
            "|    fps              | 81       |\n",
            "|    time_elapsed     | 874      |\n",
            "|    total timesteps  | 71349    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000432 |\n",
            "|    loss             | 393      |\n",
            "|    n_updates        | 61184    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 598      |\n",
            "|    ep_rew_mean      | 50.9     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 81       |\n",
            "|    time_elapsed     | 912      |\n",
            "|    total timesteps  | 74234    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00039  |\n",
            "|    loss             | 388      |\n",
            "|    n_updates        | 64000    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 613      |\n",
            "|    ep_rew_mean      | 60.4     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 244      |\n",
            "|    fps              | 81       |\n",
            "|    time_elapsed     | 944      |\n",
            "|    total timesteps  | 76604    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000352 |\n",
            "|    loss             | 361      |\n",
            "|    n_updates        | 66560    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 625      |\n",
            "|    ep_rew_mean      | 69.9     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 248      |\n",
            "|    fps              | 81       |\n",
            "|    time_elapsed     | 973      |\n",
            "|    total timesteps  | 78928    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000317 |\n",
            "|    loss             | 357      |\n",
            "|    n_updates        | 68864    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=240.71 +/- 50.98\n",
            "Episode length: 380.60 +/- 311.58\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 381      |\n",
            "|    mean_reward      | 241      |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 80000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000302 |\n",
            "|    loss             | 360      |\n",
            "|    n_updates        | 69888    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 635      |\n",
            "|    ep_rew_mean      | 78.6     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 252      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 998      |\n",
            "|    total timesteps  | 80785    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00029  |\n",
            "|    loss             | 356      |\n",
            "|    n_updates        | 70656    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 646      |\n",
            "|    ep_rew_mean      | 88.4     |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 256      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1023     |\n",
            "|    total timesteps  | 82768    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.00026  |\n",
            "|    loss             | 373      |\n",
            "|    n_updates        | 72704    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 658      |\n",
            "|    ep_rew_mean      | 99       |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 260      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1046     |\n",
            "|    total timesteps  | 84713    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000233 |\n",
            "|    loss             | 355      |\n",
            "|    n_updates        | 74496    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 675      |\n",
            "|    ep_rew_mean      | 109      |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 264      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1075     |\n",
            "|    total timesteps  | 86862    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000198 |\n",
            "|    loss             | 371      |\n",
            "|    n_updates        | 76800    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 672      |\n",
            "|    ep_rew_mean      | 117      |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 268      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1101     |\n",
            "|    total timesteps  | 88945    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000168 |\n",
            "|    loss             | 357      |\n",
            "|    n_updates        | 78848    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=204.59 +/- 47.68\n",
            "Episode length: 456.00 +/- 189.21\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 456      |\n",
            "|    mean_reward      | 205      |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 90000    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000152 |\n",
            "|    loss             | 360      |\n",
            "|    n_updates        | 79872    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 655      |\n",
            "|    ep_rew_mean      | 127      |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 272      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1124     |\n",
            "|    total timesteps  | 90374    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000144 |\n",
            "|    loss             | 359      |\n",
            "|    n_updates        | 80384    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 642      |\n",
            "|    ep_rew_mean      | 135      |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 276      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1157     |\n",
            "|    total timesteps  | 93115    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.000106 |\n",
            "|    loss             | 346      |\n",
            "|    n_updates        | 82944    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 618      |\n",
            "|    ep_rew_mean      | 141      |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 280      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1179     |\n",
            "|    total timesteps  | 94724    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 7.92e-05 |\n",
            "|    loss             | 346      |\n",
            "|    n_updates        | 84736    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 616      |\n",
            "|    ep_rew_mean      | 145      |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    episodes         | 284      |\n",
            "|    fps              | 80       |\n",
            "|    time_elapsed     | 1215     |\n",
            "|    total timesteps  | 97650    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 3.7e-05  |\n",
            "|    loss             | 349      |\n",
            "|    n_updates        | 87552    |\n",
            "----------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=254.03 +/- 21.41\n",
            "Episode length: 303.20 +/- 134.27\n",
            "----------------------------------\n",
            "| eval/               |          |\n",
            "|    mean_ep_length   | 303      |\n",
            "|    mean_reward      | 254      |\n",
            "| rollout/            |          |\n",
            "|    exploration rate | 0.18     |\n",
            "| time/               |          |\n",
            "|    total timesteps  | 100000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 2.4e-06  |\n",
            "|    loss             | 344      |\n",
            "|    n_updates        | 89856    |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "Saving to logs/qrdqn/LunarLander-v2_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fHBq73665yD"
      },
      "source": [
        "#### On PPO\n",
        "\n",
        "\n",
        "We can now train it on PPO algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw8YuEgU6bT3",
        "outputId": "87eb38ee-fb6e-4212-b58a-78b639972e67"
      },
      "source": [
        "!python train.py --algo ppo --env LunarLander-v2 --n-timesteps 100 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== LunarLander-v2 ==========\n",
            "Seed: 2967772580\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('batch_size', 64),\n",
            "             ('ent_coef', 0.01),\n",
            "             ('gae_lambda', 0.98),\n",
            "             ('gamma', 0.999),\n",
            "             ('n_envs', 16),\n",
            "             ('n_epochs', 4),\n",
            "             ('n_steps', 1024),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=100\n",
            "Creating test environment\n",
            "Using cuda device\n",
            "Log path: logs/ppo/LunarLander-v2_1\n",
            "Eval num_timesteps=10000, episode_reward=-765.83 +/- 448.75\n",
            "Episode length: 97.40 +/- 54.21\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.4     |\n",
            "|    mean_reward     | -766     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 10000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 90.1     |\n",
            "|    ep_rew_mean     | -167     |\n",
            "| time/              |          |\n",
            "|    fps             | 2506     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Saving to logs/ppo/LunarLander-v2_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Il2J0VHPLC"
      },
      "source": [
        "#### Tune Hyperparameters\n",
        "\n",
        "We use [Optuna](https://optuna.org/) for optimizing the hyperparameters.\n",
        "\n",
        "Tune the hyperparameters for PPO, using a tpe sampler and median pruner, 2 parallels jobs,\n",
        "with a budget of 1000 trials and a maximum of 50000 steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9NK48TkcS5j",
        "outputId": "28369ec3-91fb-4106-c399-2e1c2f832a40"
      },
      "source": [
        "!python train.py --algo ppo --env LunarLander-v2 -n 1000 -optimize --n-trials 500 --sampler tpe --pruner median"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== LunarLander-v2 ==========\n",
            "Seed: 335732516\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('batch_size', 64),\n",
            "             ('ent_coef', 0.01),\n",
            "             ('gae_lambda', 0.98),\n",
            "             ('gamma', 0.999),\n",
            "             ('n_envs', 16),\n",
            "             ('n_epochs', 4),\n",
            "             ('n_steps', 1024),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=50000\n",
            "Optimizing hyperparameters\n",
            "Sampler: tpe - Pruner: median\n",
            "\u001b[32m[I 2021-10-21 09:54:08,346]\u001b[0m A new study created in memory with name: no-name-d01fd325-e0e5-46a7-a68e-bacb3723ce46\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/optuna/study/study.py:397: FutureWarning:\n",
            "\n",
            "`n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.\n",
            "\n",
            "\u001b[32m[I 2021-10-21 09:56:00,309]\u001b[0m Trial 0 finished with value: -950.7362820000001 and parameters: {'batch_size': 512, 'n_steps': 512, 'gamma': 0.99, 'learning_rate': 4.34431713042486e-05, 'ent_coef': 2.338975047806776e-06, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 0.3, 'vf_coef': 0.7121600358300353, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 0 with value: -950.7362820000001.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 09:57:41,422]\u001b[0m Trial 2 finished with value: -494.1357522 and parameters: {'batch_size': 128, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.17977064006132482, 'ent_coef': 5.0430992594038865e-06, 'clip_range': 0.3, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 1, 'vf_coef': 0.8641454826724978, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 2 with value: -494.1357522.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:03:23,128]\u001b[0m Trial 3 finished with value: -214.1509204 and parameters: {'batch_size': 64, 'n_steps': 1024, 'gamma': 0.9, 'learning_rate': 0.003976432897865545, 'ent_coef': 4.034216735585758e-06, 'clip_range': 0.4, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.7, 'vf_coef': 0.062133071440617305, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 3 with value: -214.1509204.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:16:11,452]\u001b[0m Trial 4 finished with value: -1311.4250337999997 and parameters: {'batch_size': 64, 'n_steps': 16, 'gamma': 0.995, 'learning_rate': 0.031733767342803254, 'ent_coef': 1.6179777404810573e-06, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.7, 'vf_coef': 0.6218690418413247, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 3 with value: -214.1509204.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:19:22,821]\u001b[0m Trial 5 finished with value: -144.5532776 and parameters: {'batch_size': 8, 'n_steps': 16, 'gamma': 0.9999, 'learning_rate': 0.013803868585266875, 'ent_coef': 6.851222877134235e-06, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 0.95, 'max_grad_norm': 0.8, 'vf_coef': 0.4552206569695888, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 5 with value: -144.5532776.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:21:46,015]\u001b[0m Trial 1 finished with value: -491.13345439999995 and parameters: {'batch_size': 256, 'n_steps': 8, 'gamma': 0.995, 'learning_rate': 0.05085233283951726, 'ent_coef': 0.03500710775391405, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.92, 'max_grad_norm': 0.6, 'vf_coef': 0.9776510932995859, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 5 with value: -144.5532776.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:38:23,743]\u001b[0m Trial 6 finished with value: 116.0603724 and parameters: {'batch_size': 16, 'n_steps': 16, 'gamma': 0.98, 'learning_rate': 0.00118888299210859, 'ent_coef': 1.2884739205066152e-05, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.92, 'max_grad_norm': 2, 'vf_coef': 0.8192472984170593, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:51:28,419]\u001b[0m Trial 8 finished with value: -197.73568 and parameters: {'batch_size': 128, 'n_steps': 32, 'gamma': 0.9, 'learning_rate': 0.00016295805319408265, 'ent_coef': 2.0409912554594895e-05, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.9, 'max_grad_norm': 5, 'vf_coef': 0.9038166230626546, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:52:54,297]\u001b[0m Trial 7 finished with value: -719.1793913999999 and parameters: {'batch_size': 512, 'n_steps': 8, 'gamma': 0.95, 'learning_rate': 0.4268830436389958, 'ent_coef': 0.0002430017208528067, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.98, 'max_grad_norm': 0.9, 'vf_coef': 0.45119764724583855, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:55:13,350]\u001b[0m Trial 10 finished with value: -898.68527 and parameters: {'batch_size': 32, 'n_steps': 128, 'gamma': 0.9, 'learning_rate': 2.473017755608473e-05, 'ent_coef': 0.012293625973743918, 'clip_range': 0.3, 'n_epochs': 1, 'gae_lambda': 1.0, 'max_grad_norm': 5, 'vf_coef': 0.5148720405803429, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 10:56:26,511]\u001b[0m Trial 9 finished with value: -1148.7134228 and parameters: {'batch_size': 256, 'n_steps': 1024, 'gamma': 0.995, 'learning_rate': 0.0016669828828766707, 'ent_coef': 1.2030798759650032e-07, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.95, 'max_grad_norm': 0.8, 'vf_coef': 0.843500614935221, 'net_arch': 'medium', 'activation_fn': 'tanh'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 11:04:29,567]\u001b[0m Trial 12 finished with value: -134.86281379999997 and parameters: {'batch_size': 8, 'n_steps': 16, 'gamma': 0.98, 'learning_rate': 0.00219987019355935, 'ent_coef': 0.0004059162779093675, 'clip_range': 0.2, 'n_epochs': 1, 'gae_lambda': 0.92, 'max_grad_norm': 2, 'vf_coef': 0.248409994837835, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 11:20:22,622]\u001b[0m Trial 13 finished with value: -89.414642 and parameters: {'batch_size': 16, 'n_steps': 16, 'gamma': 0.98, 'learning_rate': 0.000589255855206797, 'ent_coef': 0.001466619332958923, 'clip_range': 0.2, 'n_epochs': 5, 'gae_lambda': 0.92, 'max_grad_norm': 2, 'vf_coef': 0.16059328896817726, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 11:22:41,456]\u001b[0m Trial 14 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 11:23:02,444]\u001b[0m Trial 15 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 11:31:04,587]\u001b[0m Trial 11 finished with value: -27.988211999999997 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 0.0017344310335852253, 'ent_coef': 2.600486010278297e-08, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.92, 'max_grad_norm': 2, 'vf_coef': 0.23486986266611787, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 11:56:03,428]\u001b[0m Trial 16 finished with value: -128.5286708 and parameters: {'batch_size': 16, 'n_steps': 16, 'gamma': 0.98, 'learning_rate': 0.00010118554267194095, 'ent_coef': 3.9727813006530644e-08, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.92, 'max_grad_norm': 0.5, 'vf_coef': 0.24012983854998987, 'net_arch': 'small', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 12:02:55,146]\u001b[0m Trial 17 finished with value: -99.38846219999999 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.98, 'learning_rate': 0.007253853864679125, 'ent_coef': 3.196815773503159e-08, 'clip_range': 0.2, 'n_epochs': 20, 'gae_lambda': 0.8, 'max_grad_norm': 0.5, 'vf_coef': 0.335292685853797, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 12:07:00,190]\u001b[0m Trial 19 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 12:24:44,116]\u001b[0m Trial 18 finished with value: -202.6523842 and parameters: {'batch_size': 16, 'n_steps': 64, 'gamma': 0.9999, 'learning_rate': 0.007372288594135868, 'ent_coef': 1.0233214442521873e-08, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.8, 'max_grad_norm': 2, 'vf_coef': 0.35318257615832915, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 12:33:25,039]\u001b[0m Trial 20 finished with value: 66.42081060000001 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.0010437340413789313, 'ent_coef': 4.0449614435152375e-07, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.37357530781393333, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 12:52:51,405]\u001b[0m Trial 21 finished with value: -121.8229188 and parameters: {'batch_size': 16, 'n_steps': 256, 'gamma': 0.999, 'learning_rate': 0.0011470011037477332, 'ent_coef': 4.5861185542986695e-07, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 1.0, 'max_grad_norm': 1, 'vf_coef': 0.7670489569533423, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 6 with value: 116.0603724.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:01:13,212]\u001b[0m Trial 22 finished with value: 187.9654714 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.0011760764552485696, 'ent_coef': 5.235101495608891e-07, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.5731914405853845, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:03:16,139]\u001b[0m Trial 23 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:22:03,673]\u001b[0m Trial 25 finished with value: 68.7555418 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.000846422691629936, 'ent_coef': 3.6072492210844426e-05, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.6234553647668891, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:33:38,774]\u001b[0m Trial 24 finished with value: -38.092148200000004 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.00021717707184246436, 'ent_coef': 3.406756656911139e-05, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.5561050726172742, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:35:55,494]\u001b[0m Trial 27 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:38:07,724]\u001b[0m Trial 28 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:39:07,252]\u001b[0m Trial 26 finished with value: -196.3685026 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.00524490115048375, 'ent_coef': 4.144095206314434e-05, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.5931251235007868, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:39:28,835]\u001b[0m Trial 29 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:40:16,963]\u001b[0m Trial 31 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 13:43:17,774]\u001b[0m Trial 30 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:08:01,935]\u001b[0m Trial 32 finished with value: 125.469534 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.0010781253758230709, 'ent_coef': 1.5489974475807984e-07, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.3953201493239612, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:12:08,008]\u001b[0m Trial 33 finished with value: -85.3986982 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.0010450160798750805, 'ent_coef': 1.7096890211463438e-07, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.9962926253617576, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:12:27,699]\u001b[0m Trial 35 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:25:25,665]\u001b[0m Trial 36 finished with value: -65.2057126 and parameters: {'batch_size': 512, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.0002444866782453087, 'ent_coef': 7.152703019521286e-08, 'clip_range': 0.4, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.922929950265474, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:27:41,115]\u001b[0m Trial 34 finished with value: 129.9616618 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.00048646963884171884, 'ent_coef': 1.52961169449245e-07, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.9474755980645296, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:34:53,440]\u001b[0m Trial 37 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:36:03,243]\u001b[0m Trial 38 finished with value: -44.6697056 and parameters: {'batch_size': 64, 'n_steps': 8, 'gamma': 0.9999, 'learning_rate': 0.002487790176274022, 'ent_coef': 8.864830515672358e-07, 'clip_range': 0.4, 'n_epochs': 1, 'gae_lambda': 0.98, 'max_grad_norm': 0.7, 'vf_coef': 0.8655962554373112, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:36:36,196]\u001b[0m Trial 40 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:37:43,012]\u001b[0m Trial 39 finished with value: -110.19363079999998 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.995, 'learning_rate': 0.11036540998369558, 'ent_coef': 1.0272177897549612e-06, 'clip_range': 0.4, 'n_epochs': 5, 'gae_lambda': 0.98, 'max_grad_norm': 1, 'vf_coef': 0.9342832551915387, 'net_arch': 'small', 'activation_fn': 'tanh'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:44:40,791]\u001b[0m Trial 41 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:57:11,427]\u001b[0m Trial 42 finished with value: 167.2170838 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.0007431147646660476, 'ent_coef': 1.610020922573494e-07, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 0.9, 'vf_coef': 0.4112754690885126, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 14:57:34,291]\u001b[0m Trial 44 pruned. \u001b[0m\n",
            "\u001b[32m[I 2021-10-21 15:01:49,073]\u001b[0m Trial 43 finished with value: -55.45027280000001 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.0006286227452188783, 'ent_coef': 3.0570198396581384e-06, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.99, 'max_grad_norm': 0.9, 'vf_coef': 0.6298290887526348, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 22 with value: 187.9654714.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 15:18:03,665]\u001b[0m Trial 45 finished with value: 195.9081564 and parameters: {'batch_size': 16, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 0.00011596933243196016, 'ent_coef': 2.9352370524833907e-06, 'clip_range': 0.1, 'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 5, 'vf_coef': 0.4964941214692943, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 45 with value: 195.9081564.\u001b[0m\n",
            "\u001b[32m[I 2021-10-21 15:34:13,628]\u001b[0m Trial 46 finished with value: 158.1820456 and parameters: {'batch_size': 8, 'n_steps': 32, 'gamma': 0.999, 'learning_rate': 9.624891633652073e-05, 'ent_coef': 5.643090560533325e-08, 'clip_range': 0.3, 'n_epochs': 10, 'gae_lambda': 0.95, 'max_grad_norm': 5, 'vf_coef': 0.4920730693822362, 'net_arch': 'medium', 'activation_fn': 'relu'}. Best is trial 45 with value: 195.9081564.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Record  a Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "source": [
        "# Set up display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip3AauLzwNGP",
        "outputId": "ad462416-98b7-4374-b91b-447056e179a9"
      },
      "source": [
        "!python -m utils.record_video --algo ppo --env LunarLander-v2 --exp-id 0 -f logs/ -n 1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading latest experiment, id=4\n",
            "Saving video to /content/rl-baselines3-zoo/logs/ppo/LunarLander-v2_4/videos/final-model-ppo-LunarLander-v2-step-0-to-step-1000.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBuUfnzI8DN6"
      },
      "source": [
        "### Display the video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHu2bDOEx4JL",
        "outputId": "1c8167f6-fa4b-4a8f-c965-22e8eb0b135b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC3OTfpf8CXu"
      },
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  \"\"\"\n",
        "  Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "  :param video_path: (str) Path to the folder containing videos\n",
        "  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "  \"\"\"\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oKOjFuwK9HI0",
        "outputId": "13873af4-63f7-4a0e-8e71-39a3036eb45b"
      },
      "source": [
        "show_videos(video_path='logs/qrdqn/videos/', prefix='final')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjdpP0HE8D2p"
      },
      "source": [
        "### Continue Training\n",
        "\n",
        "Here, we will continue training of the previous model\n",
        "\n",
        "Following is a list of re-training and evaluating the models. There is not much to say, as we already made comments on the associated report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgMZQJJF6u1C",
        "outputId": "a462ea55-fe98-4775-b994-49e54e73f4df"
      },
      "source": [
        "!python train.py --algo ppo --env LunarLander-v2 -n 1000000 -i logs/ppo/LunarLander-v2_5/LunarLander-v2#ppo#nom1_newTEST.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== LunarLander-v2 ==========\n",
            "Seed: 46406515\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('batch_size', 64),\n",
            "             ('ent_coef', 0.01),\n",
            "             ('gae_lambda', 0.98),\n",
            "             ('gamma', 0.999),\n",
            "             ('n_envs', 16),\n",
            "             ('n_epochs', 4),\n",
            "             ('n_steps', 1024),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=1000000\n",
            "Creating test environment\n",
            "Loading pretrained agent\n",
            "Log path: logs/ppo/LunarLander-v2_6\n",
            "Eval num_timesteps=10000, episode_reward=282.07 +/- 17.91\n",
            "Episode length: 250.60 +/- 10.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 251      |\n",
            "|    mean_reward     | 282      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 10000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 232      |\n",
            "|    ep_rew_mean     | 269      |\n",
            "| time/              |          |\n",
            "|    fps             | 2278     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 7        |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=271.03 +/- 18.29\n",
            "Episode length: 244.80 +/- 19.09\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 245          |\n",
            "|    mean_reward          | 271          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 20000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038283332 |\n",
            "|    clip_fraction        | 0.0268       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.653       |\n",
            "|    explained_variance   | 0.935        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 175          |\n",
            "|    n_updates            | 556          |\n",
            "|    policy_gradient_loss | 6.9e-05      |\n",
            "|    value_loss           | 194          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=291.34 +/- 17.25\n",
            "Episode length: 242.00 +/- 12.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 242      |\n",
            "|    mean_reward     | 291      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 30000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 232      |\n",
            "|    ep_rew_mean     | 270      |\n",
            "| time/              |          |\n",
            "|    fps             | 1737     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 18       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=299.69 +/- 15.42\n",
            "Episode length: 239.20 +/- 29.23\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 239         |\n",
            "|    mean_reward          | 300         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 40000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004572779 |\n",
            "|    clip_fraction        | 0.041       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.631      |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.27        |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.000468   |\n",
            "|    value_loss           | 150         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 246      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1685     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 29       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=264.43 +/- 22.40\n",
            "Episode length: 237.60 +/- 8.96\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 238          |\n",
            "|    mean_reward          | 264          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035429425 |\n",
            "|    clip_fraction        | 0.0339       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.641       |\n",
            "|    explained_variance   | 0.954        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.94         |\n",
            "|    n_updates            | 564          |\n",
            "|    policy_gradient_loss | 0.000175     |\n",
            "|    value_loss           | 151          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=268.03 +/- 15.96\n",
            "Episode length: 237.40 +/- 18.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | 268      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 60000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 228      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1626     |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 40       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=265.33 +/- 18.52\n",
            "Episode length: 222.40 +/- 4.08\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 222          |\n",
            "|    mean_reward          | 265          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 70000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035900201 |\n",
            "|    clip_fraction        | 0.0434       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.618       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 133          |\n",
            "|    n_updates            | 568          |\n",
            "|    policy_gradient_loss | -0.000154    |\n",
            "|    value_loss           | 107          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=292.41 +/- 14.30\n",
            "Episode length: 243.20 +/- 26.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 243      |\n",
            "|    mean_reward     | 292      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 80000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | 267      |\n",
            "| time/              |          |\n",
            "|    fps             | 1596     |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 51       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=270.79 +/- 11.00\n",
            "Episode length: 241.40 +/- 16.78\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 241          |\n",
            "|    mean_reward          | 271          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 90000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036045879 |\n",
            "|    clip_fraction        | 0.0431       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.629       |\n",
            "|    explained_variance   | 0.955        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 93.7         |\n",
            "|    n_updates            | 572          |\n",
            "|    policy_gradient_loss | 0.000147     |\n",
            "|    value_loss           | 136          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 224      |\n",
            "|    ep_rew_mean     | 271      |\n",
            "| time/              |          |\n",
            "|    fps             | 1583     |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 62       |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=285.11 +/- 14.57\n",
            "Episode length: 220.80 +/- 16.47\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 221          |\n",
            "|    mean_reward          | 285          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 100000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041577145 |\n",
            "|    clip_fraction        | 0.059        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.644       |\n",
            "|    explained_variance   | 0.962        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.85         |\n",
            "|    n_updates            | 576          |\n",
            "|    policy_gradient_loss | 0.000758     |\n",
            "|    value_loss           | 67.6         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=294.56 +/- 13.03\n",
            "Episode length: 226.20 +/- 12.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | 295      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 110000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 246      |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 1557     |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 73       |\n",
            "|    total_timesteps | 114688   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=286.09 +/- 10.68\n",
            "Episode length: 231.80 +/- 16.65\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 232          |\n",
            "|    mean_reward          | 286          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 120000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032041715 |\n",
            "|    clip_fraction        | 0.0416       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.607       |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.38         |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | -0.000304    |\n",
            "|    value_loss           | 63.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=279.21 +/- 9.24\n",
            "Episode length: 224.00 +/- 15.11\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 224      |\n",
            "|    mean_reward     | 279      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 130000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 228      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1550     |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 84       |\n",
            "|    total_timesteps | 131072   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=270.30 +/- 19.51\n",
            "Episode length: 222.80 +/- 8.61\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 223          |\n",
            "|    mean_reward          | 270          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 140000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045454474 |\n",
            "|    clip_fraction        | 0.0563       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.613       |\n",
            "|    explained_variance   | 0.966        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.8         |\n",
            "|    n_updates            | 584          |\n",
            "|    policy_gradient_loss | 0.000438     |\n",
            "|    value_loss           | 90.8         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 224      |\n",
            "|    ep_rew_mean     | 272      |\n",
            "| time/              |          |\n",
            "|    fps             | 1565     |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 94       |\n",
            "|    total_timesteps | 147456   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=228.93 +/- 88.06\n",
            "Episode length: 218.40 +/- 16.27\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 218          |\n",
            "|    mean_reward          | 229          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 150000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049531376 |\n",
            "|    clip_fraction        | 0.0545       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.627       |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.74         |\n",
            "|    n_updates            | 588          |\n",
            "|    policy_gradient_loss | 0.00171      |\n",
            "|    value_loss           | 38.5         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=286.05 +/- 14.25\n",
            "Episode length: 240.00 +/- 14.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 240      |\n",
            "|    mean_reward     | 286      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 160000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 229      |\n",
            "|    ep_rew_mean     | 278      |\n",
            "| time/              |          |\n",
            "|    fps             | 1553     |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 105      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=277.30 +/- 21.62\n",
            "Episode length: 233.20 +/- 15.43\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 233         |\n",
            "|    mean_reward          | 277         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 170000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004868458 |\n",
            "|    clip_fraction        | 0.0562      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.627      |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 27          |\n",
            "|    n_updates            | 592         |\n",
            "|    policy_gradient_loss | 0.000691    |\n",
            "|    value_loss           | 83.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=289.64 +/- 10.40\n",
            "Episode length: 238.40 +/- 27.35\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 238      |\n",
            "|    mean_reward     | 290      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 180000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 228      |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 1541     |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 116      |\n",
            "|    total_timesteps | 180224   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=278.54 +/- 18.70\n",
            "Episode length: 221.20 +/- 6.73\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 221          |\n",
            "|    mean_reward          | 279          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 190000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035245707 |\n",
            "|    clip_fraction        | 0.0342       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.628       |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 25.5         |\n",
            "|    n_updates            | 596          |\n",
            "|    policy_gradient_loss | -2.8e-05     |\n",
            "|    value_loss           | 121          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 239      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1536     |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 127      |\n",
            "|    total_timesteps | 196608   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=290.44 +/- 15.49\n",
            "Episode length: 229.80 +/- 21.26\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 230          |\n",
            "|    mean_reward          | 290          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 200000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030275588 |\n",
            "|    clip_fraction        | 0.0261       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.651       |\n",
            "|    explained_variance   | 0.928        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 46.7         |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.000184    |\n",
            "|    value_loss           | 213          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=289.46 +/- 16.62\n",
            "Episode length: 237.00 +/- 16.35\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | 289      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 210000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 243      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1514     |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 140      |\n",
            "|    total_timesteps | 212992   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=286.92 +/- 16.48\n",
            "Episode length: 234.00 +/- 14.11\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 234          |\n",
            "|    mean_reward          | 287          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 220000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053483145 |\n",
            "|    clip_fraction        | 0.0618       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.631       |\n",
            "|    explained_variance   | 0.956        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.8         |\n",
            "|    n_updates            | 604          |\n",
            "|    policy_gradient_loss | -0.000608    |\n",
            "|    value_loss           | 154          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 246      |\n",
            "|    ep_rew_mean     | 273      |\n",
            "| time/              |          |\n",
            "|    fps             | 1507     |\n",
            "|    iterations      | 14       |\n",
            "|    time_elapsed    | 152      |\n",
            "|    total_timesteps | 229376   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=272.00 +/- 20.37\n",
            "Episode length: 229.00 +/- 19.48\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 229          |\n",
            "|    mean_reward          | 272          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 230000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038818042 |\n",
            "|    clip_fraction        | 0.0426       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.642       |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 38.2         |\n",
            "|    n_updates            | 608          |\n",
            "|    policy_gradient_loss | 8.91e-05     |\n",
            "|    value_loss           | 94.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=287.18 +/- 18.63\n",
            "Episode length: 245.80 +/- 8.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 246      |\n",
            "|    mean_reward     | 287      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 240000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 247      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1501     |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 163      |\n",
            "|    total_timesteps | 245760   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=285.43 +/- 20.69\n",
            "Episode length: 220.60 +/- 12.61\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 221         |\n",
            "|    mean_reward          | 285         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 250000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003062295 |\n",
            "|    clip_fraction        | 0.0386      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.619      |\n",
            "|    explained_variance   | 0.978       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 68          |\n",
            "|    n_updates            | 612         |\n",
            "|    policy_gradient_loss | 0.00106     |\n",
            "|    value_loss           | 51.2        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=283.17 +/- 14.79\n",
            "Episode length: 228.40 +/- 6.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 228      |\n",
            "|    mean_reward     | 283      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 260000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 222      |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 1502     |\n",
            "|    iterations      | 16       |\n",
            "|    time_elapsed    | 174      |\n",
            "|    total_timesteps | 262144   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=276.18 +/- 22.05\n",
            "Episode length: 231.20 +/- 16.86\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 231         |\n",
            "|    mean_reward          | 276         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 270000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005072452 |\n",
            "|    clip_fraction        | 0.0646      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.618      |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.9        |\n",
            "|    n_updates            | 616         |\n",
            "|    policy_gradient_loss | 0.000462    |\n",
            "|    value_loss           | 72.3        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 219      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1511     |\n",
            "|    iterations      | 17       |\n",
            "|    time_elapsed    | 184      |\n",
            "|    total_timesteps | 278528   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=273.95 +/- 14.57\n",
            "Episode length: 240.80 +/- 18.13\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 241         |\n",
            "|    mean_reward          | 274         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 280000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004153438 |\n",
            "|    clip_fraction        | 0.0504      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.628      |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 115         |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | 0.000784    |\n",
            "|    value_loss           | 41.2        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=280.99 +/- 14.82\n",
            "Episode length: 219.40 +/- 9.46\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 219      |\n",
            "|    mean_reward     | 281      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 290000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 219      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1511     |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 195      |\n",
            "|    total_timesteps | 294912   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=269.55 +/- 20.39\n",
            "Episode length: 211.00 +/- 7.04\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 211          |\n",
            "|    mean_reward          | 270          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 300000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033099859 |\n",
            "|    clip_fraction        | 0.0331       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.621       |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 26.9         |\n",
            "|    n_updates            | 624          |\n",
            "|    policy_gradient_loss | 7.52e-05     |\n",
            "|    value_loss           | 91.1         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=287.72 +/- 17.28\n",
            "Episode length: 225.00 +/- 13.31\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 225      |\n",
            "|    mean_reward     | 288      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 310000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 223      |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 1510     |\n",
            "|    iterations      | 19       |\n",
            "|    time_elapsed    | 206      |\n",
            "|    total_timesteps | 311296   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=285.80 +/- 21.23\n",
            "Episode length: 219.20 +/- 9.91\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 219          |\n",
            "|    mean_reward          | 286          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 320000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041243415 |\n",
            "|    clip_fraction        | 0.0458       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.599       |\n",
            "|    explained_variance   | 0.961        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 81.8         |\n",
            "|    n_updates            | 628          |\n",
            "|    policy_gradient_loss | 0.000104     |\n",
            "|    value_loss           | 118          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 222      |\n",
            "|    ep_rew_mean     | 272      |\n",
            "| time/              |          |\n",
            "|    fps             | 1514     |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 216      |\n",
            "|    total_timesteps | 327680   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=273.48 +/- 20.78\n",
            "Episode length: 215.60 +/- 11.89\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 216          |\n",
            "|    mean_reward          | 273          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 330000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033647125 |\n",
            "|    clip_fraction        | 0.0314       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.586       |\n",
            "|    explained_variance   | 0.924        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 32.9         |\n",
            "|    n_updates            | 632          |\n",
            "|    policy_gradient_loss | -0.00149     |\n",
            "|    value_loss           | 244          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=286.68 +/- 18.91\n",
            "Episode length: 226.40 +/- 23.69\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | 287      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 340000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 231      |\n",
            "|    ep_rew_mean     | 270      |\n",
            "| time/              |          |\n",
            "|    fps             | 1509     |\n",
            "|    iterations      | 21       |\n",
            "|    time_elapsed    | 227      |\n",
            "|    total_timesteps | 344064   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=268.39 +/- 18.75\n",
            "Episode length: 222.00 +/- 18.58\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 222          |\n",
            "|    mean_reward          | 268          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 350000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039736177 |\n",
            "|    clip_fraction        | 0.0416       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.615       |\n",
            "|    explained_variance   | 0.951        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.32         |\n",
            "|    n_updates            | 636          |\n",
            "|    policy_gradient_loss | 0.000571     |\n",
            "|    value_loss           | 153          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=267.38 +/- 20.55\n",
            "Episode length: 221.60 +/- 12.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 222      |\n",
            "|    mean_reward     | 267      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 360000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 224      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1506     |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 239      |\n",
            "|    total_timesteps | 360448   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=281.15 +/- 26.12\n",
            "Episode length: 221.80 +/- 16.22\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 222          |\n",
            "|    mean_reward          | 281          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 370000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048652263 |\n",
            "|    clip_fraction        | 0.0748       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.617       |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 20.1         |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | -0.0003      |\n",
            "|    value_loss           | 16.9         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 231      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1506     |\n",
            "|    iterations      | 23       |\n",
            "|    time_elapsed    | 250      |\n",
            "|    total_timesteps | 376832   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=286.31 +/- 15.90\n",
            "Episode length: 277.40 +/- 100.91\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 277          |\n",
            "|    mean_reward          | 286          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 380000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035102842 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.617       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.88         |\n",
            "|    n_updates            | 644          |\n",
            "|    policy_gradient_loss | 0.000467     |\n",
            "|    value_loss           | 78.7         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=289.90 +/- 16.08\n",
            "Episode length: 226.20 +/- 21.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | 290      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 390000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 233      |\n",
            "|    ep_rew_mean     | 271      |\n",
            "| time/              |          |\n",
            "|    fps             | 1503     |\n",
            "|    iterations      | 24       |\n",
            "|    time_elapsed    | 261      |\n",
            "|    total_timesteps | 393216   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=277.46 +/- 21.22\n",
            "Episode length: 236.20 +/- 11.72\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 236         |\n",
            "|    mean_reward          | 277         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 400000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002785369 |\n",
            "|    clip_fraction        | 0.039       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.612      |\n",
            "|    explained_variance   | 0.959       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.21        |\n",
            "|    n_updates            | 648         |\n",
            "|    policy_gradient_loss | 0.000193    |\n",
            "|    value_loss           | 104         |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | 279      |\n",
            "| time/              |          |\n",
            "|    fps             | 1508     |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 271      |\n",
            "|    total_timesteps | 409600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=288.50 +/- 18.96\n",
            "Episode length: 223.20 +/- 14.73\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 223          |\n",
            "|    mean_reward          | 288          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 410000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035249218 |\n",
            "|    clip_fraction        | 0.0443       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.603       |\n",
            "|    explained_variance   | 0.975        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.31         |\n",
            "|    n_updates            | 652          |\n",
            "|    policy_gradient_loss | 0.000855     |\n",
            "|    value_loss           | 47.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=294.89 +/- 15.98\n",
            "Episode length: 226.60 +/- 15.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 227      |\n",
            "|    mean_reward     | 295      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 420000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 219      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1508     |\n",
            "|    iterations      | 26       |\n",
            "|    time_elapsed    | 282      |\n",
            "|    total_timesteps | 425984   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=285.78 +/- 16.54\n",
            "Episode length: 217.60 +/- 5.71\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 218          |\n",
            "|    mean_reward          | 286          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 430000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047708545 |\n",
            "|    clip_fraction        | 0.0497       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.598       |\n",
            "|    explained_variance   | 0.964        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 65.7         |\n",
            "|    n_updates            | 656          |\n",
            "|    policy_gradient_loss | -3.79e-05    |\n",
            "|    value_loss           | 94.1         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=280.75 +/- 4.96\n",
            "Episode length: 215.60 +/- 11.69\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 216      |\n",
            "|    mean_reward     | 281      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 440000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 222      |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 1508     |\n",
            "|    iterations      | 27       |\n",
            "|    time_elapsed    | 293      |\n",
            "|    total_timesteps | 442368   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=282.63 +/- 17.11\n",
            "Episode length: 222.40 +/- 16.12\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 222          |\n",
            "|    mean_reward          | 283          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 450000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038467797 |\n",
            "|    clip_fraction        | 0.0433       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.611       |\n",
            "|    explained_variance   | 0.981        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 197          |\n",
            "|    n_updates            | 660          |\n",
            "|    policy_gradient_loss | 0.00013      |\n",
            "|    value_loss           | 53.5         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 220      |\n",
            "|    ep_rew_mean     | 278      |\n",
            "| time/              |          |\n",
            "|    fps             | 1515     |\n",
            "|    iterations      | 28       |\n",
            "|    time_elapsed    | 302      |\n",
            "|    total_timesteps | 458752   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=288.76 +/- 13.80\n",
            "Episode length: 230.00 +/- 16.66\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 230          |\n",
            "|    mean_reward          | 289          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 460000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039974377 |\n",
            "|    clip_fraction        | 0.054        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.611       |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.22         |\n",
            "|    n_updates            | 664          |\n",
            "|    policy_gradient_loss | 0.000382     |\n",
            "|    value_loss           | 70.2         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=281.46 +/- 17.95\n",
            "Episode length: 217.00 +/- 6.07\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 281      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 470000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 220      |\n",
            "|    ep_rew_mean     | 278      |\n",
            "| time/              |          |\n",
            "|    fps             | 1517     |\n",
            "|    iterations      | 29       |\n",
            "|    time_elapsed    | 313      |\n",
            "|    total_timesteps | 475136   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=283.79 +/- 22.59\n",
            "Episode length: 216.20 +/- 16.82\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 216         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 480000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003408325 |\n",
            "|    clip_fraction        | 0.0487      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.601      |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 668         |\n",
            "|    policy_gradient_loss | 0.000201    |\n",
            "|    value_loss           | 12.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=267.46 +/- 25.69\n",
            "Episode length: 216.20 +/- 10.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 216      |\n",
            "|    mean_reward     | 267      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 490000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 217      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1516     |\n",
            "|    iterations      | 30       |\n",
            "|    time_elapsed    | 324      |\n",
            "|    total_timesteps | 491520   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=284.36 +/- 18.34\n",
            "Episode length: 229.80 +/- 17.00\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 230         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 500000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004265689 |\n",
            "|    clip_fraction        | 0.038       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.616      |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.76        |\n",
            "|    n_updates            | 672         |\n",
            "|    policy_gradient_loss | 0.000744    |\n",
            "|    value_loss           | 18.6        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 227      |\n",
            "|    ep_rew_mean     | 278      |\n",
            "| time/              |          |\n",
            "|    fps             | 1518     |\n",
            "|    iterations      | 31       |\n",
            "|    time_elapsed    | 334      |\n",
            "|    total_timesteps | 507904   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=288.19 +/- 19.93\n",
            "Episode length: 224.20 +/- 14.99\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 224          |\n",
            "|    mean_reward          | 288          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 510000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045138747 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.612       |\n",
            "|    explained_variance   | 0.971        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 303          |\n",
            "|    n_updates            | 676          |\n",
            "|    policy_gradient_loss | 3.08e-05     |\n",
            "|    value_loss           | 100          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=273.67 +/- 20.34\n",
            "Episode length: 221.40 +/- 7.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 221      |\n",
            "|    mean_reward     | 274      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 520000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 231      |\n",
            "|    ep_rew_mean     | 276      |\n",
            "| time/              |          |\n",
            "|    fps             | 1517     |\n",
            "|    iterations      | 32       |\n",
            "|    time_elapsed    | 345      |\n",
            "|    total_timesteps | 524288   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=284.30 +/- 22.50\n",
            "Episode length: 215.80 +/- 15.18\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 216         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 530000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002835039 |\n",
            "|    clip_fraction        | 0.0218      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.587      |\n",
            "|    explained_variance   | 0.97        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 81.9        |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | 0.000629    |\n",
            "|    value_loss           | 68.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=297.89 +/- 7.09\n",
            "Episode length: 222.80 +/- 8.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 223      |\n",
            "|    mean_reward     | 298      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 540000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 227      |\n",
            "|    ep_rew_mean     | 281      |\n",
            "| time/              |          |\n",
            "|    fps             | 1514     |\n",
            "|    iterations      | 33       |\n",
            "|    time_elapsed    | 356      |\n",
            "|    total_timesteps | 540672   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=273.77 +/- 10.56\n",
            "Episode length: 219.40 +/- 11.04\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 219         |\n",
            "|    mean_reward          | 274         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 550000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004506558 |\n",
            "|    clip_fraction        | 0.0435      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.606      |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 15.2        |\n",
            "|    n_updates            | 684         |\n",
            "|    policy_gradient_loss | 0.000423    |\n",
            "|    value_loss           | 26.8        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 233      |\n",
            "|    ep_rew_mean     | 280      |\n",
            "| time/              |          |\n",
            "|    fps             | 1517     |\n",
            "|    iterations      | 34       |\n",
            "|    time_elapsed    | 367      |\n",
            "|    total_timesteps | 557056   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=284.91 +/- 14.27\n",
            "Episode length: 222.60 +/- 15.64\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 223          |\n",
            "|    mean_reward          | 285          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 560000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037639462 |\n",
            "|    clip_fraction        | 0.0567       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.613       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.62         |\n",
            "|    n_updates            | 688          |\n",
            "|    policy_gradient_loss | 0.000814     |\n",
            "|    value_loss           | 4.24         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=297.72 +/- 10.50\n",
            "Episode length: 218.80 +/- 13.48\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 219      |\n",
            "|    mean_reward     | 298      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 570000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 218      |\n",
            "|    ep_rew_mean     | 278      |\n",
            "| time/              |          |\n",
            "|    fps             | 1515     |\n",
            "|    iterations      | 35       |\n",
            "|    time_elapsed    | 378      |\n",
            "|    total_timesteps | 573440   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=291.60 +/- 10.88\n",
            "Episode length: 221.40 +/- 10.48\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 221         |\n",
            "|    mean_reward          | 292         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 580000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004203336 |\n",
            "|    clip_fraction        | 0.0508      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.614      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.753       |\n",
            "|    n_updates            | 692         |\n",
            "|    policy_gradient_loss | -0.000302   |\n",
            "|    value_loss           | 2.72        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | 271      |\n",
            "| time/              |          |\n",
            "|    fps             | 1518     |\n",
            "|    iterations      | 36       |\n",
            "|    time_elapsed    | 388      |\n",
            "|    total_timesteps | 589824   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=286.66 +/- 6.63\n",
            "Episode length: 220.20 +/- 16.44\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 220          |\n",
            "|    mean_reward          | 287          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 590000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024619694 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.582       |\n",
            "|    explained_variance   | 0.927        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 235          |\n",
            "|    n_updates            | 696          |\n",
            "|    policy_gradient_loss | -0.000756    |\n",
            "|    value_loss           | 198          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=285.97 +/- 25.86\n",
            "Episode length: 229.20 +/- 15.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 229      |\n",
            "|    mean_reward     | 286      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 600000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 221      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1516     |\n",
            "|    iterations      | 37       |\n",
            "|    time_elapsed    | 399      |\n",
            "|    total_timesteps | 606208   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=268.54 +/- 20.60\n",
            "Episode length: 224.00 +/- 11.31\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 224          |\n",
            "|    mean_reward          | 269          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 610000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038494302 |\n",
            "|    clip_fraction        | 0.0307       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.604       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 39.2         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | 0.000337     |\n",
            "|    value_loss           | 109          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=289.95 +/- 9.44\n",
            "Episode length: 214.00 +/- 15.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 290      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 620000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 228      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1511     |\n",
            "|    iterations      | 38       |\n",
            "|    time_elapsed    | 411      |\n",
            "|    total_timesteps | 622592   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=281.27 +/- 16.10\n",
            "Episode length: 219.00 +/- 8.88\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 219          |\n",
            "|    mean_reward          | 281          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 630000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042843074 |\n",
            "|    clip_fraction        | 0.0318       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.58        |\n",
            "|    explained_variance   | 0.971        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 39.5         |\n",
            "|    n_updates            | 704          |\n",
            "|    policy_gradient_loss | 0.000204     |\n",
            "|    value_loss           | 81.6         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 1514     |\n",
            "|    iterations      | 39       |\n",
            "|    time_elapsed    | 421      |\n",
            "|    total_timesteps | 638976   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=284.28 +/- 23.34\n",
            "Episode length: 232.00 +/- 14.31\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 232         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 640000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003841284 |\n",
            "|    clip_fraction        | 0.0524      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.602      |\n",
            "|    explained_variance   | 0.952       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 115         |\n",
            "|    n_updates            | 708         |\n",
            "|    policy_gradient_loss | 7.2e-05     |\n",
            "|    value_loss           | 162         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=650000, episode_reward=222.63 +/- 74.05\n",
            "Episode length: 210.20 +/- 26.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 210      |\n",
            "|    mean_reward     | 223      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 650000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 226      |\n",
            "|    ep_rew_mean     | 275      |\n",
            "| time/              |          |\n",
            "|    fps             | 1513     |\n",
            "|    iterations      | 40       |\n",
            "|    time_elapsed    | 433      |\n",
            "|    total_timesteps | 655360   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=284.18 +/- 18.05\n",
            "Episode length: 223.60 +/- 18.02\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 224         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 660000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003983846 |\n",
            "|    clip_fraction        | 0.0542      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.593      |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.5        |\n",
            "|    n_updates            | 712         |\n",
            "|    policy_gradient_loss | 0.000989    |\n",
            "|    value_loss           | 124         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=261.77 +/- 40.97\n",
            "Episode length: 389.60 +/- 306.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 390      |\n",
            "|    mean_reward     | 262      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 670000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1507     |\n",
            "|    iterations      | 41       |\n",
            "|    time_elapsed    | 445      |\n",
            "|    total_timesteps | 671744   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=269.39 +/- 17.73\n",
            "Episode length: 222.60 +/- 11.48\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 223         |\n",
            "|    mean_reward          | 269         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 680000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003305512 |\n",
            "|    clip_fraction        | 0.044       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.602      |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.5        |\n",
            "|    n_updates            | 716         |\n",
            "|    policy_gradient_loss | 0.000883    |\n",
            "|    value_loss           | 128         |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 223      |\n",
            "|    ep_rew_mean     | 282      |\n",
            "| time/              |          |\n",
            "|    fps             | 1510     |\n",
            "|    iterations      | 42       |\n",
            "|    time_elapsed    | 455      |\n",
            "|    total_timesteps | 688128   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=301.61 +/- 9.66\n",
            "Episode length: 230.40 +/- 17.51\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 230          |\n",
            "|    mean_reward          | 302          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 690000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051338854 |\n",
            "|    clip_fraction        | 0.0643       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.587       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.65         |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | 1.64e-05     |\n",
            "|    value_loss           | 6.86         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=700000, episode_reward=287.60 +/- 20.62\n",
            "Episode length: 230.60 +/- 20.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 231      |\n",
            "|    mean_reward     | 288      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 700000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 227      |\n",
            "|    ep_rew_mean     | 280      |\n",
            "| time/              |          |\n",
            "|    fps             | 1509     |\n",
            "|    iterations      | 43       |\n",
            "|    time_elapsed    | 466      |\n",
            "|    total_timesteps | 704512   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=278.79 +/- 7.14\n",
            "Episode length: 228.80 +/- 11.32\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 229          |\n",
            "|    mean_reward          | 279          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 710000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033161049 |\n",
            "|    clip_fraction        | 0.026        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.572       |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.12         |\n",
            "|    n_updates            | 724          |\n",
            "|    policy_gradient_loss | 0.000189     |\n",
            "|    value_loss           | 37.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=276.67 +/- 16.26\n",
            "Episode length: 227.20 +/- 8.21\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 227      |\n",
            "|    mean_reward     | 277      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 720000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 222      |\n",
            "|    ep_rew_mean     | 276      |\n",
            "| time/              |          |\n",
            "|    fps             | 1510     |\n",
            "|    iterations      | 44       |\n",
            "|    time_elapsed    | 477      |\n",
            "|    total_timesteps | 720896   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=293.63 +/- 13.24\n",
            "Episode length: 316.60 +/- 170.56\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 317          |\n",
            "|    mean_reward          | 294          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 730000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039098435 |\n",
            "|    clip_fraction        | 0.05         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.581       |\n",
            "|    explained_variance   | 0.956        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 50.2         |\n",
            "|    n_updates            | 728          |\n",
            "|    policy_gradient_loss | -8.37e-05    |\n",
            "|    value_loss           | 130          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 225      |\n",
            "|    ep_rew_mean     | 283      |\n",
            "| time/              |          |\n",
            "|    fps             | 1509     |\n",
            "|    iterations      | 45       |\n",
            "|    time_elapsed    | 488      |\n",
            "|    total_timesteps | 737280   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=281.03 +/- 2.12\n",
            "Episode length: 243.20 +/- 22.25\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 243        |\n",
            "|    mean_reward          | 281        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 740000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00582149 |\n",
            "|    clip_fraction        | 0.0605     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.585     |\n",
            "|    explained_variance   | 0.988      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 18.8       |\n",
            "|    n_updates            | 732        |\n",
            "|    policy_gradient_loss | 0.000764   |\n",
            "|    value_loss           | 15.1       |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=294.09 +/- 13.90\n",
            "Episode length: 227.80 +/- 19.43\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 228      |\n",
            "|    mean_reward     | 294      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 750000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 234      |\n",
            "|    ep_rew_mean     | 279      |\n",
            "| time/              |          |\n",
            "|    fps             | 1507     |\n",
            "|    iterations      | 46       |\n",
            "|    time_elapsed    | 499      |\n",
            "|    total_timesteps | 753664   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=275.70 +/- 5.75\n",
            "Episode length: 228.40 +/- 13.92\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 228          |\n",
            "|    mean_reward          | 276          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 760000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043133525 |\n",
            "|    clip_fraction        | 0.0397       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.61        |\n",
            "|    explained_variance   | 0.955        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.26         |\n",
            "|    n_updates            | 736          |\n",
            "|    policy_gradient_loss | -0.000465    |\n",
            "|    value_loss           | 147          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=274.99 +/- 20.31\n",
            "Episode length: 221.80 +/- 20.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 222      |\n",
            "|    mean_reward     | 275      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 770000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 229      |\n",
            "|    ep_rew_mean     | 272      |\n",
            "| time/              |          |\n",
            "|    fps             | 1507     |\n",
            "|    iterations      | 47       |\n",
            "|    time_elapsed    | 510      |\n",
            "|    total_timesteps | 770048   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=268.01 +/- 16.31\n",
            "Episode length: 222.00 +/- 14.59\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 222        |\n",
            "|    mean_reward          | 268        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 780000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00331662 |\n",
            "|    clip_fraction        | 0.0266     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.567     |\n",
            "|    explained_variance   | 0.899      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 127        |\n",
            "|    n_updates            | 740        |\n",
            "|    policy_gradient_loss | -0.00146   |\n",
            "|    value_loss           | 331        |\n",
            "----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 226      |\n",
            "|    ep_rew_mean     | 279      |\n",
            "| time/              |          |\n",
            "|    fps             | 1509     |\n",
            "|    iterations      | 48       |\n",
            "|    time_elapsed    | 521      |\n",
            "|    total_timesteps | 786432   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=250.41 +/- 42.92\n",
            "Episode length: 370.20 +/- 315.17\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 370          |\n",
            "|    mean_reward          | 250          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 790000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041437466 |\n",
            "|    clip_fraction        | 0.0464       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.564       |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 148          |\n",
            "|    n_updates            | 744          |\n",
            "|    policy_gradient_loss | 0.00093      |\n",
            "|    value_loss           | 79.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=280.38 +/- 18.07\n",
            "Episode length: 225.00 +/- 11.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 225      |\n",
            "|    mean_reward     | 280      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 800000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 227      |\n",
            "|    ep_rew_mean     | 281      |\n",
            "| time/              |          |\n",
            "|    fps             | 1503     |\n",
            "|    iterations      | 49       |\n",
            "|    time_elapsed    | 533      |\n",
            "|    total_timesteps | 802816   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=289.48 +/- 12.37\n",
            "Episode length: 219.40 +/- 16.18\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 219         |\n",
            "|    mean_reward          | 289         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 810000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004585023 |\n",
            "|    clip_fraction        | 0.0506      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.603      |\n",
            "|    explained_variance   | 0.972       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.69        |\n",
            "|    n_updates            | 748         |\n",
            "|    policy_gradient_loss | 0.000495    |\n",
            "|    value_loss           | 89.8        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 226      |\n",
            "|    ep_rew_mean     | 276      |\n",
            "| time/              |          |\n",
            "|    fps             | 1506     |\n",
            "|    iterations      | 50       |\n",
            "|    time_elapsed    | 543      |\n",
            "|    total_timesteps | 819200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=289.46 +/- 16.13\n",
            "Episode length: 228.80 +/- 14.43\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 229         |\n",
            "|    mean_reward          | 289         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 820000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004311629 |\n",
            "|    clip_fraction        | 0.0456      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.595      |\n",
            "|    explained_variance   | 0.975       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.39        |\n",
            "|    n_updates            | 752         |\n",
            "|    policy_gradient_loss | 0.000158    |\n",
            "|    value_loss           | 75.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=287.19 +/- 18.45\n",
            "Episode length: 234.60 +/- 17.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 235      |\n",
            "|    mean_reward     | 287      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 830000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 229      |\n",
            "|    ep_rew_mean     | 276      |\n",
            "| time/              |          |\n",
            "|    fps             | 1504     |\n",
            "|    iterations      | 51       |\n",
            "|    time_elapsed    | 555      |\n",
            "|    total_timesteps | 835584   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=293.23 +/- 25.91\n",
            "Episode length: 217.00 +/- 14.35\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 217          |\n",
            "|    mean_reward          | 293          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 840000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030467254 |\n",
            "|    clip_fraction        | 0.035        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.598       |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13           |\n",
            "|    n_updates            | 756          |\n",
            "|    policy_gradient_loss | 0.000798     |\n",
            "|    value_loss           | 90           |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=264.71 +/- 4.62\n",
            "Episode length: 219.20 +/- 14.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 219      |\n",
            "|    mean_reward     | 265      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 850000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 229      |\n",
            "|    ep_rew_mean     | 280      |\n",
            "| time/              |          |\n",
            "|    fps             | 1504     |\n",
            "|    iterations      | 52       |\n",
            "|    time_elapsed    | 566      |\n",
            "|    total_timesteps | 851968   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=263.51 +/- 8.63\n",
            "Episode length: 237.40 +/- 6.53\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 237          |\n",
            "|    mean_reward          | 264          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 860000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044284556 |\n",
            "|    clip_fraction        | 0.0551       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.592       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.25         |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00025     |\n",
            "|    value_loss           | 4.33         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 233      |\n",
            "|    ep_rew_mean     | 280      |\n",
            "| time/              |          |\n",
            "|    fps             | 1506     |\n",
            "|    iterations      | 53       |\n",
            "|    time_elapsed    | 576      |\n",
            "|    total_timesteps | 868352   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=283.69 +/- 24.85\n",
            "Episode length: 223.20 +/- 13.23\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 223          |\n",
            "|    mean_reward          | 284          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 870000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033018324 |\n",
            "|    clip_fraction        | 0.0351       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.59        |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.3          |\n",
            "|    n_updates            | 764          |\n",
            "|    policy_gradient_loss | 0.000275     |\n",
            "|    value_loss           | 66.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=231.00 +/- 100.56\n",
            "Episode length: 213.80 +/- 26.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 231      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 880000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 229      |\n",
            "|    ep_rew_mean     | 282      |\n",
            "| time/              |          |\n",
            "|    fps             | 1506     |\n",
            "|    iterations      | 54       |\n",
            "|    time_elapsed    | 587      |\n",
            "|    total_timesteps | 884736   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=291.71 +/- 18.10\n",
            "Episode length: 228.60 +/- 14.85\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 229         |\n",
            "|    mean_reward          | 292         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 890000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005176466 |\n",
            "|    clip_fraction        | 0.055       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.577      |\n",
            "|    explained_variance   | 0.976       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.59        |\n",
            "|    n_updates            | 768         |\n",
            "|    policy_gradient_loss | -0.00064    |\n",
            "|    value_loss           | 77.1        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=227.27 +/- 114.62\n",
            "Episode length: 214.40 +/- 25.82\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 227      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 900000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 227      |\n",
            "|    ep_rew_mean     | 281      |\n",
            "| time/              |          |\n",
            "|    fps             | 1507     |\n",
            "|    iterations      | 55       |\n",
            "|    time_elapsed    | 597      |\n",
            "|    total_timesteps | 901120   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=270.28 +/- 21.14\n",
            "Episode length: 227.80 +/- 13.48\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 228         |\n",
            "|    mean_reward          | 270         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 910000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003717069 |\n",
            "|    clip_fraction        | 0.0443      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.565      |\n",
            "|    explained_variance   | 0.996       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.36        |\n",
            "|    n_updates            | 772         |\n",
            "|    policy_gradient_loss | 0.00115     |\n",
            "|    value_loss           | 7.23        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 216      |\n",
            "|    ep_rew_mean     | 270      |\n",
            "| time/              |          |\n",
            "|    fps             | 1509     |\n",
            "|    iterations      | 56       |\n",
            "|    time_elapsed    | 607      |\n",
            "|    total_timesteps | 917504   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=273.80 +/- 12.11\n",
            "Episode length: 223.20 +/- 12.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 223         |\n",
            "|    mean_reward          | 274         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 920000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002633032 |\n",
            "|    clip_fraction        | 0.025       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.583      |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 35.9        |\n",
            "|    n_updates            | 776         |\n",
            "|    policy_gradient_loss | 0.000196    |\n",
            "|    value_loss           | 258         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=292.94 +/- 11.31\n",
            "Episode length: 226.00 +/- 17.48\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | 293      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 930000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 232      |\n",
            "|    ep_rew_mean     | 269      |\n",
            "| time/              |          |\n",
            "|    fps             | 1508     |\n",
            "|    iterations      | 57       |\n",
            "|    time_elapsed    | 618      |\n",
            "|    total_timesteps | 933888   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=239.74 +/- 86.70\n",
            "Episode length: 215.00 +/- 28.31\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 215          |\n",
            "|    mean_reward          | 240          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 940000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047731213 |\n",
            "|    clip_fraction        | 0.0363       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.578       |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.11         |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | 0.000696     |\n",
            "|    value_loss           | 119          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=290.18 +/- 7.36\n",
            "Episode length: 231.20 +/- 18.05\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 231      |\n",
            "|    mean_reward     | 290      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 950000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 221      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1507     |\n",
            "|    iterations      | 58       |\n",
            "|    time_elapsed    | 630      |\n",
            "|    total_timesteps | 950272   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=284.14 +/- 20.52\n",
            "Episode length: 241.20 +/- 17.86\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 241          |\n",
            "|    mean_reward          | 284          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 960000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063379537 |\n",
            "|    clip_fraction        | 0.0755       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.578       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2            |\n",
            "|    n_updates            | 784          |\n",
            "|    policy_gradient_loss | 0.000368     |\n",
            "|    value_loss           | 4.51         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 221      |\n",
            "|    ep_rew_mean     | 278      |\n",
            "| time/              |          |\n",
            "|    fps             | 1509     |\n",
            "|    iterations      | 59       |\n",
            "|    time_elapsed    | 640      |\n",
            "|    total_timesteps | 966656   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=214.15 +/- 94.67\n",
            "Episode length: 211.40 +/- 26.14\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 211          |\n",
            "|    mean_reward          | 214          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 970000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040967283 |\n",
            "|    clip_fraction        | 0.0515       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.559       |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.8          |\n",
            "|    n_updates            | 788          |\n",
            "|    policy_gradient_loss | 0.000584     |\n",
            "|    value_loss           | 64.6         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=282.80 +/- 9.99\n",
            "Episode length: 222.20 +/- 15.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 222      |\n",
            "|    mean_reward     | 283      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 980000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 223      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 1508     |\n",
            "|    iterations      | 60       |\n",
            "|    time_elapsed    | 651      |\n",
            "|    total_timesteps | 983040   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=265.89 +/- 18.45\n",
            "Episode length: 226.80 +/- 15.99\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 227         |\n",
            "|    mean_reward          | 266         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 990000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003262122 |\n",
            "|    clip_fraction        | 0.0222      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.577      |\n",
            "|    explained_variance   | 0.945       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.4        |\n",
            "|    n_updates            | 792         |\n",
            "|    policy_gradient_loss | -0.000565   |\n",
            "|    value_loss           | 161         |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 224      |\n",
            "|    ep_rew_mean     | 277      |\n",
            "| time/              |          |\n",
            "|    fps             | 1511     |\n",
            "|    iterations      | 61       |\n",
            "|    time_elapsed    | 661      |\n",
            "|    total_timesteps | 999424   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=279.42 +/- 13.93\n",
            "Episode length: 225.60 +/- 21.66\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 226          |\n",
            "|    mean_reward          | 279          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 1000000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052900873 |\n",
            "|    clip_fraction        | 0.0574       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.571       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.89         |\n",
            "|    n_updates            | 796          |\n",
            "|    policy_gradient_loss | -0.00118     |\n",
            "|    value_loss           | 120          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=1010000, episode_reward=226.46 +/- 96.93\n",
            "Episode length: 201.40 +/- 20.97\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 201      |\n",
            "|    mean_reward     | 226      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 1010000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 216      |\n",
            "|    ep_rew_mean     | 272      |\n",
            "| time/              |          |\n",
            "|    fps             | 1511     |\n",
            "|    iterations      | 62       |\n",
            "|    time_elapsed    | 672      |\n",
            "|    total_timesteps | 1015808  |\n",
            "---------------------------------\n",
            "Saving to logs/ppo/LunarLander-v2_6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSaoyiAE8cVj",
        "outputId": "ae19e900-95d8-41b5-9b02-3a4ce554dc59"
      },
      "source": [
        "!python scripts/all_plots.py -a ppo --env LunarLander-v2 -f logs/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval not found for logs/ppo/LunarLander-v2_5\n",
            "# results_table\n",
            "| Environments |   PPO   |\n",
            "|--------------|---------|\n",
            "|              |logs/    |\n",
            "|LunarLander-v2|226 +/- 0|\n",
            "<Figure size 640x480 with 1 Axes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jL9u4I1H-48O",
        "outputId": "a185f52c-f70b-4c0e-fcea-80a8846ca87d"
      },
      "source": [
        "!python train.py --algo ppo --env LunarLander-v2 -n 100000 -i logs/ppo/LunarLander-v2_6/best_model.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== LunarLander-v2 ==========\n",
            "Seed: 2316408788\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('activation_fn', 'relu'),\n",
            "             ('batch_size', 64),\n",
            "             ('clip_range', 0.1),\n",
            "             ('ent_coef', 2.9352370524833907e-06),\n",
            "             ('gae_lambda', 0.95),\n",
            "             ('gamma', 0.999),\n",
            "             ('learning_rate', 0.00011596933243196016),\n",
            "             (\"max_grad_norm'\", 5),\n",
            "             ('n_envs', 16),\n",
            "             ('n_epochs', 10),\n",
            "             ('n_steps', 32),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('net_arch', 'medium'),\n",
            "             ('policy', 'MlpPolicy'),\n",
            "             ('vf_coef', 0.4964941214692943)])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=100000\n",
            "Creating test environment\n",
            "Loading pretrained agent\n",
            "Log path: logs/ppo/LunarLander-v2_7\n",
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 4369 |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 0    |\n",
            "|    total_timesteps | 512  |\n",
            "-----------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1885         |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 0            |\n",
            "|    total_timesteps      | 1024         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011227598 |\n",
            "|    clip_fraction        | 0.0139       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.681       |\n",
            "|    explained_variance   | 0.897        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.46         |\n",
            "|    n_updates            | 730          |\n",
            "|    policy_gradient_loss | -0.00144     |\n",
            "|    value_loss           | 7.01         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 1518        |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 1           |\n",
            "|    total_timesteps      | 1536        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002767644 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.728      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 0.603       |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00688    |\n",
            "|    value_loss           | 1.95        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1389         |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 1            |\n",
            "|    total_timesteps      | 2048         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062452615 |\n",
            "|    clip_fraction        | 0.14         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.789       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.562        |\n",
            "|    n_updates            | 750          |\n",
            "|    policy_gradient_loss | -0.00561     |\n",
            "|    value_loss           | 2.25         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 1308         |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 1            |\n",
            "|    total_timesteps      | 2560         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016290144 |\n",
            "|    clip_fraction        | 0.0641       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.865       |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.76         |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00313     |\n",
            "|    value_loss           | 4.4          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 189          |\n",
            "|    ep_rew_mean          | 293          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1237         |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 2            |\n",
            "|    total_timesteps      | 3072         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009704038 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.579       |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.52         |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -0.0035      |\n",
            "|    value_loss           | 5.73         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 210          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1211         |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 2            |\n",
            "|    total_timesteps      | 3584         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008862115 |\n",
            "|    clip_fraction        | 0.0287       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.143       |\n",
            "|    explained_variance   | 0.99         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.634        |\n",
            "|    n_updates            | 780          |\n",
            "|    policy_gradient_loss | -0.00529     |\n",
            "|    value_loss           | 2.2          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1216         |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 3            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005205171 |\n",
            "|    clip_fraction        | 0.0176       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.247       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.482        |\n",
            "|    n_updates            | 790          |\n",
            "|    policy_gradient_loss | -0.000245    |\n",
            "|    value_loss           | 1.31         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1218         |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 3            |\n",
            "|    total_timesteps      | 4608         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015825137 |\n",
            "|    clip_fraction        | 0.0711       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.681       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.661        |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.0056      |\n",
            "|    value_loss           | 2.18         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1220         |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 5120         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015673515 |\n",
            "|    clip_fraction        | 0.0494       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.774       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.328        |\n",
            "|    n_updates            | 810          |\n",
            "|    policy_gradient_loss | -0.00251     |\n",
            "|    value_loss           | 1.26         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1208         |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 5632         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023187648 |\n",
            "|    clip_fraction        | 0.112        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.768       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.553        |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -0.0065      |\n",
            "|    value_loss           | 1.85         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1197         |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012976485 |\n",
            "|    clip_fraction        | 0.0221       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.723       |\n",
            "|    explained_variance   | 0.96         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.872        |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    value_loss           | 7.74         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 210          |\n",
            "|    ep_rew_mean          | 289          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1170         |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 5            |\n",
            "|    total_timesteps      | 6656         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005444344 |\n",
            "|    clip_fraction        | 0.00684      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.469       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.776        |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00179     |\n",
            "|    value_loss           | 2.59         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 212          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1161         |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 7168         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011028423 |\n",
            "|    clip_fraction        | 0.0281       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.319       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.67         |\n",
            "|    n_updates            | 850          |\n",
            "|    policy_gradient_loss | -0.00266     |\n",
            "|    value_loss           | 2.87         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1164         |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 6            |\n",
            "|    total_timesteps      | 7680         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006638082 |\n",
            "|    clip_fraction        | 0.0248       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.441       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.22         |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | -0.00372     |\n",
            "|    value_loss           | 3.12         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 220         |\n",
            "|    ep_rew_mean          | 288         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1175        |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001392583 |\n",
            "|    clip_fraction        | 0.0783      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.532      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 0.328       |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.00375    |\n",
            "|    value_loss           | 1.3         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1181         |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 8704         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008396144 |\n",
            "|    clip_fraction        | 0.0307       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.72        |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.355        |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.00408     |\n",
            "|    value_loss           | 1.89         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 288          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1184         |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 7            |\n",
            "|    total_timesteps      | 9216         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011927207 |\n",
            "|    clip_fraction        | 0.0619       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.821       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.332        |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    value_loss           | 1.75         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 281          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1186         |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 8            |\n",
            "|    total_timesteps      | 9728         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014164502 |\n",
            "|    clip_fraction        | 0.033        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.716       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.748        |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | -0.00288     |\n",
            "|    value_loss           | 1.85         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=280.58 +/- 20.76\n",
            "Episode length: 216.40 +/- 12.89\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 216          |\n",
            "|    mean_reward          | 281          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 10000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.392074e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.481       |\n",
            "|    explained_variance   | 0.788        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 336          |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | -0.000106    |\n",
            "|    value_loss           | 554          |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 217      |\n",
            "|    ep_rew_mean     | 280      |\n",
            "| time/              |          |\n",
            "|    fps             | 1063     |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 10240    |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 218           |\n",
            "|    ep_rew_mean          | 279           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1071          |\n",
            "|    iterations           | 21            |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 10752         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00065419043 |\n",
            "|    clip_fraction        | 0.0189        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.428        |\n",
            "|    explained_variance   | 0.998         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 1.94          |\n",
            "|    n_updates            | 920           |\n",
            "|    policy_gradient_loss | -0.00321      |\n",
            "|    value_loss           | 5.29          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 218           |\n",
            "|    ep_rew_mean          | 278           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1080          |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 11264         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00078768854 |\n",
            "|    clip_fraction        | 0.0279        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.464        |\n",
            "|    explained_variance   | 0.999         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 0.464         |\n",
            "|    n_updates            | 930           |\n",
            "|    policy_gradient_loss | -0.00279      |\n",
            "|    value_loss           | 2.02          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1088         |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 11776        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011451918 |\n",
            "|    clip_fraction        | 0.0443       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.658       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.359        |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    value_loss           | 1.8          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1090         |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 11           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013328799 |\n",
            "|    clip_fraction        | 0.0463       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.773       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.52         |\n",
            "|    n_updates            | 950          |\n",
            "|    policy_gradient_loss | -0.0053      |\n",
            "|    value_loss           | 4.6          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 220         |\n",
            "|    ep_rew_mean          | 278         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1092        |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 11          |\n",
            "|    total_timesteps      | 12800       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003027771 |\n",
            "|    clip_fraction        | 0.0971      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.654      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 0.643       |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.00641    |\n",
            "|    value_loss           | 1.03        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 220          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1093         |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 13312        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007930298 |\n",
            "|    clip_fraction        | 0.0113       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.559       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.917        |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | -0.00179     |\n",
            "|    value_loss           | 2.78         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 280          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1093         |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 12           |\n",
            "|    total_timesteps      | 13824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009829077 |\n",
            "|    clip_fraction        | 0.0273       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.38        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.92         |\n",
            "|    n_updates            | 980          |\n",
            "|    policy_gradient_loss | -0.00217     |\n",
            "|    value_loss           | 2.16         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 219         |\n",
            "|    ep_rew_mean          | 279         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1095        |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 13          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002195704 |\n",
            "|    clip_fraction        | 0.0477      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.419      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 0.326       |\n",
            "|    n_updates            | 990         |\n",
            "|    policy_gradient_loss | -0.00388    |\n",
            "|    value_loss           | 1.32        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1100         |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 14848        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018783108 |\n",
            "|    clip_fraction        | 0.0596       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.542       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.33         |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | -0.00423     |\n",
            "|    value_loss           | 0.891        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1104         |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 13           |\n",
            "|    total_timesteps      | 15360        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021940432 |\n",
            "|    clip_fraction        | 0.112        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.678       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.344        |\n",
            "|    n_updates            | 1010         |\n",
            "|    policy_gradient_loss | -0.00399     |\n",
            "|    value_loss           | 1.46         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1106         |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 15872        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011107926 |\n",
            "|    clip_fraction        | 0.0545       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.737       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.464        |\n",
            "|    n_updates            | 1020         |\n",
            "|    policy_gradient_loss | -0.00293     |\n",
            "|    value_loss           | 1.74         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 219          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1105         |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013474972 |\n",
            "|    clip_fraction        | 0.0328       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.728       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.508        |\n",
            "|    n_updates            | 1030         |\n",
            "|    policy_gradient_loss | -0.00326     |\n",
            "|    value_loss           | 1.38         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 218           |\n",
            "|    ep_rew_mean          | 278           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1106          |\n",
            "|    iterations           | 33            |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 16896         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00061812566 |\n",
            "|    clip_fraction        | 0.0141        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.615        |\n",
            "|    explained_variance   | 0.992         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 6.65          |\n",
            "|    n_updates            | 1040          |\n",
            "|    policy_gradient_loss | -0.00237      |\n",
            "|    value_loss           | 8.41          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 278          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1107         |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 15           |\n",
            "|    total_timesteps      | 17408        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010931139 |\n",
            "|    clip_fraction        | 0.0525       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.545       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.556        |\n",
            "|    n_updates            | 1050         |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    value_loss           | 1.47         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1110         |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 16           |\n",
            "|    total_timesteps      | 17920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017220526 |\n",
            "|    clip_fraction        | 0.0623       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.531       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.607        |\n",
            "|    n_updates            | 1060         |\n",
            "|    policy_gradient_loss | -0.00349     |\n",
            "|    value_loss           | 3.65         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1111         |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 16           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013641546 |\n",
            "|    clip_fraction        | 0.0398       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.656       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.829        |\n",
            "|    n_updates            | 1070         |\n",
            "|    policy_gradient_loss | -0.00215     |\n",
            "|    value_loss           | 3.06         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 224           |\n",
            "|    ep_rew_mean          | 279           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1113          |\n",
            "|    iterations           | 37            |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 18944         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00027025037 |\n",
            "|    clip_fraction        | 0.00391       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.748        |\n",
            "|    explained_variance   | 0.989         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 1.88          |\n",
            "|    n_updates            | 1080          |\n",
            "|    policy_gradient_loss | -0.000657     |\n",
            "|    value_loss           | 13.6          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 279          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1115         |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 19456        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012752098 |\n",
            "|    clip_fraction        | 0.0312       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.761       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.809        |\n",
            "|    n_updates            | 1090         |\n",
            "|    policy_gradient_loss | -0.0033      |\n",
            "|    value_loss           | 5.61         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 276          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1116         |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 19968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006946521 |\n",
            "|    clip_fraction        | 0.00234      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.709       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.61         |\n",
            "|    n_updates            | 1100         |\n",
            "|    policy_gradient_loss | -0.000864    |\n",
            "|    value_loss           | 8.36         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=269.09 +/- 29.21\n",
            "Episode length: 225.20 +/- 11.77\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 225          |\n",
            "|    mean_reward          | 269          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 20000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016588309 |\n",
            "|    clip_fraction        | 0.0463       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.657       |\n",
            "|    explained_variance   | 0.955        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.86         |\n",
            "|    n_updates            | 1110         |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    value_loss           | 161          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 222      |\n",
            "|    ep_rew_mean     | 273      |\n",
            "| time/              |          |\n",
            "|    fps             | 1056     |\n",
            "|    iterations      | 40       |\n",
            "|    time_elapsed    | 19       |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1056         |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 20992        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004743908 |\n",
            "|    clip_fraction        | 0.00449      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.582       |\n",
            "|    explained_variance   | 0.872        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 531          |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | -1.65e-05    |\n",
            "|    value_loss           | 668          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1058         |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 21504        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012796493 |\n",
            "|    clip_fraction        | 0.0381       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.513       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.14         |\n",
            "|    n_updates            | 1130         |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 4.42         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1059         |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 20           |\n",
            "|    total_timesteps      | 22016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019531227 |\n",
            "|    clip_fraction        | 0.0477       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.626       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.961        |\n",
            "|    n_updates            | 1140         |\n",
            "|    policy_gradient_loss | -0.0037      |\n",
            "|    value_loss           | 3.69         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1061         |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 21           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016732664 |\n",
            "|    clip_fraction        | 0.0535       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.718       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.831        |\n",
            "|    n_updates            | 1150         |\n",
            "|    policy_gradient_loss | -0.00385     |\n",
            "|    value_loss           | 4.29         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 272          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1061         |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 21           |\n",
            "|    total_timesteps      | 23040        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014606685 |\n",
            "|    clip_fraction        | 0.0643       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.657       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.378        |\n",
            "|    n_updates            | 1160         |\n",
            "|    policy_gradient_loss | -0.0024      |\n",
            "|    value_loss           | 2            |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 227          |\n",
            "|    ep_rew_mean          | 271          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1062         |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 22           |\n",
            "|    total_timesteps      | 23552        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015324323 |\n",
            "|    clip_fraction        | 0.0615       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.593       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.495        |\n",
            "|    n_updates            | 1170         |\n",
            "|    policy_gradient_loss | -0.00386     |\n",
            "|    value_loss           | 1.87         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 236         |\n",
            "|    ep_rew_mean          | 269         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1064        |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 22          |\n",
            "|    total_timesteps      | 24064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000397488 |\n",
            "|    clip_fraction        | 0.00352     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.579      |\n",
            "|    explained_variance   | 0.944       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 82.9        |\n",
            "|    n_updates            | 1180        |\n",
            "|    policy_gradient_loss | -0.000322   |\n",
            "|    value_loss           | 142         |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 235           |\n",
            "|    ep_rew_mean          | 270           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1065          |\n",
            "|    iterations           | 48            |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024217891 |\n",
            "|    clip_fraction        | 0.00508       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.501        |\n",
            "|    explained_variance   | 0.975         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 34.4          |\n",
            "|    n_updates            | 1190          |\n",
            "|    policy_gradient_loss | -0.00201      |\n",
            "|    value_loss           | 106           |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 235         |\n",
            "|    ep_rew_mean          | 270         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1067        |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 23          |\n",
            "|    total_timesteps      | 25088       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001505314 |\n",
            "|    clip_fraction        | 0.0357      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.445      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 3.05        |\n",
            "|    n_updates            | 1200        |\n",
            "|    policy_gradient_loss | -0.0053     |\n",
            "|    value_loss           | 12.7        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 236           |\n",
            "|    ep_rew_mean          | 270           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1071          |\n",
            "|    iterations           | 50            |\n",
            "|    time_elapsed         | 23            |\n",
            "|    total_timesteps      | 25600         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00071284873 |\n",
            "|    clip_fraction        | 0.0141        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.637        |\n",
            "|    explained_variance   | 0.998         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 0.752         |\n",
            "|    n_updates            | 1210          |\n",
            "|    policy_gradient_loss | -0.00127      |\n",
            "|    value_loss           | 5.04          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 236          |\n",
            "|    ep_rew_mean          | 270          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1073         |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 26112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021308977 |\n",
            "|    clip_fraction        | 0.0822       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.69        |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.8          |\n",
            "|    n_updates            | 1220         |\n",
            "|    policy_gradient_loss | -0.0063      |\n",
            "|    value_loss           | 5.67         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 236          |\n",
            "|    ep_rew_mean          | 270          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1074         |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016680973 |\n",
            "|    clip_fraction        | 0.0455       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.647       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.23         |\n",
            "|    n_updates            | 1230         |\n",
            "|    policy_gradient_loss | -0.00398     |\n",
            "|    value_loss           | 7.44         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 237          |\n",
            "|    ep_rew_mean          | 270          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1075         |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 27136        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011483595 |\n",
            "|    clip_fraction        | 0.0512       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.528       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.59         |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.00472     |\n",
            "|    value_loss           | 2.35         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 236           |\n",
            "|    ep_rew_mean          | 268           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1076          |\n",
            "|    iterations           | 54            |\n",
            "|    time_elapsed         | 25            |\n",
            "|    total_timesteps      | 27648         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044661958 |\n",
            "|    clip_fraction        | 0.00625       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.664        |\n",
            "|    explained_variance   | 0.988         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 2.81          |\n",
            "|    n_updates            | 1250          |\n",
            "|    policy_gradient_loss | -0.00238      |\n",
            "|    value_loss           | 39.5          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 236          |\n",
            "|    ep_rew_mean          | 267          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1074         |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 28160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006186534 |\n",
            "|    clip_fraction        | 0.0104       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.635       |\n",
            "|    explained_variance   | 0.957        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 128          |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    value_loss           | 213          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 236          |\n",
            "|    ep_rew_mean          | 268          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1071         |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 26           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006740326 |\n",
            "|    clip_fraction        | 0.0193       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.566       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.4          |\n",
            "|    n_updates            | 1270         |\n",
            "|    policy_gradient_loss | -0.00327     |\n",
            "|    value_loss           | 20.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 237          |\n",
            "|    ep_rew_mean          | 268          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1070         |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 29184        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007290159 |\n",
            "|    clip_fraction        | 0.0107       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.607       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.74         |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | -0.00121     |\n",
            "|    value_loss           | 10.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 237          |\n",
            "|    ep_rew_mean          | 268          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1067         |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 27           |\n",
            "|    total_timesteps      | 29696        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016849504 |\n",
            "|    clip_fraction        | 0.098        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.577       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.914        |\n",
            "|    n_updates            | 1290         |\n",
            "|    policy_gradient_loss | -0.00704     |\n",
            "|    value_loss           | 7.94         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=30000, episode_reward=286.89 +/- 20.67\n",
            "Episode length: 229.80 +/- 17.68\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 230          |\n",
            "|    mean_reward          | 287          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 30000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017667043 |\n",
            "|    clip_fraction        | 0.048        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.601       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 4.5          |\n",
            "|    n_updates            | 1300         |\n",
            "|    policy_gradient_loss | -0.00339     |\n",
            "|    value_loss           | 9.94         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 239      |\n",
            "|    ep_rew_mean     | 267      |\n",
            "| time/              |          |\n",
            "|    fps             | 1022     |\n",
            "|    iterations      | 59       |\n",
            "|    time_elapsed    | 29       |\n",
            "|    total_timesteps | 30208    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 267          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1025         |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 29           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005257743 |\n",
            "|    clip_fraction        | 0.0109       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.501       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.893        |\n",
            "|    n_updates            | 1310         |\n",
            "|    policy_gradient_loss | -0.000631    |\n",
            "|    value_loss           | 9.32         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 266          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1026         |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 31232        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012739034 |\n",
            "|    clip_fraction        | 0.0338       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.649       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.3          |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | -0.00297     |\n",
            "|    value_loss           | 5.32         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 267          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1028         |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 30           |\n",
            "|    total_timesteps      | 31744        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009974224 |\n",
            "|    clip_fraction        | 0.0496       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.561       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.956        |\n",
            "|    n_updates            | 1330         |\n",
            "|    policy_gradient_loss | -0.00269     |\n",
            "|    value_loss           | 8.64         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1030         |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 32256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015099699 |\n",
            "|    clip_fraction        | 0.0625       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.706       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.36         |\n",
            "|    n_updates            | 1340         |\n",
            "|    policy_gradient_loss | -0.00317     |\n",
            "|    value_loss           | 4.13         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1031         |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022402878 |\n",
            "|    clip_fraction        | 0.0668       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.676       |\n",
            "|    explained_variance   | 0.945        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 84.6         |\n",
            "|    n_updates            | 1350         |\n",
            "|    policy_gradient_loss | -0.00495     |\n",
            "|    value_loss           | 161          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1033         |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 32           |\n",
            "|    total_timesteps      | 33280        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006516769 |\n",
            "|    clip_fraction        | 0.0287       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.571       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.1          |\n",
            "|    n_updates            | 1360         |\n",
            "|    policy_gradient_loss | -0.00214     |\n",
            "|    value_loss           | 2.58         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 245          |\n",
            "|    ep_rew_mean          | 265          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1035         |\n",
            "|    iterations           | 66           |\n",
            "|    time_elapsed         | 32           |\n",
            "|    total_timesteps      | 33792        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010163401 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.468       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.382        |\n",
            "|    n_updates            | 1370         |\n",
            "|    policy_gradient_loss | -0.00336     |\n",
            "|    value_loss           | 1.4          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 245          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1038         |\n",
            "|    iterations           | 67           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 34304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013235027 |\n",
            "|    clip_fraction        | 0.0467       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.599       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.479        |\n",
            "|    n_updates            | 1380         |\n",
            "|    policy_gradient_loss | -0.00511     |\n",
            "|    value_loss           | 2.21         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1041         |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005734606 |\n",
            "|    clip_fraction        | 0.0135       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.628       |\n",
            "|    explained_variance   | 0.864        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.91         |\n",
            "|    n_updates            | 1390         |\n",
            "|    policy_gradient_loss | -0.000726    |\n",
            "|    value_loss           | 670          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1043         |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 33           |\n",
            "|    total_timesteps      | 35328        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011991939 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.639       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.21         |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | -0.00308     |\n",
            "|    value_loss           | 3.63         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 244           |\n",
            "|    ep_rew_mean          | 264           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1045          |\n",
            "|    iterations           | 70            |\n",
            "|    time_elapsed         | 34            |\n",
            "|    total_timesteps      | 35840         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00089537585 |\n",
            "|    clip_fraction        | 0.0312        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.645        |\n",
            "|    explained_variance   | 0.996         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 3.42          |\n",
            "|    n_updates            | 1410          |\n",
            "|    policy_gradient_loss | -0.00251      |\n",
            "|    value_loss           | 7.09          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 244         |\n",
            "|    ep_rew_mean          | 264         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1047        |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 34          |\n",
            "|    total_timesteps      | 36352       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001958304 |\n",
            "|    clip_fraction        | 0.0775      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.699      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 0.379       |\n",
            "|    n_updates            | 1420        |\n",
            "|    policy_gradient_loss | -0.00577    |\n",
            "|    value_loss           | 1.96        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1049         |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010173154 |\n",
            "|    clip_fraction        | 0.0266       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.61        |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.69         |\n",
            "|    n_updates            | 1430         |\n",
            "|    policy_gradient_loss | -0.00358     |\n",
            "|    value_loss           | 4.16         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1050         |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 35           |\n",
            "|    total_timesteps      | 37376        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024403068 |\n",
            "|    clip_fraction        | 0.0785       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.448       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.553        |\n",
            "|    n_updates            | 1440         |\n",
            "|    policy_gradient_loss | -0.00361     |\n",
            "|    value_loss           | 1.11         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1050         |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 37888        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011251504 |\n",
            "|    clip_fraction        | 0.0395       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.54        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.663        |\n",
            "|    n_updates            | 1450         |\n",
            "|    policy_gradient_loss | -0.00465     |\n",
            "|    value_loss           | 2.56         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 245          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1051         |\n",
            "|    iterations           | 75           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 38400        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024312218 |\n",
            "|    clip_fraction        | 0.0547       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.57        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.615        |\n",
            "|    n_updates            | 1460         |\n",
            "|    policy_gradient_loss | -0.00238     |\n",
            "|    value_loss           | 1.47         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 246          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1052         |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 36           |\n",
            "|    total_timesteps      | 38912        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023434393 |\n",
            "|    clip_fraction        | 0.0852       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.707       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.307        |\n",
            "|    n_updates            | 1470         |\n",
            "|    policy_gradient_loss | -0.00682     |\n",
            "|    value_loss           | 1.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 246          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1054         |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 39424        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023951973 |\n",
            "|    clip_fraction        | 0.0949       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.72        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.386        |\n",
            "|    n_updates            | 1480         |\n",
            "|    policy_gradient_loss | -0.00702     |\n",
            "|    value_loss           | 1.36         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 246          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1054         |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 39936        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007931703 |\n",
            "|    clip_fraction        | 0.0246       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.76        |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.819        |\n",
            "|    n_updates            | 1490         |\n",
            "|    policy_gradient_loss | -0.00455     |\n",
            "|    value_loss           | 5.2          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=278.58 +/- 21.10\n",
            "Episode length: 238.00 +/- 6.10\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 238           |\n",
            "|    mean_reward          | 279           |\n",
            "| time/                   |               |\n",
            "|    total timesteps      | 40000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044326007 |\n",
            "|    clip_fraction        | 0.0043        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.589        |\n",
            "|    explained_variance   | 0.968         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 2.04          |\n",
            "|    n_updates            | 1500          |\n",
            "|    policy_gradient_loss | -0.000506     |\n",
            "|    value_loss           | 27            |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 246      |\n",
            "|    ep_rew_mean     | 265      |\n",
            "| time/              |          |\n",
            "|    fps             | 1023     |\n",
            "|    iterations      | 79       |\n",
            "|    time_elapsed    | 39       |\n",
            "|    total_timesteps | 40448    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 246          |\n",
            "|    ep_rew_mean          | 262          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1023         |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015503515 |\n",
            "|    clip_fraction        | 0.0395       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.55        |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.83         |\n",
            "|    n_updates            | 1510         |\n",
            "|    policy_gradient_loss | -0.00349     |\n",
            "|    value_loss           | 4.1          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 247          |\n",
            "|    ep_rew_mean          | 262          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1024         |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 41472        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007418017 |\n",
            "|    clip_fraction        | 0.0209       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.501       |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.791        |\n",
            "|    n_updates            | 1520         |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    value_loss           | 83.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 243          |\n",
            "|    ep_rew_mean          | 261          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1026         |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 40           |\n",
            "|    total_timesteps      | 41984        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008394697 |\n",
            "|    clip_fraction        | 0.0318       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.478       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.67         |\n",
            "|    n_updates            | 1530         |\n",
            "|    policy_gradient_loss | -0.00142     |\n",
            "|    value_loss           | 12.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 243          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1028         |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 41           |\n",
            "|    total_timesteps      | 42496        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011368396 |\n",
            "|    clip_fraction        | 0.0158       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.564       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.03         |\n",
            "|    n_updates            | 1540         |\n",
            "|    policy_gradient_loss | -0.00116     |\n",
            "|    value_loss           | 4.06         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 243          |\n",
            "|    ep_rew_mean          | 263          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1030         |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 41           |\n",
            "|    total_timesteps      | 43008        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025029944 |\n",
            "|    clip_fraction        | 0.107        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.774       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.26         |\n",
            "|    n_updates            | 1550         |\n",
            "|    policy_gradient_loss | -0.00832     |\n",
            "|    value_loss           | 4.45         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 242           |\n",
            "|    ep_rew_mean          | 257           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1031          |\n",
            "|    iterations           | 85            |\n",
            "|    time_elapsed         | 42            |\n",
            "|    total_timesteps      | 43520         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016462605 |\n",
            "|    clip_fraction        | 0.00254       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.746        |\n",
            "|    explained_variance   | 0.987         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 2.01          |\n",
            "|    n_updates            | 1560          |\n",
            "|    policy_gradient_loss | -0.000979     |\n",
            "|    value_loss           | 8.85          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 242           |\n",
            "|    ep_rew_mean          | 258           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1032          |\n",
            "|    iterations           | 86            |\n",
            "|    time_elapsed         | 42            |\n",
            "|    total_timesteps      | 44032         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.4695764e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.668        |\n",
            "|    explained_variance   | 0.595         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 722           |\n",
            "|    n_updates            | 1570          |\n",
            "|    policy_gradient_loss | 0.000486      |\n",
            "|    value_loss           | 2.59e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 240          |\n",
            "|    ep_rew_mean          | 254          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1033         |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 43           |\n",
            "|    total_timesteps      | 44544        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003943781 |\n",
            "|    clip_fraction        | 0.00605      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.67        |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 16.6         |\n",
            "|    n_updates            | 1580         |\n",
            "|    policy_gradient_loss | -0.00235     |\n",
            "|    value_loss           | 61.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 240          |\n",
            "|    ep_rew_mean          | 254          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1034         |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 43           |\n",
            "|    total_timesteps      | 45056        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.103997e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.502       |\n",
            "|    explained_variance   | 0.825        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 252          |\n",
            "|    n_updates            | 1590         |\n",
            "|    policy_gradient_loss | -7.87e-05    |\n",
            "|    value_loss           | 812          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 240          |\n",
            "|    ep_rew_mean          | 254          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1035         |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 45568        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017931535 |\n",
            "|    clip_fraction        | 0.0564       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.57        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.9          |\n",
            "|    n_updates            | 1600         |\n",
            "|    policy_gradient_loss | -0.00334     |\n",
            "|    value_loss           | 2.17         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 237          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1036         |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 46080        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020258676 |\n",
            "|    clip_fraction        | 0.0928       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.655       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.979        |\n",
            "|    n_updates            | 1610         |\n",
            "|    policy_gradient_loss | -0.00465     |\n",
            "|    value_loss           | 2.98         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 229          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1037         |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 44           |\n",
            "|    total_timesteps      | 46592        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017160041 |\n",
            "|    clip_fraction        | 0.0682       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.546       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.38         |\n",
            "|    n_updates            | 1620         |\n",
            "|    policy_gradient_loss | -0.00368     |\n",
            "|    value_loss           | 4.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 229          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1039         |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 45           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011939516 |\n",
            "|    clip_fraction        | 0.068        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.533       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.52         |\n",
            "|    n_updates            | 1630         |\n",
            "|    policy_gradient_loss | -0.00303     |\n",
            "|    value_loss           | 4.83         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 229          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1040         |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 45           |\n",
            "|    total_timesteps      | 47616        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012674967 |\n",
            "|    clip_fraction        | 0.0332       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.546       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.981        |\n",
            "|    n_updates            | 1640         |\n",
            "|    policy_gradient_loss | -0.00222     |\n",
            "|    value_loss           | 3.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 227          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1042         |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 46           |\n",
            "|    total_timesteps      | 48128        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011101388 |\n",
            "|    clip_fraction        | 0.0236       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.613       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.76         |\n",
            "|    n_updates            | 1650         |\n",
            "|    policy_gradient_loss | -0.00125     |\n",
            "|    value_loss           | 4.57         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 227           |\n",
            "|    ep_rew_mean          | 253           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1044          |\n",
            "|    iterations           | 95            |\n",
            "|    time_elapsed         | 46            |\n",
            "|    total_timesteps      | 48640         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033734576 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.608        |\n",
            "|    explained_variance   | 0.836         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 432           |\n",
            "|    n_updates            | 1660          |\n",
            "|    policy_gradient_loss | -0.000915     |\n",
            "|    value_loss           | 580           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 227          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1045         |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 47           |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005153179 |\n",
            "|    clip_fraction        | 0.00371      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.682       |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 4.87         |\n",
            "|    n_updates            | 1670         |\n",
            "|    policy_gradient_loss | -0.00103     |\n",
            "|    value_loss           | 17           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 227          |\n",
            "|    ep_rew_mean          | 250          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1045         |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 47           |\n",
            "|    total_timesteps      | 49664        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007127678 |\n",
            "|    clip_fraction        | 0.0139       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.645       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 1680         |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 7.09         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=269.01 +/- 17.80\n",
            "Episode length: 220.20 +/- 17.50\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 220           |\n",
            "|    mean_reward          | 269           |\n",
            "| time/                   |               |\n",
            "|    total timesteps      | 50000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.2690694e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.695        |\n",
            "|    explained_variance   | 0.822         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 159           |\n",
            "|    n_updates            | 1690          |\n",
            "|    policy_gradient_loss | -0.00019      |\n",
            "|    value_loss           | 653           |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 227      |\n",
            "|    ep_rew_mean     | 250      |\n",
            "| time/              |          |\n",
            "|    fps             | 1022     |\n",
            "|    iterations      | 98       |\n",
            "|    time_elapsed    | 49       |\n",
            "|    total_timesteps | 50176    |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 226         |\n",
            "|    ep_rew_mean          | 247         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1023        |\n",
            "|    iterations           | 99          |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 50688       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001738249 |\n",
            "|    clip_fraction        | 0.049       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.711      |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 1.45        |\n",
            "|    n_updates            | 1700        |\n",
            "|    policy_gradient_loss | -0.00315    |\n",
            "|    value_loss           | 9.79        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 225           |\n",
            "|    ep_rew_mean          | 247           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1025          |\n",
            "|    iterations           | 100           |\n",
            "|    time_elapsed         | 49            |\n",
            "|    total_timesteps      | 51200         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00036579627 |\n",
            "|    clip_fraction        | 0.00332       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.522        |\n",
            "|    explained_variance   | 0.875         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 667           |\n",
            "|    n_updates            | 1710          |\n",
            "|    policy_gradient_loss | -0.000911     |\n",
            "|    value_loss           | 486           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 247          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1027         |\n",
            "|    iterations           | 101          |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 51712        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018813728 |\n",
            "|    clip_fraction        | 0.0938       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.631       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.496        |\n",
            "|    n_updates            | 1720         |\n",
            "|    policy_gradient_loss | -0.00431     |\n",
            "|    value_loss           | 1.81         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 246          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1028         |\n",
            "|    iterations           | 102          |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 52224        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020162752 |\n",
            "|    clip_fraction        | 0.0975       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.692       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.634        |\n",
            "|    n_updates            | 1730         |\n",
            "|    policy_gradient_loss | -0.00678     |\n",
            "|    value_loss           | 2.56         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 224           |\n",
            "|    ep_rew_mean          | 247           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1029          |\n",
            "|    iterations           | 103           |\n",
            "|    time_elapsed         | 51            |\n",
            "|    total_timesteps      | 52736         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00096653355 |\n",
            "|    clip_fraction        | 0.0434        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.625        |\n",
            "|    explained_variance   | 0.999         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 0.904         |\n",
            "|    n_updates            | 1740          |\n",
            "|    policy_gradient_loss | -0.00383      |\n",
            "|    value_loss           | 2.34          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 245          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1030         |\n",
            "|    iterations           | 104          |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014180809 |\n",
            "|    clip_fraction        | 0.0449       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.54        |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.22         |\n",
            "|    n_updates            | 1750         |\n",
            "|    policy_gradient_loss | -0.00225     |\n",
            "|    value_loss           | 6.65         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 245          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1031         |\n",
            "|    iterations           | 105          |\n",
            "|    time_elapsed         | 52           |\n",
            "|    total_timesteps      | 53760        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005816767 |\n",
            "|    clip_fraction        | 0.00918      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.507       |\n",
            "|    explained_variance   | 0.914        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.11         |\n",
            "|    n_updates            | 1760         |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    value_loss           | 325          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 218          |\n",
            "|    ep_rew_mean          | 249          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1032         |\n",
            "|    iterations           | 106          |\n",
            "|    time_elapsed         | 52           |\n",
            "|    total_timesteps      | 54272        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010142548 |\n",
            "|    clip_fraction        | 0.0467       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.541       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.26         |\n",
            "|    n_updates            | 1770         |\n",
            "|    policy_gradient_loss | -0.00544     |\n",
            "|    value_loss           | 5.98         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 217          |\n",
            "|    ep_rew_mean          | 249          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1033         |\n",
            "|    iterations           | 107          |\n",
            "|    time_elapsed         | 53           |\n",
            "|    total_timesteps      | 54784        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010248084 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.635       |\n",
            "|    explained_variance   | 0.99         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.56         |\n",
            "|    n_updates            | 1780         |\n",
            "|    policy_gradient_loss | -0.00625     |\n",
            "|    value_loss           | 13.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | 246         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1033        |\n",
            "|    iterations           | 108         |\n",
            "|    time_elapsed         | 53          |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001087589 |\n",
            "|    clip_fraction        | 0.0234      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 4.58        |\n",
            "|    n_updates            | 1790        |\n",
            "|    policy_gradient_loss | -0.00334    |\n",
            "|    value_loss           | 21.2        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 215           |\n",
            "|    ep_rew_mean          | 242           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1033          |\n",
            "|    iterations           | 109           |\n",
            "|    time_elapsed         | 53            |\n",
            "|    total_timesteps      | 55808         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012056739 |\n",
            "|    clip_fraction        | 0.000586      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.507        |\n",
            "|    explained_variance   | 0.852         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 3.31          |\n",
            "|    n_updates            | 1800          |\n",
            "|    policy_gradient_loss | -0.000306     |\n",
            "|    value_loss           | 470           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 216           |\n",
            "|    ep_rew_mean          | 245           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 1034          |\n",
            "|    iterations           | 110           |\n",
            "|    time_elapsed         | 54            |\n",
            "|    total_timesteps      | 56320         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030364224 |\n",
            "|    clip_fraction        | 0.0129        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.589        |\n",
            "|    explained_variance   | 0.866         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 768           |\n",
            "|    n_updates            | 1810          |\n",
            "|    policy_gradient_loss | -0.000539     |\n",
            "|    value_loss           | 754           |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | 245         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 1030        |\n",
            "|    iterations           | 111         |\n",
            "|    time_elapsed         | 55          |\n",
            "|    total_timesteps      | 56832       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001390145 |\n",
            "|    clip_fraction        | 0.0367      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.585      |\n",
            "|    explained_variance   | 0.998       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 1.07        |\n",
            "|    n_updates            | 1820        |\n",
            "|    policy_gradient_loss | -0.0027     |\n",
            "|    value_loss           | 3.07        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | 245          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 1008         |\n",
            "|    iterations           | 112          |\n",
            "|    time_elapsed         | 56           |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014870559 |\n",
            "|    clip_fraction        | 0.059        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.678       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.481        |\n",
            "|    n_updates            | 1830         |\n",
            "|    policy_gradient_loss | -0.00346     |\n",
            "|    value_loss           | 2.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | 244          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 982          |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 58           |\n",
            "|    total_timesteps      | 57856        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025695837 |\n",
            "|    clip_fraction        | 0.114        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.725       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.39         |\n",
            "|    n_updates            | 1840         |\n",
            "|    policy_gradient_loss | -0.00641     |\n",
            "|    value_loss           | 4.23         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 216         |\n",
            "|    ep_rew_mean          | 245         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 957         |\n",
            "|    iterations           | 114         |\n",
            "|    time_elapsed         | 60          |\n",
            "|    total_timesteps      | 58368       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001198262 |\n",
            "|    clip_fraction        | 0.0223      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.653      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 2.02        |\n",
            "|    n_updates            | 1850        |\n",
            "|    policy_gradient_loss | -0.00279    |\n",
            "|    value_loss           | 5.6         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | 244          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 937          |\n",
            "|    iterations           | 115          |\n",
            "|    time_elapsed         | 62           |\n",
            "|    total_timesteps      | 58880        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012679731 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.442       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.923        |\n",
            "|    n_updates            | 1860         |\n",
            "|    policy_gradient_loss | -0.00217     |\n",
            "|    value_loss           | 3.04         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | 244          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 934          |\n",
            "|    iterations           | 116          |\n",
            "|    time_elapsed         | 63           |\n",
            "|    total_timesteps      | 59392        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010114762 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.482       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.651        |\n",
            "|    n_updates            | 1870         |\n",
            "|    policy_gradient_loss | -0.00221     |\n",
            "|    value_loss           | 2.23         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 214          |\n",
            "|    ep_rew_mean          | 244          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 934          |\n",
            "|    iterations           | 117          |\n",
            "|    time_elapsed         | 64           |\n",
            "|    total_timesteps      | 59904        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018007162 |\n",
            "|    clip_fraction        | 0.0752       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.546       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.59         |\n",
            "|    n_updates            | 1880         |\n",
            "|    policy_gradient_loss | -0.00504     |\n",
            "|    value_loss           | 1.78         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=285.68 +/- 21.32\n",
            "Episode length: 213.60 +/- 14.92\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 214          |\n",
            "|    mean_reward          | 286          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 60000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010413645 |\n",
            "|    clip_fraction        | 0.0568       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.673       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.781        |\n",
            "|    n_updates            | 1890         |\n",
            "|    policy_gradient_loss | -0.00315     |\n",
            "|    value_loss           | 2.66         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 215      |\n",
            "|    ep_rew_mean     | 245      |\n",
            "| time/              |          |\n",
            "|    fps             | 916      |\n",
            "|    iterations      | 118      |\n",
            "|    time_elapsed    | 65       |\n",
            "|    total_timesteps | 60416    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 215          |\n",
            "|    ep_rew_mean          | 245          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 914          |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 66           |\n",
            "|    total_timesteps      | 60928        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024726694 |\n",
            "|    clip_fraction        | 0.074        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.702       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.751        |\n",
            "|    n_updates            | 1900         |\n",
            "|    policy_gradient_loss | -0.00563     |\n",
            "|    value_loss           | 1.72         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 214           |\n",
            "|    ep_rew_mean          | 245           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 913           |\n",
            "|    iterations           | 120           |\n",
            "|    time_elapsed         | 67            |\n",
            "|    total_timesteps      | 61440         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00073689746 |\n",
            "|    clip_fraction        | 0.0189        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.647        |\n",
            "|    explained_variance   | 0.998         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 1.04          |\n",
            "|    n_updates            | 1910          |\n",
            "|    policy_gradient_loss | -0.0027       |\n",
            "|    value_loss           | 3.45          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 221           |\n",
            "|    ep_rew_mean          | 242           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 914           |\n",
            "|    iterations           | 121           |\n",
            "|    time_elapsed         | 67            |\n",
            "|    total_timesteps      | 61952         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00057222624 |\n",
            "|    clip_fraction        | 0.0145        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.524        |\n",
            "|    explained_variance   | 0.996         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 2.25          |\n",
            "|    n_updates            | 1920          |\n",
            "|    policy_gradient_loss | -0.00204      |\n",
            "|    value_loss           | 7.37          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 221           |\n",
            "|    ep_rew_mean          | 245           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 916           |\n",
            "|    iterations           | 122           |\n",
            "|    time_elapsed         | 68            |\n",
            "|    total_timesteps      | 62464         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018765987 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.498        |\n",
            "|    explained_variance   | 0.886         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 239           |\n",
            "|    n_updates            | 1930          |\n",
            "|    policy_gradient_loss | -0.000389     |\n",
            "|    value_loss           | 564           |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 221        |\n",
            "|    ep_rew_mean          | 245        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 917        |\n",
            "|    iterations           | 123        |\n",
            "|    time_elapsed         | 68         |\n",
            "|    total_timesteps      | 62976      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00088809 |\n",
            "|    clip_fraction        | 0.0547     |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -0.602     |\n",
            "|    explained_variance   | 0.996      |\n",
            "|    learning_rate        | 0.000116   |\n",
            "|    loss                 | 8.75       |\n",
            "|    n_updates            | 1940       |\n",
            "|    policy_gradient_loss | -0.00367   |\n",
            "|    value_loss           | 14         |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 222          |\n",
            "|    ep_rew_mean          | 245          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 919          |\n",
            "|    iterations           | 124          |\n",
            "|    time_elapsed         | 69           |\n",
            "|    total_timesteps      | 63488        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011550618 |\n",
            "|    clip_fraction        | 0.0289       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.664       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.498        |\n",
            "|    n_updates            | 1950         |\n",
            "|    policy_gradient_loss | -0.00218     |\n",
            "|    value_loss           | 2.67         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 221         |\n",
            "|    ep_rew_mean          | 245         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 920         |\n",
            "|    iterations           | 125         |\n",
            "|    time_elapsed         | 69          |\n",
            "|    total_timesteps      | 64000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001709075 |\n",
            "|    clip_fraction        | 0.073       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.739      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 0.541       |\n",
            "|    n_updates            | 1960        |\n",
            "|    policy_gradient_loss | -0.00515    |\n",
            "|    value_loss           | 1.36        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | 243          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 126          |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 64512        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020097555 |\n",
            "|    clip_fraction        | 0.0451       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.7         |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.361        |\n",
            "|    n_updates            | 1970         |\n",
            "|    policy_gradient_loss | -0.00396     |\n",
            "|    value_loss           | 1.82         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | 244          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 922          |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 65024        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.157439e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.602       |\n",
            "|    explained_variance   | 0.793        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 304          |\n",
            "|    n_updates            | 1980         |\n",
            "|    policy_gradient_loss | -0.000141    |\n",
            "|    value_loss           | 710          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 221          |\n",
            "|    ep_rew_mean          | 244          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 923          |\n",
            "|    iterations           | 128          |\n",
            "|    time_elapsed         | 70           |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008993285 |\n",
            "|    clip_fraction        | 0.0135       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.639       |\n",
            "|    explained_variance   | 0.994        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 5.3          |\n",
            "|    n_updates            | 1990         |\n",
            "|    policy_gradient_loss | -0.00104     |\n",
            "|    value_loss           | 13.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 250          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 924          |\n",
            "|    iterations           | 129          |\n",
            "|    time_elapsed         | 71           |\n",
            "|    total_timesteps      | 66048        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010273289 |\n",
            "|    clip_fraction        | 0.0283       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.559       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.63         |\n",
            "|    n_updates            | 2000         |\n",
            "|    policy_gradient_loss | -0.00151     |\n",
            "|    value_loss           | 5.08         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 925          |\n",
            "|    iterations           | 130          |\n",
            "|    time_elapsed         | 71           |\n",
            "|    total_timesteps      | 66560        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018450472 |\n",
            "|    clip_fraction        | 0.0564       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.521       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.578        |\n",
            "|    n_updates            | 2010         |\n",
            "|    policy_gradient_loss | -0.00588     |\n",
            "|    value_loss           | 2.19         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 926          |\n",
            "|    iterations           | 131          |\n",
            "|    time_elapsed         | 72           |\n",
            "|    total_timesteps      | 67072        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027773366 |\n",
            "|    clip_fraction        | 0.0941       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.765       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.518        |\n",
            "|    n_updates            | 2020         |\n",
            "|    policy_gradient_loss | -0.00632     |\n",
            "|    value_loss           | 1.48         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 927          |\n",
            "|    iterations           | 132          |\n",
            "|    time_elapsed         | 72           |\n",
            "|    total_timesteps      | 67584        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.813987e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.698       |\n",
            "|    explained_variance   | 0.833        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 99.8         |\n",
            "|    n_updates            | 2030         |\n",
            "|    policy_gradient_loss | -0.000752    |\n",
            "|    value_loss           | 691          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 224           |\n",
            "|    ep_rew_mean          | 253           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 928           |\n",
            "|    iterations           | 133           |\n",
            "|    time_elapsed         | 73            |\n",
            "|    total_timesteps      | 68096         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1124335e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.591        |\n",
            "|    explained_variance   | 0.925         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 231           |\n",
            "|    n_updates            | 2040          |\n",
            "|    policy_gradient_loss | -0.000621     |\n",
            "|    value_loss           | 284           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 929          |\n",
            "|    iterations           | 134          |\n",
            "|    time_elapsed         | 73           |\n",
            "|    total_timesteps      | 68608        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011344677 |\n",
            "|    clip_fraction        | 0.0209       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.607       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.306        |\n",
            "|    n_updates            | 2050         |\n",
            "|    policy_gradient_loss | -0.00257     |\n",
            "|    value_loss           | 2.51         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 930          |\n",
            "|    iterations           | 135          |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 69120        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022570374 |\n",
            "|    clip_fraction        | 0.0502       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.641       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.48         |\n",
            "|    n_updates            | 2060         |\n",
            "|    policy_gradient_loss | -0.00483     |\n",
            "|    value_loss           | 8.29         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 930          |\n",
            "|    iterations           | 136          |\n",
            "|    time_elapsed         | 74           |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017696318 |\n",
            "|    clip_fraction        | 0.0496       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.619       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.39         |\n",
            "|    n_updates            | 2070         |\n",
            "|    policy_gradient_loss | -0.0035      |\n",
            "|    value_loss           | 4.65         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=268.98 +/- 16.28\n",
            "Episode length: 231.00 +/- 17.40\n",
            "-------------------------------------------\n",
            "| eval/                   |               |\n",
            "|    mean_ep_length       | 231           |\n",
            "|    mean_reward          | 269           |\n",
            "| time/                   |               |\n",
            "|    total timesteps      | 70000         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.8551592e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.61         |\n",
            "|    explained_variance   | 0.917         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 130           |\n",
            "|    n_updates            | 2080          |\n",
            "|    policy_gradient_loss | 0.000166      |\n",
            "|    value_loss           | 271           |\n",
            "-------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 223      |\n",
            "|    ep_rew_mean     | 251      |\n",
            "| time/              |          |\n",
            "|    fps             | 918      |\n",
            "|    iterations      | 137      |\n",
            "|    time_elapsed    | 76       |\n",
            "|    total_timesteps | 70144    |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 224           |\n",
            "|    ep_rew_mean          | 252           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 918           |\n",
            "|    iterations           | 138           |\n",
            "|    time_elapsed         | 76            |\n",
            "|    total_timesteps      | 70656         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00033830828 |\n",
            "|    clip_fraction        | 0.00117       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.619        |\n",
            "|    explained_variance   | 0.991         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 15.7          |\n",
            "|    n_updates            | 2090          |\n",
            "|    policy_gradient_loss | -0.000293     |\n",
            "|    value_loss           | 25.5          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 224          |\n",
            "|    ep_rew_mean          | 252          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 919          |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 71168        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005185356 |\n",
            "|    clip_fraction        | 0.00332      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.654       |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 5.13         |\n",
            "|    n_updates            | 2100         |\n",
            "|    policy_gradient_loss | -0.00224     |\n",
            "|    value_loss           | 37           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 223          |\n",
            "|    ep_rew_mean          | 252          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 919          |\n",
            "|    iterations           | 140          |\n",
            "|    time_elapsed         | 77           |\n",
            "|    total_timesteps      | 71680        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009670871 |\n",
            "|    clip_fraction        | 0.0328       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.65        |\n",
            "|    explained_variance   | 0.988        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.923        |\n",
            "|    n_updates            | 2110         |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 18.5         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 223         |\n",
            "|    ep_rew_mean          | 252         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 920         |\n",
            "|    iterations           | 141         |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 72192       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 5.89072e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.595      |\n",
            "|    explained_variance   | 0.915       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 81.8        |\n",
            "|    n_updates            | 2120        |\n",
            "|    policy_gradient_loss | -0.000228   |\n",
            "|    value_loss           | 278         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 224         |\n",
            "|    ep_rew_mean          | 251         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 920         |\n",
            "|    iterations           | 142         |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 72704       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000931256 |\n",
            "|    clip_fraction        | 0.00352     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.675      |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 16.7        |\n",
            "|    n_updates            | 2130        |\n",
            "|    policy_gradient_loss | -0.00128    |\n",
            "|    value_loss           | 18.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 225         |\n",
            "|    ep_rew_mean          | 253         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 921         |\n",
            "|    iterations           | 143         |\n",
            "|    time_elapsed         | 79          |\n",
            "|    total_timesteps      | 73216       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000930181 |\n",
            "|    clip_fraction        | 0.0604      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.671      |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 1.92        |\n",
            "|    n_updates            | 2140        |\n",
            "|    policy_gradient_loss | -0.00207    |\n",
            "|    value_loss           | 44.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 253          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 144          |\n",
            "|    time_elapsed         | 79           |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022799019 |\n",
            "|    clip_fraction        | 0.0637       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.743       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.923        |\n",
            "|    n_updates            | 2150         |\n",
            "|    policy_gradient_loss | -0.00316     |\n",
            "|    value_loss           | 6.44         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 145          |\n",
            "|    time_elapsed         | 80           |\n",
            "|    total_timesteps      | 74240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016825922 |\n",
            "|    clip_fraction        | 0.05         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.701       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.43         |\n",
            "|    n_updates            | 2160         |\n",
            "|    policy_gradient_loss | -0.00362     |\n",
            "|    value_loss           | 2.22         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 224         |\n",
            "|    ep_rew_mean          | 254         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 921         |\n",
            "|    iterations           | 146         |\n",
            "|    time_elapsed         | 81          |\n",
            "|    total_timesteps      | 74752       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000763478 |\n",
            "|    clip_fraction        | 0.0277      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.628      |\n",
            "|    explained_variance   | 0.973       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 17.5        |\n",
            "|    n_updates            | 2170        |\n",
            "|    policy_gradient_loss | -0.00308    |\n",
            "|    value_loss           | 40.3        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 225          |\n",
            "|    ep_rew_mean          | 254          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 147          |\n",
            "|    time_elapsed         | 81           |\n",
            "|    total_timesteps      | 75264        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.786097e-05 |\n",
            "|    clip_fraction        | 0.000586     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.569       |\n",
            "|    explained_variance   | 0.822        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 194          |\n",
            "|    n_updates            | 2180         |\n",
            "|    policy_gradient_loss | -0.000899    |\n",
            "|    value_loss           | 631          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 224           |\n",
            "|    ep_rew_mean          | 253           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 921           |\n",
            "|    iterations           | 148           |\n",
            "|    time_elapsed         | 82            |\n",
            "|    total_timesteps      | 75776         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00042400544 |\n",
            "|    clip_fraction        | 0.0123        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.48         |\n",
            "|    explained_variance   | 0.994         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 1.98          |\n",
            "|    n_updates            | 2190          |\n",
            "|    policy_gradient_loss | -0.00209      |\n",
            "|    value_loss           | 13.3          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 232          |\n",
            "|    ep_rew_mean          | 251          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 922          |\n",
            "|    iterations           | 149          |\n",
            "|    time_elapsed         | 82           |\n",
            "|    total_timesteps      | 76288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001238616 |\n",
            "|    clip_fraction        | 0.000977     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.554       |\n",
            "|    explained_variance   | 0.967        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.42         |\n",
            "|    n_updates            | 2200         |\n",
            "|    policy_gradient_loss | -0.000595    |\n",
            "|    value_loss           | 173          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 232           |\n",
            "|    ep_rew_mean          | 251           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 922           |\n",
            "|    iterations           | 150           |\n",
            "|    time_elapsed         | 83            |\n",
            "|    total_timesteps      | 76800         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00075518654 |\n",
            "|    clip_fraction        | 0.0271        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.742        |\n",
            "|    explained_variance   | 0.974         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 57.2          |\n",
            "|    n_updates            | 2210          |\n",
            "|    policy_gradient_loss | -0.00135      |\n",
            "|    value_loss           | 134           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 232          |\n",
            "|    ep_rew_mean          | 251          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 923          |\n",
            "|    iterations           | 151          |\n",
            "|    time_elapsed         | 83           |\n",
            "|    total_timesteps      | 77312        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016581294 |\n",
            "|    clip_fraction        | 0.0213       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.684       |\n",
            "|    explained_variance   | 0.99         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 8.41         |\n",
            "|    n_updates            | 2220         |\n",
            "|    policy_gradient_loss | -0.00241     |\n",
            "|    value_loss           | 21.1         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 232           |\n",
            "|    ep_rew_mean          | 251           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 924           |\n",
            "|    iterations           | 152           |\n",
            "|    time_elapsed         | 84            |\n",
            "|    total_timesteps      | 77824         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015619758 |\n",
            "|    clip_fraction        | 0.000195      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.692        |\n",
            "|    explained_variance   | 0.936         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 7.29          |\n",
            "|    n_updates            | 2230          |\n",
            "|    policy_gradient_loss | -0.000459     |\n",
            "|    value_loss           | 157           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 231           |\n",
            "|    ep_rew_mean          | 249           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 924           |\n",
            "|    iterations           | 153           |\n",
            "|    time_elapsed         | 84            |\n",
            "|    total_timesteps      | 78336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00069744873 |\n",
            "|    clip_fraction        | 0.00918       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.613        |\n",
            "|    explained_variance   | 0.987         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 15.6          |\n",
            "|    n_updates            | 2240          |\n",
            "|    policy_gradient_loss | -0.00211      |\n",
            "|    value_loss           | 37.2          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 232           |\n",
            "|    ep_rew_mean          | 250           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 925           |\n",
            "|    iterations           | 154           |\n",
            "|    time_elapsed         | 85            |\n",
            "|    total_timesteps      | 78848         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013234175 |\n",
            "|    clip_fraction        | 0.00176       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.543        |\n",
            "|    explained_variance   | 0.954         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 43.4          |\n",
            "|    n_updates            | 2250          |\n",
            "|    policy_gradient_loss | -0.00101      |\n",
            "|    value_loss           | 140           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 241          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 925          |\n",
            "|    iterations           | 155          |\n",
            "|    time_elapsed         | 85           |\n",
            "|    total_timesteps      | 79360        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005068162 |\n",
            "|    clip_fraction        | 0.00898      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.489       |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 8.65         |\n",
            "|    n_updates            | 2260         |\n",
            "|    policy_gradient_loss | -0.000949    |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 241           |\n",
            "|    ep_rew_mean          | 254           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 926           |\n",
            "|    iterations           | 156           |\n",
            "|    time_elapsed         | 86            |\n",
            "|    total_timesteps      | 79872         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00061247183 |\n",
            "|    clip_fraction        | 0.0121        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.576        |\n",
            "|    explained_variance   | 0.978         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 79.1          |\n",
            "|    n_updates            | 2270          |\n",
            "|    policy_gradient_loss | -0.00133      |\n",
            "|    value_loss           | 89.2          |\n",
            "-------------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=269.45 +/- 13.93\n",
            "Episode length: 222.40 +/- 17.56\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 222          |\n",
            "|    mean_reward          | 269          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 80000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010366974 |\n",
            "|    clip_fraction        | 0.00977      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.695       |\n",
            "|    explained_variance   | 0.97         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 13.9         |\n",
            "|    n_updates            | 2280         |\n",
            "|    policy_gradient_loss | -0.000874    |\n",
            "|    value_loss           | 103          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 241      |\n",
            "|    ep_rew_mean     | 254      |\n",
            "| time/              |          |\n",
            "|    fps             | 915      |\n",
            "|    iterations      | 157      |\n",
            "|    time_elapsed    | 87       |\n",
            "|    total_timesteps | 80384    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 241          |\n",
            "|    ep_rew_mean          | 254          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 916          |\n",
            "|    iterations           | 158          |\n",
            "|    time_elapsed         | 88           |\n",
            "|    total_timesteps      | 80896        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013936097 |\n",
            "|    clip_fraction        | 0.0279       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.572       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 83           |\n",
            "|    n_updates            | 2290         |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    value_loss           | 94.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 241          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 916          |\n",
            "|    iterations           | 159          |\n",
            "|    time_elapsed         | 88           |\n",
            "|    total_timesteps      | 81408        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016954604 |\n",
            "|    clip_fraction        | 0.0412       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.547       |\n",
            "|    explained_variance   | 0.993        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 4.33         |\n",
            "|    n_updates            | 2300         |\n",
            "|    policy_gradient_loss | -0.004       |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 241          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 917          |\n",
            "|    iterations           | 160          |\n",
            "|    time_elapsed         | 89           |\n",
            "|    total_timesteps      | 81920        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002473879 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.697       |\n",
            "|    explained_variance   | 0.955        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 21.4         |\n",
            "|    n_updates            | 2310         |\n",
            "|    policy_gradient_loss | -0.000571    |\n",
            "|    value_loss           | 98           |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 241           |\n",
            "|    ep_rew_mean          | 255           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 916           |\n",
            "|    iterations           | 161           |\n",
            "|    time_elapsed         | 89            |\n",
            "|    total_timesteps      | 82432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00069828297 |\n",
            "|    clip_fraction        | 0.0107        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.497        |\n",
            "|    explained_variance   | 0.989         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 4.33          |\n",
            "|    n_updates            | 2320          |\n",
            "|    policy_gradient_loss | -0.00249      |\n",
            "|    value_loss           | 26.5          |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 246           |\n",
            "|    ep_rew_mean          | 255           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 917           |\n",
            "|    iterations           | 162           |\n",
            "|    time_elapsed         | 90            |\n",
            "|    total_timesteps      | 82944         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00053983217 |\n",
            "|    clip_fraction        | 0.0152        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.48         |\n",
            "|    explained_variance   | 0.926         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 63.4          |\n",
            "|    n_updates            | 2330          |\n",
            "|    policy_gradient_loss | -4.41e-05     |\n",
            "|    value_loss           | 140           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 246          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 917          |\n",
            "|    iterations           | 163          |\n",
            "|    time_elapsed         | 90           |\n",
            "|    total_timesteps      | 83456        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017494102 |\n",
            "|    clip_fraction        | 0.0488       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.52        |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.06         |\n",
            "|    n_updates            | 2340         |\n",
            "|    policy_gradient_loss | -0.0066      |\n",
            "|    value_loss           | 26.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 247          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 918          |\n",
            "|    iterations           | 164          |\n",
            "|    time_elapsed         | 91           |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018714132 |\n",
            "|    clip_fraction        | 0.0385       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.74        |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.64         |\n",
            "|    n_updates            | 2350         |\n",
            "|    policy_gradient_loss | -0.00436     |\n",
            "|    value_loss           | 13.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 247          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 919          |\n",
            "|    iterations           | 165          |\n",
            "|    time_elapsed         | 91           |\n",
            "|    total_timesteps      | 84480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013331384 |\n",
            "|    clip_fraction        | 0.0576       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.644       |\n",
            "|    explained_variance   | 0.99         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.61         |\n",
            "|    n_updates            | 2360         |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 22.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 247          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 920          |\n",
            "|    iterations           | 166          |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 84992        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009851694 |\n",
            "|    clip_fraction        | 0.0191       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.631       |\n",
            "|    explained_variance   | 0.992        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 4.91         |\n",
            "|    n_updates            | 2370         |\n",
            "|    policy_gradient_loss | -0.00186     |\n",
            "|    value_loss           | 12.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 247          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 167          |\n",
            "|    time_elapsed         | 92           |\n",
            "|    total_timesteps      | 85504        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008327671 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.654       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.01         |\n",
            "|    n_updates            | 2380         |\n",
            "|    policy_gradient_loss | -0.00154     |\n",
            "|    value_loss           | 4.51         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 247          |\n",
            "|    ep_rew_mean          | 255          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 168          |\n",
            "|    time_elapsed         | 93           |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017055692 |\n",
            "|    clip_fraction        | 0.0408       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.58        |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.81         |\n",
            "|    n_updates            | 2390         |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    value_loss           | 8.7          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 240          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 922          |\n",
            "|    iterations           | 169          |\n",
            "|    time_elapsed         | 93           |\n",
            "|    total_timesteps      | 86528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007732366 |\n",
            "|    clip_fraction        | 0.0115       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.445       |\n",
            "|    explained_variance   | 0.995        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.03         |\n",
            "|    n_updates            | 2400         |\n",
            "|    policy_gradient_loss | -0.00132     |\n",
            "|    value_loss           | 5.38         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 240           |\n",
            "|    ep_rew_mean          | 257           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 923           |\n",
            "|    iterations           | 170           |\n",
            "|    time_elapsed         | 94            |\n",
            "|    total_timesteps      | 87040         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00096837885 |\n",
            "|    clip_fraction        | 0.0367        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.544        |\n",
            "|    explained_variance   | 0.972         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 1.26          |\n",
            "|    n_updates            | 2410          |\n",
            "|    policy_gradient_loss | -0.00661      |\n",
            "|    value_loss           | 31            |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 240          |\n",
            "|    ep_rew_mean          | 257          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 923          |\n",
            "|    iterations           | 171          |\n",
            "|    time_elapsed         | 94           |\n",
            "|    total_timesteps      | 87552        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020312837 |\n",
            "|    clip_fraction        | 0.0779       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.608       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.22         |\n",
            "|    n_updates            | 2420         |\n",
            "|    policy_gradient_loss | -0.00559     |\n",
            "|    value_loss           | 6.4          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 257          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 924          |\n",
            "|    iterations           | 172          |\n",
            "|    time_elapsed         | 95           |\n",
            "|    total_timesteps      | 88064        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017543831 |\n",
            "|    clip_fraction        | 0.0512       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.606       |\n",
            "|    explained_variance   | 0.996        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 4.31         |\n",
            "|    n_updates            | 2430         |\n",
            "|    policy_gradient_loss | -0.00275     |\n",
            "|    value_loss           | 12.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 239          |\n",
            "|    ep_rew_mean          | 259          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 925          |\n",
            "|    iterations           | 173          |\n",
            "|    time_elapsed         | 95           |\n",
            "|    total_timesteps      | 88576        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016283157 |\n",
            "|    clip_fraction        | 0.0471       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.57        |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.12         |\n",
            "|    n_updates            | 2440         |\n",
            "|    policy_gradient_loss | -0.00273     |\n",
            "|    value_loss           | 6.12         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 238          |\n",
            "|    ep_rew_mean          | 257          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 925          |\n",
            "|    iterations           | 174          |\n",
            "|    time_elapsed         | 96           |\n",
            "|    total_timesteps      | 89088        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012195904 |\n",
            "|    clip_fraction        | 0.0416       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.673       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 4.64         |\n",
            "|    n_updates            | 2450         |\n",
            "|    policy_gradient_loss | -0.00442     |\n",
            "|    value_loss           | 11.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 238          |\n",
            "|    ep_rew_mean          | 257          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 926          |\n",
            "|    iterations           | 175          |\n",
            "|    time_elapsed         | 96           |\n",
            "|    total_timesteps      | 89600        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010829564 |\n",
            "|    clip_fraction        | 0.0422       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.609       |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 4.85         |\n",
            "|    n_updates            | 2460         |\n",
            "|    policy_gradient_loss | -0.00479     |\n",
            "|    value_loss           | 93.4         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=286.09 +/- 16.75\n",
            "Episode length: 242.20 +/- 25.95\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 242          |\n",
            "|    mean_reward          | 286          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 90000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033155957 |\n",
            "|    clip_fraction        | 0.0682       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.587       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.645        |\n",
            "|    n_updates            | 2470         |\n",
            "|    policy_gradient_loss | -0.00325     |\n",
            "|    value_loss           | 5.59         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 237      |\n",
            "|    ep_rew_mean     | 256      |\n",
            "| time/              |          |\n",
            "|    fps             | 915      |\n",
            "|    iterations      | 176      |\n",
            "|    time_elapsed    | 98       |\n",
            "|    total_timesteps | 90112    |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 237          |\n",
            "|    ep_rew_mean          | 256          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 916          |\n",
            "|    iterations           | 177          |\n",
            "|    time_elapsed         | 98           |\n",
            "|    total_timesteps      | 90624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007051609 |\n",
            "|    clip_fraction        | 0.0186       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.604       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 2.13         |\n",
            "|    n_updates            | 2480         |\n",
            "|    policy_gradient_loss | -0.00238     |\n",
            "|    value_loss           | 6.39         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 236         |\n",
            "|    ep_rew_mean          | 254         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 916         |\n",
            "|    iterations           | 178         |\n",
            "|    time_elapsed         | 99          |\n",
            "|    total_timesteps      | 91136       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001523957 |\n",
            "|    clip_fraction        | 0.0447      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.533      |\n",
            "|    explained_variance   | 0.999       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 0.968       |\n",
            "|    n_updates            | 2490        |\n",
            "|    policy_gradient_loss | -0.00458    |\n",
            "|    value_loss           | 4.55        |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 245           |\n",
            "|    ep_rew_mean          | 257           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 916           |\n",
            "|    iterations           | 179           |\n",
            "|    time_elapsed         | 99            |\n",
            "|    total_timesteps      | 91648         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.3511467e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.529        |\n",
            "|    explained_variance   | 0.885         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 711           |\n",
            "|    n_updates            | 2500          |\n",
            "|    policy_gradient_loss | 0.00047       |\n",
            "|    value_loss           | 637           |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 917          |\n",
            "|    iterations           | 180          |\n",
            "|    time_elapsed         | 100          |\n",
            "|    total_timesteps      | 92160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011788221 |\n",
            "|    clip_fraction        | 0.0266       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.624       |\n",
            "|    explained_variance   | 0.99         |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 33.4         |\n",
            "|    n_updates            | 2510         |\n",
            "|    policy_gradient_loss | -0.00276     |\n",
            "|    value_loss           | 44.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 917          |\n",
            "|    iterations           | 181          |\n",
            "|    time_elapsed         | 100          |\n",
            "|    total_timesteps      | 92672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022813792 |\n",
            "|    clip_fraction        | 0.0598       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.665       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 3.23         |\n",
            "|    n_updates            | 2520         |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    value_loss           | 7.82         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 918          |\n",
            "|    iterations           | 182          |\n",
            "|    time_elapsed         | 101          |\n",
            "|    total_timesteps      | 93184        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033184523 |\n",
            "|    clip_fraction        | 0.123        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.581       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.15         |\n",
            "|    n_updates            | 2530         |\n",
            "|    policy_gradient_loss | -0.00459     |\n",
            "|    value_loss           | 8.82         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 258          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 918          |\n",
            "|    iterations           | 183          |\n",
            "|    time_elapsed         | 101          |\n",
            "|    total_timesteps      | 93696        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028945883 |\n",
            "|    clip_fraction        | 0.0674       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.554       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.45         |\n",
            "|    n_updates            | 2540         |\n",
            "|    policy_gradient_loss | -0.00587     |\n",
            "|    value_loss           | 2.77         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 260          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 918          |\n",
            "|    iterations           | 184          |\n",
            "|    time_elapsed         | 102          |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026742034 |\n",
            "|    clip_fraction        | 0.068        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.502       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.03         |\n",
            "|    n_updates            | 2550         |\n",
            "|    policy_gradient_loss | -0.00439     |\n",
            "|    value_loss           | 2.37         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 260          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 919          |\n",
            "|    iterations           | 185          |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 94720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014145335 |\n",
            "|    clip_fraction        | 0.0596       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.405       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.624        |\n",
            "|    n_updates            | 2560         |\n",
            "|    policy_gradient_loss | -0.00298     |\n",
            "|    value_loss           | 3.12         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 262          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 920          |\n",
            "|    iterations           | 186          |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 95232        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013617151 |\n",
            "|    clip_fraction        | 0.0439       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.501       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.508        |\n",
            "|    n_updates            | 2570         |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    value_loss           | 1.32         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 261          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 920          |\n",
            "|    iterations           | 187          |\n",
            "|    time_elapsed         | 103          |\n",
            "|    total_timesteps      | 95744        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016339419 |\n",
            "|    clip_fraction        | 0.0492       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.642       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.627        |\n",
            "|    n_updates            | 2580         |\n",
            "|    policy_gradient_loss | -0.00441     |\n",
            "|    value_loss           | 2.39         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 244          |\n",
            "|    ep_rew_mean          | 262          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 188          |\n",
            "|    time_elapsed         | 104          |\n",
            "|    total_timesteps      | 96256        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015332304 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.686       |\n",
            "|    explained_variance   | 1            |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.241        |\n",
            "|    n_updates            | 2590         |\n",
            "|    policy_gradient_loss | -0.00503     |\n",
            "|    value_loss           | 1.18         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 243          |\n",
            "|    ep_rew_mean          | 262          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 921          |\n",
            "|    iterations           | 189          |\n",
            "|    time_elapsed         | 104          |\n",
            "|    total_timesteps      | 96768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017938901 |\n",
            "|    clip_fraction        | 0.0742       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.59        |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 1.05         |\n",
            "|    n_updates            | 2600         |\n",
            "|    policy_gradient_loss | -0.00563     |\n",
            "|    value_loss           | 3.1          |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 243           |\n",
            "|    ep_rew_mean          | 262           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 922           |\n",
            "|    iterations           | 190           |\n",
            "|    time_elapsed         | 105           |\n",
            "|    total_timesteps      | 97280         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044682832 |\n",
            "|    clip_fraction        | 0.0111        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -0.55         |\n",
            "|    explained_variance   | 0.987         |\n",
            "|    learning_rate        | 0.000116      |\n",
            "|    loss                 | 5.87          |\n",
            "|    n_updates            | 2610          |\n",
            "|    policy_gradient_loss | -0.00135      |\n",
            "|    value_loss           | 32.4          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 244         |\n",
            "|    ep_rew_mean          | 263         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 922         |\n",
            "|    iterations           | 191         |\n",
            "|    time_elapsed         | 106         |\n",
            "|    total_timesteps      | 97792       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000807742 |\n",
            "|    clip_fraction        | 0.0154      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -0.507      |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.000116    |\n",
            "|    loss                 | 16.5        |\n",
            "|    n_updates            | 2620        |\n",
            "|    policy_gradient_loss | -0.00303    |\n",
            "|    value_loss           | 28          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 245          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 922          |\n",
            "|    iterations           | 192          |\n",
            "|    time_elapsed         | 106          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.252729e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.444       |\n",
            "|    explained_variance   | 0.871        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 44.4         |\n",
            "|    n_updates            | 2630         |\n",
            "|    policy_gradient_loss | -0.000323    |\n",
            "|    value_loss           | 683          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 253          |\n",
            "|    ep_rew_mean          | 264          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 922          |\n",
            "|    iterations           | 193          |\n",
            "|    time_elapsed         | 107          |\n",
            "|    total_timesteps      | 98816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016901525 |\n",
            "|    clip_fraction        | 0.0396       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.524       |\n",
            "|    explained_variance   | 0.998        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 17.6         |\n",
            "|    n_updates            | 2640         |\n",
            "|    policy_gradient_loss | -0.00332     |\n",
            "|    value_loss           | 13.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 254          |\n",
            "|    ep_rew_mean          | 265          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 923          |\n",
            "|    iterations           | 194          |\n",
            "|    time_elapsed         | 107          |\n",
            "|    total_timesteps      | 99328        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019285897 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.617       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.809        |\n",
            "|    n_updates            | 2650         |\n",
            "|    policy_gradient_loss | -0.00458     |\n",
            "|    value_loss           | 6.43         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 254          |\n",
            "|    ep_rew_mean          | 265          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 924          |\n",
            "|    iterations           | 195          |\n",
            "|    time_elapsed         | 108          |\n",
            "|    total_timesteps      | 99840        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006544178 |\n",
            "|    clip_fraction        | 0.0516       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.737       |\n",
            "|    explained_variance   | 0.999        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.35         |\n",
            "|    n_updates            | 2660         |\n",
            "|    policy_gradient_loss | -0.00213     |\n",
            "|    value_loss           | 1.37         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=241.26 +/- 86.73\n",
            "Episode length: 229.60 +/- 32.51\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 230          |\n",
            "|    mean_reward          | 241          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 100000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008137794 |\n",
            "|    clip_fraction        | 0.0355       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -0.722       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.000116     |\n",
            "|    loss                 | 0.808        |\n",
            "|    n_updates            | 2670         |\n",
            "|    policy_gradient_loss | -0.00416     |\n",
            "|    value_loss           | 5.11         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 255      |\n",
            "|    ep_rew_mean     | 265      |\n",
            "| time/              |          |\n",
            "|    fps             | 915      |\n",
            "|    iterations      | 196      |\n",
            "|    time_elapsed    | 109      |\n",
            "|    total_timesteps | 100352   |\n",
            "---------------------------------\n",
            "Saving to logs/ppo/LunarLander-v2_7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3NMsPuXnZ-H",
        "outputId": "1dd021ae-bc4f-4560-cf05-60fb304a896b"
      },
      "source": [
        "!python enjoy.py --algo ppo --env LunarLander-v2 --no-render --n-timesteps 1000 --folder logs/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading latest experiment, id=4\n",
            "Loading logs/ppo/LunarLander-v2_4/LunarLander-v2.zip\n",
            "Episode Reward: 218.93\n",
            "Episode Length 329\n",
            "Episode Reward: 281.72\n",
            "Episode Length 335\n",
            "Episode Reward: 215.52\n",
            "Episode Length 325\n",
            "3 Episodes\n",
            "Mean reward: 238.72 +/- 30.43\n",
            "Mean episode length: 329.67 +/- 4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ck1BygFoPEV",
        "outputId": "c1c49cd5-a875-4371-c012-9a3622b43bc4"
      },
      "source": [
        "!python sb3_evaluator.py --algo ppo --env LunarLander-v2 --folder logs/ppo/LunarLander-v2_4/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/rl-baselines3-zoo/data/policies/\n",
            "['LunarLander-v2#ppo#nom1_nom2.zip']\n",
            "LunarLander-v2#ppo#nom1_nom2.zip\n",
            "Hall of fame\n",
            "Environment : LunarLander-v2\n",
            "team:  nom1_nom2  \t \t algo: ppo  \t \t mean score:  247.528189685 std:  30.342655527676392\n",
            "Time : 1mn 46s 565ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmZivHuEwi-s",
        "outputId": "e8ee425e-7176-40cd-eee5-b9107a6f25bd"
      },
      "source": [
        "!python sb3_evaluator.py --algo ppo --env LunarLander-v2 --folder logs/ppo/LunarLander-v2_5/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/rl-baselines3-zoo/data/policies/\n",
            "['LunarLander-v2#ppo#TEST_nom2.zip']\n",
            "LunarLander-v2#ppo#TEST_nom2.zip\n",
            "Hall of fame\n",
            "Environment : LunarLander-v2\n",
            "team:  TEST_nom2  \t \t algo: ppo  \t \t mean score:  267.80452508499997 std:  22.044322989280555\n",
            "Time : 1mn 1s 570ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBk2ppAmKB0H",
        "outputId": "bb24f766-4313-4437-d9ab-e1acbe095ca4"
      },
      "source": [
        "!python sb3_evaluator.py --algo ppo --env LunarLander-v2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/rl-baselines3-zoo/data/policies/\n",
            "['LunarLander-v2#ppo#nom1_newTEST.zip']\n",
            "LunarLander-v2#ppo#nom1_newTEST.zip\n",
            "Hall of fame\n",
            "Environment : LunarLander-v2\n",
            "team:  nom1_newTEST  \t \t algo: ppo  \t \t mean score:  276.72351061 std:  34.85189371147843\n",
            "Time : 54s 807ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zatypKuON7_",
        "outputId": "9d742450-d6d4-4125-fa1e-4033f163c2ba"
      },
      "source": [
        "!python sb3_evaluator.py --algo ppo --env LunarLander-v2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LunarLander-v2#ppo#nom1_new.zip\n",
            "Hall of fame\n",
            "Environment : LunarLander-v2\n",
            "team:  nom1_new  \t \t algo: ppo  \t \t mean score:  278.74926112500003 std:  27.85180712175887\n",
            "Time : 48s 508ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTrRETFA2_Jt",
        "outputId": "4b025804-7dd5-486b-e5eb-0479c87b61a4"
      },
      "source": [
        "!python sb3_evaluator.py --algo ppo --env LunarLander-v2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LunarLander-v2#ppo#nom1_nom.zip\n",
            "Hall of fame\n",
            "Environment : LunarLander-v2\n",
            "team:  nom1_nom  \t \t algo: ppo  \t \t mean score:  275.83186239 std:  29.82184076749405\n",
            "Time : 49s 470ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc0P_c8V5kWy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e6XmjXJ5-7c",
        "outputId": "2ce6dda7-1a65-44a4-8ba2-4164684e472b"
      },
      "source": [
        "!python train.py --algo ppo --env LunarLander-v2 -n 1000000 --eval-freq 10000 --eval-episodes 10 --n-eval-envs 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== LunarLander-v2 ==========\n",
            "Seed: 293841494\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('batch_size', 64),\n",
            "             ('ent_coef', 0.01),\n",
            "             ('gae_lambda', 0.98),\n",
            "             ('gamma', 0.999),\n",
            "             ('n_envs', 16),\n",
            "             ('n_epochs', 4),\n",
            "             ('n_steps', 1024),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=1000000\n",
            "Creating test environment\n",
            "Using cpu device\n",
            "Log path: logs/ppo/LunarLander-v2_9\n",
            "Eval num_timesteps=10000, episode_reward=-648.51 +/- 82.79\n",
            "Episode length: 100.00 +/- 15.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 100      |\n",
            "|    mean_reward     | -649     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 10000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 91.5     |\n",
            "|    ep_rew_mean     | -178     |\n",
            "| time/              |          |\n",
            "|    fps             | 3452     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=-192.24 +/- 58.94\n",
            "Episode length: 67.70 +/- 14.44\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 67.7       |\n",
            "|    mean_reward          | -192       |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 20000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00569051 |\n",
            "|    clip_fraction        | 0.0491     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.38      |\n",
            "|    explained_variance   | -0.00127   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.95e+03   |\n",
            "|    n_updates            | 4          |\n",
            "|    policy_gradient_loss | -0.00513   |\n",
            "|    value_loss           | 4.68e+03   |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=-199.85 +/- 116.87\n",
            "Episode length: 72.10 +/- 10.97\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 72.1     |\n",
            "|    mean_reward     | -200     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 30000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 103      |\n",
            "|    ep_rew_mean     | -141     |\n",
            "| time/              |          |\n",
            "|    fps             | 2355     |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 13       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=98.49 +/- 153.97\n",
            "Episode length: 171.60 +/- 68.24\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 172          |\n",
            "|    mean_reward          | 98.5         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 40000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051393732 |\n",
            "|    clip_fraction        | 0.0211       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | -0.00228     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 642          |\n",
            "|    n_updates            | 8            |\n",
            "|    policy_gradient_loss | -0.00436     |\n",
            "|    value_loss           | 2.07e+03     |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 102      |\n",
            "|    ep_rew_mean     | -127     |\n",
            "| time/              |          |\n",
            "|    fps             | 2112     |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 23       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=34.40 +/- 100.09\n",
            "Episode length: 589.70 +/- 337.34\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 590          |\n",
            "|    mean_reward          | 34.4         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 50000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068469057 |\n",
            "|    clip_fraction        | 0.0361       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.36        |\n",
            "|    explained_variance   | 0.00101      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 599          |\n",
            "|    n_updates            | 12           |\n",
            "|    policy_gradient_loss | -0.0039      |\n",
            "|    value_loss           | 1.4e+03      |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=60000, episode_reward=-3.02 +/- 107.57\n",
            "Episode length: 834.00 +/- 244.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 834      |\n",
            "|    mean_reward     | -3.02    |\n",
            "| time/              |          |\n",
            "|    total timesteps | 60000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 96.1     |\n",
            "|    ep_rew_mean     | -112     |\n",
            "| time/              |          |\n",
            "|    fps             | 1055     |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 62       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=-139.99 +/- 23.91\n",
            "Episode length: 518.40 +/- 97.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 518         |\n",
            "|    mean_reward          | -140        |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 70000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009030951 |\n",
            "|    clip_fraction        | 0.0957      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.33       |\n",
            "|    explained_variance   | -0.00598    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 230         |\n",
            "|    n_updates            | 16          |\n",
            "|    policy_gradient_loss | -0.00477    |\n",
            "|    value_loss           | 706         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=-157.27 +/- 25.38\n",
            "Episode length: 556.20 +/- 83.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 556      |\n",
            "|    mean_reward     | -157     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 80000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 104      |\n",
            "|    ep_rew_mean     | -111     |\n",
            "| time/              |          |\n",
            "|    fps             | 887      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 92       |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=-173.51 +/- 18.22\n",
            "Episode length: 441.70 +/- 92.01\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 442          |\n",
            "|    mean_reward          | -174         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 90000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071278927 |\n",
            "|    clip_fraction        | 0.0819       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | -0.00162     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 308          |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.0052      |\n",
            "|    value_loss           | 848          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 105      |\n",
            "|    ep_rew_mean     | -98.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 927      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 106      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=-168.45 +/- 41.44\n",
            "Episode length: 662.00 +/- 191.76\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 662        |\n",
            "|    mean_reward          | -168       |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 100000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00956407 |\n",
            "|    clip_fraction        | 0.113      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.29      |\n",
            "|    explained_variance   | -0.0373    |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 353        |\n",
            "|    n_updates            | 24         |\n",
            "|    policy_gradient_loss | -0.0065    |\n",
            "|    value_loss           | 766        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=-163.11 +/- 35.02\n",
            "Episode length: 507.30 +/- 93.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 507      |\n",
            "|    mean_reward     | -163     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 110000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 114      |\n",
            "|    ep_rew_mean     | -65.3    |\n",
            "| time/              |          |\n",
            "|    fps             | 857      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 133      |\n",
            "|    total_timesteps | 114688   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=-188.85 +/- 51.10\n",
            "Episode length: 668.10 +/- 209.49\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 668          |\n",
            "|    mean_reward          | -189         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 120000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0071223425 |\n",
            "|    clip_fraction        | 0.0499       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.00406      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 253          |\n",
            "|    n_updates            | 28           |\n",
            "|    policy_gradient_loss | -0.00295     |\n",
            "|    value_loss           | 542          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=-159.21 +/- 43.96\n",
            "Episode length: 688.90 +/- 227.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 689      |\n",
            "|    mean_reward     | -159     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 130000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 128      |\n",
            "|    ep_rew_mean     | -58.9    |\n",
            "| time/              |          |\n",
            "|    fps             | 773      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 169      |\n",
            "|    total_timesteps | 131072   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=-198.86 +/- 51.57\n",
            "Episode length: 531.50 +/- 231.95\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 532          |\n",
            "|    mean_reward          | -199         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 140000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062217033 |\n",
            "|    clip_fraction        | 0.0281       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.26         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 338          |\n",
            "|    n_updates            | 32           |\n",
            "|    policy_gradient_loss | -0.00382     |\n",
            "|    value_loss           | 555          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 127      |\n",
            "|    ep_rew_mean     | -36.7    |\n",
            "| time/              |          |\n",
            "|    fps             | 785      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 187      |\n",
            "|    total_timesteps | 147456   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=-174.05 +/- 32.39\n",
            "Episode length: 493.00 +/- 217.61\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 493         |\n",
            "|    mean_reward          | -174        |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 150000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009782089 |\n",
            "|    clip_fraction        | 0.0924      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | -0.00326    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 218         |\n",
            "|    n_updates            | 36          |\n",
            "|    policy_gradient_loss | -0.0047     |\n",
            "|    value_loss           | 489         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=-183.35 +/- 65.46\n",
            "Episode length: 564.50 +/- 248.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 564      |\n",
            "|    mean_reward     | -183     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 160000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 148      |\n",
            "|    ep_rew_mean     | -26.6    |\n",
            "| time/              |          |\n",
            "|    fps             | 747      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 219      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=-195.66 +/- 64.83\n",
            "Episode length: 539.60 +/- 282.80\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 540          |\n",
            "|    mean_reward          | -196         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 170000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0066810725 |\n",
            "|    clip_fraction        | 0.023        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.19        |\n",
            "|    explained_variance   | -0.0104      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 245          |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00224     |\n",
            "|    value_loss           | 457          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=-172.75 +/- 46.30\n",
            "Episode length: 486.90 +/- 236.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 487      |\n",
            "|    mean_reward     | -173     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 180000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 217      |\n",
            "|    ep_rew_mean     | -21.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 710      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 253      |\n",
            "|    total_timesteps | 180224   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=-192.71 +/- 34.20\n",
            "Episode length: 668.50 +/- 167.54\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 668         |\n",
            "|    mean_reward          | -193        |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 190000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003684322 |\n",
            "|    clip_fraction        | 0.011       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.00216     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 192         |\n",
            "|    n_updates            | 44          |\n",
            "|    policy_gradient_loss | -0.000772   |\n",
            "|    value_loss           | 509         |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 268      |\n",
            "|    ep_rew_mean     | -15.9    |\n",
            "| time/              |          |\n",
            "|    fps             | 692      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 283      |\n",
            "|    total_timesteps | 196608   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=-177.69 +/- 51.88\n",
            "Episode length: 721.80 +/- 254.76\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 722         |\n",
            "|    mean_reward          | -178        |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 200000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006737019 |\n",
            "|    clip_fraction        | 0.0325      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.2        |\n",
            "|    explained_variance   | 0.00585     |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 377         |\n",
            "|    n_updates            | 48          |\n",
            "|    policy_gradient_loss | -0.00244    |\n",
            "|    value_loss           | 590         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=-199.28 +/- 48.36\n",
            "Episode length: 815.30 +/- 193.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 815      |\n",
            "|    mean_reward     | -199     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 210000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 349      |\n",
            "|    ep_rew_mean     | -10.1    |\n",
            "| time/              |          |\n",
            "|    fps             | 633      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 336      |\n",
            "|    total_timesteps | 212992   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=-137.53 +/- 40.05\n",
            "Episode length: 907.60 +/- 131.73\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 908          |\n",
            "|    mean_reward          | -138         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 220000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0062547047 |\n",
            "|    clip_fraction        | 0.0304       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.2         |\n",
            "|    explained_variance   | 0.00166      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 168          |\n",
            "|    n_updates            | 52           |\n",
            "|    policy_gradient_loss | -0.00244     |\n",
            "|    value_loss           | 513          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 400      |\n",
            "|    ep_rew_mean     | 0.594    |\n",
            "| time/              |          |\n",
            "|    fps             | 609      |\n",
            "|    iterations      | 14       |\n",
            "|    time_elapsed    | 376      |\n",
            "|    total_timesteps | 229376   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=-97.07 +/- 19.66\n",
            "Episode length: 933.50 +/- 139.83\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 934         |\n",
            "|    mean_reward          | -97.1       |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 230000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006442463 |\n",
            "|    clip_fraction        | 0.0307      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.268       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 222         |\n",
            "|    n_updates            | 56          |\n",
            "|    policy_gradient_loss | -0.00351    |\n",
            "|    value_loss           | 298         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=240000, episode_reward=-81.97 +/- 38.56\n",
            "Episode length: 946.40 +/- 107.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 946      |\n",
            "|    mean_reward     | -82      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 240000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 498      |\n",
            "|    ep_rew_mean     | 12.5     |\n",
            "| time/              |          |\n",
            "|    fps             | 559      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 438      |\n",
            "|    total_timesteps | 245760   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=-95.58 +/- 39.42\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 1e+03        |\n",
            "|    mean_reward          | -95.6        |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 250000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060965577 |\n",
            "|    clip_fraction        | 0.0406       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | 0.5          |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 96.1         |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00263     |\n",
            "|    value_loss           | 215          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=-87.65 +/- 38.74\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -87.6    |\n",
            "| time/              |          |\n",
            "|    total timesteps | 260000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 562      |\n",
            "|    ep_rew_mean     | 20.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 514      |\n",
            "|    iterations      | 16       |\n",
            "|    time_elapsed    | 509      |\n",
            "|    total_timesteps | 262144   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=-89.28 +/- 67.14\n",
            "Episode length: 921.40 +/- 145.99\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 921         |\n",
            "|    mean_reward          | -89.3       |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 270000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005020079 |\n",
            "|    clip_fraction        | 0.0346      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.14       |\n",
            "|    explained_variance   | 0.523       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 163         |\n",
            "|    n_updates            | 64          |\n",
            "|    policy_gradient_loss | -0.00149    |\n",
            "|    value_loss           | 238         |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 632      |\n",
            "|    ep_rew_mean     | 30.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 505      |\n",
            "|    iterations      | 17       |\n",
            "|    time_elapsed    | 551      |\n",
            "|    total_timesteps | 278528   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=-87.81 +/- 45.02\n",
            "Episode length: 992.60 +/- 18.82\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 993         |\n",
            "|    mean_reward          | -87.8       |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 280000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003747931 |\n",
            "|    clip_fraction        | 0.0192      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.597       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 127         |\n",
            "|    n_updates            | 68          |\n",
            "|    policy_gradient_loss | -0.00103    |\n",
            "|    value_loss           | 227         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=-29.57 +/- 69.65\n",
            "Episode length: 904.60 +/- 149.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 905      |\n",
            "|    mean_reward     | -29.6    |\n",
            "| time/              |          |\n",
            "|    total timesteps | 290000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 674      |\n",
            "|    ep_rew_mean     | 38.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 477      |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 618      |\n",
            "|    total_timesteps | 294912   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=-81.29 +/- 54.04\n",
            "Episode length: 926.20 +/- 153.59\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 926          |\n",
            "|    mean_reward          | -81.3        |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 300000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064449636 |\n",
            "|    clip_fraction        | 0.0439       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.13        |\n",
            "|    explained_variance   | 0.769        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 46.1         |\n",
            "|    n_updates            | 72           |\n",
            "|    policy_gradient_loss | -0.00153     |\n",
            "|    value_loss           | 111          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=-97.34 +/- 52.52\n",
            "Episode length: 875.00 +/- 168.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 875      |\n",
            "|    mean_reward     | -97.3    |\n",
            "| time/              |          |\n",
            "|    total timesteps | 310000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 728      |\n",
            "|    ep_rew_mean     | 42.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 453      |\n",
            "|    iterations      | 19       |\n",
            "|    time_elapsed    | 687      |\n",
            "|    total_timesteps | 311296   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=-122.40 +/- 58.08\n",
            "Episode length: 904.30 +/- 192.75\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 904        |\n",
            "|    mean_reward          | -122       |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 320000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00389798 |\n",
            "|    clip_fraction        | 0.0271     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.15      |\n",
            "|    explained_variance   | 0.711      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 82         |\n",
            "|    n_updates            | 76         |\n",
            "|    policy_gradient_loss | -0.0018    |\n",
            "|    value_loss           | 183        |\n",
            "----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 740      |\n",
            "|    ep_rew_mean     | 39.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 445      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 734      |\n",
            "|    total_timesteps | 327680   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=-71.98 +/- 34.84\n",
            "Episode length: 971.10 +/- 86.70\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 971         |\n",
            "|    mean_reward          | -72         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 330000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004281125 |\n",
            "|    clip_fraction        | 0.0379      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.667       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 130         |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00201    |\n",
            "|    value_loss           | 269         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=-29.20 +/- 78.60\n",
            "Episode length: 924.50 +/- 217.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 924      |\n",
            "|    mean_reward     | -29.2    |\n",
            "| time/              |          |\n",
            "|    total timesteps | 340000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 756      |\n",
            "|    ep_rew_mean     | 38.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 428      |\n",
            "|    iterations      | 21       |\n",
            "|    time_elapsed    | 802      |\n",
            "|    total_timesteps | 344064   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=-16.73 +/- 72.38\n",
            "Episode length: 986.80 +/- 36.70\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 987         |\n",
            "|    mean_reward          | -16.7       |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 350000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004708527 |\n",
            "|    clip_fraction        | 0.0421      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.78        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 38.1        |\n",
            "|    n_updates            | 84          |\n",
            "|    policy_gradient_loss | -0.00176    |\n",
            "|    value_loss           | 145         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=360000, episode_reward=20.98 +/- 87.29\n",
            "Episode length: 993.70 +/- 11.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 994      |\n",
            "|    mean_reward     | 21       |\n",
            "| time/              |          |\n",
            "|    total timesteps | 360000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 824      |\n",
            "|    ep_rew_mean     | 47.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 411      |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 875      |\n",
            "|    total_timesteps | 360448   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=19.56 +/- 82.91\n",
            "Episode length: 953.90 +/- 92.43\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 954          |\n",
            "|    mean_reward          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 370000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036848038 |\n",
            "|    clip_fraction        | 0.0392       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.15        |\n",
            "|    explained_variance   | 0.866        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 32.5         |\n",
            "|    n_updates            | 88           |\n",
            "|    policy_gradient_loss | -0.00162     |\n",
            "|    value_loss           | 75           |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 825      |\n",
            "|    ep_rew_mean     | 46       |\n",
            "| time/              |          |\n",
            "|    fps             | 410      |\n",
            "|    iterations      | 23       |\n",
            "|    time_elapsed    | 918      |\n",
            "|    total_timesteps | 376832   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=103.49 +/- 93.61\n",
            "Episode length: 887.80 +/- 70.21\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 888         |\n",
            "|    mean_reward          | 103         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 380000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003924939 |\n",
            "|    clip_fraction        | 0.0305      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.12       |\n",
            "|    explained_variance   | 0.816       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 104         |\n",
            "|    n_updates            | 92          |\n",
            "|    policy_gradient_loss | -0.00136    |\n",
            "|    value_loss           | 137         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=390000, episode_reward=148.70 +/- 47.11\n",
            "Episode length: 883.00 +/- 73.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 883      |\n",
            "|    mean_reward     | 149      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 390000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 837      |\n",
            "|    ep_rew_mean     | 50.3     |\n",
            "| time/              |          |\n",
            "|    fps             | 400      |\n",
            "|    iterations      | 24       |\n",
            "|    time_elapsed    | 981      |\n",
            "|    total_timesteps | 393216   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=113.72 +/- 96.90\n",
            "Episode length: 681.70 +/- 77.63\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 682          |\n",
            "|    mean_reward          | 114          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 400000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044628824 |\n",
            "|    clip_fraction        | 0.0526       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.16        |\n",
            "|    explained_variance   | 0.873        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 39.8         |\n",
            "|    n_updates            | 96           |\n",
            "|    policy_gradient_loss | -0.00247     |\n",
            "|    value_loss           | 80.6         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 866      |\n",
            "|    ep_rew_mean     | 60.8     |\n",
            "| time/              |          |\n",
            "|    fps             | 402      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 1017     |\n",
            "|    total_timesteps | 409600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=121.42 +/- 78.12\n",
            "Episode length: 841.30 +/- 75.58\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 841          |\n",
            "|    mean_reward          | 121          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 410000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043886793 |\n",
            "|    clip_fraction        | 0.026        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0.881        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 56.2         |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000717    |\n",
            "|    value_loss           | 77.3         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=71.30 +/- 102.03\n",
            "Episode length: 872.60 +/- 128.05\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 873      |\n",
            "|    mean_reward     | 71.3     |\n",
            "| time/              |          |\n",
            "|    total timesteps | 420000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 896      |\n",
            "|    ep_rew_mean     | 70.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 395      |\n",
            "|    iterations      | 26       |\n",
            "|    time_elapsed    | 1078     |\n",
            "|    total_timesteps | 425984   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=117.07 +/- 67.84\n",
            "Episode length: 866.40 +/- 92.37\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 866         |\n",
            "|    mean_reward          | 117         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 430000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005308528 |\n",
            "|    clip_fraction        | 0.0527      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.889       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 38          |\n",
            "|    n_updates            | 104         |\n",
            "|    policy_gradient_loss | -0.00272    |\n",
            "|    value_loss           | 79.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=145.64 +/- 47.99\n",
            "Episode length: 844.20 +/- 90.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 844      |\n",
            "|    mean_reward     | 146      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 440000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 902      |\n",
            "|    ep_rew_mean     | 79.2     |\n",
            "| time/              |          |\n",
            "|    fps             | 386      |\n",
            "|    iterations      | 27       |\n",
            "|    time_elapsed    | 1144     |\n",
            "|    total_timesteps | 442368   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=173.71 +/- 14.61\n",
            "Episode length: 776.40 +/- 74.39\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 776          |\n",
            "|    mean_reward          | 174          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 450000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056351256 |\n",
            "|    clip_fraction        | 0.0447       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 0.917        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 47           |\n",
            "|    n_updates            | 108          |\n",
            "|    policy_gradient_loss | -0.00196     |\n",
            "|    value_loss           | 67.4         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 905      |\n",
            "|    ep_rew_mean     | 82.6     |\n",
            "| time/              |          |\n",
            "|    fps             | 386      |\n",
            "|    iterations      | 28       |\n",
            "|    time_elapsed    | 1186     |\n",
            "|    total_timesteps | 458752   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=155.13 +/- 65.60\n",
            "Episode length: 779.80 +/- 146.10\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 780          |\n",
            "|    mean_reward          | 155          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 460000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044153226 |\n",
            "|    clip_fraction        | 0.036        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | 0.908        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.7         |\n",
            "|    n_updates            | 112          |\n",
            "|    policy_gradient_loss | -0.00086     |\n",
            "|    value_loss           | 73.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=184.71 +/- 57.96\n",
            "Episode length: 736.30 +/- 167.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 736      |\n",
            "|    mean_reward     | 185      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 470000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 888      |\n",
            "|    ep_rew_mean     | 89.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 381      |\n",
            "|    iterations      | 29       |\n",
            "|    time_elapsed    | 1244     |\n",
            "|    total_timesteps | 475136   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=197.61 +/- 16.94\n",
            "Episode length: 737.40 +/- 61.02\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 737          |\n",
            "|    mean_reward          | 198          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 480000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042679235 |\n",
            "|    clip_fraction        | 0.0429       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0.928        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 27.3         |\n",
            "|    n_updates            | 116          |\n",
            "|    policy_gradient_loss | -0.00107     |\n",
            "|    value_loss           | 62.7         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=490000, episode_reward=184.17 +/- 23.56\n",
            "Episode length: 680.90 +/- 50.16\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 681      |\n",
            "|    mean_reward     | 184      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 490000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 896      |\n",
            "|    ep_rew_mean     | 97.1     |\n",
            "| time/              |          |\n",
            "|    fps             | 379      |\n",
            "|    iterations      | 30       |\n",
            "|    time_elapsed    | 1296     |\n",
            "|    total_timesteps | 491520   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=134.49 +/- 87.51\n",
            "Episode length: 787.50 +/- 119.86\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 788          |\n",
            "|    mean_reward          | 134          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 500000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042649573 |\n",
            "|    clip_fraction        | 0.034        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0.945        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.3         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00072     |\n",
            "|    value_loss           | 39.1         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 896      |\n",
            "|    ep_rew_mean     | 100      |\n",
            "| time/              |          |\n",
            "|    fps             | 378      |\n",
            "|    iterations      | 31       |\n",
            "|    time_elapsed    | 1342     |\n",
            "|    total_timesteps | 507904   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=179.62 +/- 26.82\n",
            "Episode length: 685.70 +/- 56.19\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 686         |\n",
            "|    mean_reward          | 180         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 510000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004890171 |\n",
            "|    clip_fraction        | 0.0398      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.934       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.55        |\n",
            "|    n_updates            | 124         |\n",
            "|    policy_gradient_loss | -0.000738   |\n",
            "|    value_loss           | 55.7        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=202.30 +/- 32.54\n",
            "Episode length: 639.60 +/- 81.46\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 640      |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 520000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 889      |\n",
            "|    ep_rew_mean     | 103      |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 32       |\n",
            "|    time_elapsed    | 1393     |\n",
            "|    total_timesteps | 524288   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=193.96 +/- 17.62\n",
            "Episode length: 718.90 +/- 70.63\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 719          |\n",
            "|    mean_reward          | 194          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 530000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044322894 |\n",
            "|    clip_fraction        | 0.0432       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.08        |\n",
            "|    explained_variance   | 0.944        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11           |\n",
            "|    n_updates            | 128          |\n",
            "|    policy_gradient_loss | -0.00114     |\n",
            "|    value_loss           | 49.2         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=191.27 +/- 18.86\n",
            "Episode length: 730.10 +/- 95.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 730      |\n",
            "|    mean_reward     | 191      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 540000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 909      |\n",
            "|    ep_rew_mean     | 111      |\n",
            "| time/              |          |\n",
            "|    fps             | 373      |\n",
            "|    iterations      | 33       |\n",
            "|    time_elapsed    | 1448     |\n",
            "|    total_timesteps | 540672   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=187.74 +/- 26.84\n",
            "Episode length: 695.70 +/- 54.61\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 696          |\n",
            "|    mean_reward          | 188          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 550000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040307646 |\n",
            "|    clip_fraction        | 0.0337       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.06        |\n",
            "|    explained_variance   | 0.969        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.2         |\n",
            "|    n_updates            | 132          |\n",
            "|    policy_gradient_loss | -0.000374    |\n",
            "|    value_loss           | 30.8         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 918      |\n",
            "|    ep_rew_mean     | 114      |\n",
            "| time/              |          |\n",
            "|    fps             | 374      |\n",
            "|    iterations      | 34       |\n",
            "|    time_elapsed    | 1488     |\n",
            "|    total_timesteps | 557056   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=186.97 +/- 16.17\n",
            "Episode length: 709.20 +/- 52.14\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 709          |\n",
            "|    mean_reward          | 187          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 560000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042809993 |\n",
            "|    clip_fraction        | 0.0233       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0.936        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 35.1         |\n",
            "|    n_updates            | 136          |\n",
            "|    policy_gradient_loss | -0.000821    |\n",
            "|    value_loss           | 67.6         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=176.77 +/- 16.43\n",
            "Episode length: 678.10 +/- 30.62\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 678      |\n",
            "|    mean_reward     | 177      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 570000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 925      |\n",
            "|    ep_rew_mean     | 117      |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 35       |\n",
            "|    time_elapsed    | 1542     |\n",
            "|    total_timesteps | 573440   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=194.01 +/- 19.36\n",
            "Episode length: 661.60 +/- 44.38\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 662          |\n",
            "|    mean_reward          | 194          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 580000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052774544 |\n",
            "|    clip_fraction        | 0.0571       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | 0.979        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.86         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00071     |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 925      |\n",
            "|    ep_rew_mean     | 118      |\n",
            "| time/              |          |\n",
            "|    fps             | 372      |\n",
            "|    iterations      | 36       |\n",
            "|    time_elapsed    | 1581     |\n",
            "|    total_timesteps | 589824   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=203.86 +/- 30.10\n",
            "Episode length: 658.60 +/- 73.82\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 659          |\n",
            "|    mean_reward          | 204          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 590000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025882525 |\n",
            "|    clip_fraction        | 0.0254       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.05        |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.19         |\n",
            "|    n_updates            | 144          |\n",
            "|    policy_gradient_loss | 0.000676     |\n",
            "|    value_loss           | 42.5         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=600000, episode_reward=213.22 +/- 35.00\n",
            "Episode length: 616.60 +/- 80.22\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 617      |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 600000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 946      |\n",
            "|    ep_rew_mean     | 122      |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 37       |\n",
            "|    time_elapsed    | 1631     |\n",
            "|    total_timesteps | 606208   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=204.65 +/- 21.14\n",
            "Episode length: 638.30 +/- 55.75\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 638         |\n",
            "|    mean_reward          | 205         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 610000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004140316 |\n",
            "|    clip_fraction        | 0.0315      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.03       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.84        |\n",
            "|    n_updates            | 148         |\n",
            "|    policy_gradient_loss | -1.76e-05   |\n",
            "|    value_loss           | 11.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=201.57 +/- 26.54\n",
            "Episode length: 647.40 +/- 78.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 647      |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 620000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 953      |\n",
            "|    ep_rew_mean     | 125      |\n",
            "| time/              |          |\n",
            "|    fps             | 370      |\n",
            "|    iterations      | 38       |\n",
            "|    time_elapsed    | 1682     |\n",
            "|    total_timesteps | 622592   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=192.67 +/- 44.34\n",
            "Episode length: 611.60 +/- 133.19\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 612          |\n",
            "|    mean_reward          | 193          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 630000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032219237 |\n",
            "|    clip_fraction        | 0.0338       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.69         |\n",
            "|    n_updates            | 152          |\n",
            "|    policy_gradient_loss | -0.000414    |\n",
            "|    value_loss           | 26.6         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 953      |\n",
            "|    ep_rew_mean     | 126      |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 39       |\n",
            "|    time_elapsed    | 1719     |\n",
            "|    total_timesteps | 638976   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=218.22 +/- 26.57\n",
            "Episode length: 541.00 +/- 41.31\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 541        |\n",
            "|    mean_reward          | 218        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 640000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00427367 |\n",
            "|    clip_fraction        | 0.0302     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 0.991      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 5.42       |\n",
            "|    n_updates            | 156        |\n",
            "|    policy_gradient_loss | -0.000406  |\n",
            "|    value_loss           | 8.43       |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=650000, episode_reward=203.57 +/- 19.90\n",
            "Episode length: 545.20 +/- 38.82\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 545      |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 650000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 974      |\n",
            "|    ep_rew_mean     | 132      |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 40       |\n",
            "|    time_elapsed    | 1763     |\n",
            "|    total_timesteps | 655360   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=218.67 +/- 29.06\n",
            "Episode length: 516.00 +/- 46.09\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 516         |\n",
            "|    mean_reward          | 219         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 660000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004248739 |\n",
            "|    clip_fraction        | 0.0357      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.69        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | 7.45e-05    |\n",
            "|    value_loss           | 7.76        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=670000, episode_reward=217.78 +/- 19.34\n",
            "Episode length: 553.70 +/- 49.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 554      |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 670000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 974      |\n",
            "|    ep_rew_mean     | 132      |\n",
            "| time/              |          |\n",
            "|    fps             | 371      |\n",
            "|    iterations      | 41       |\n",
            "|    time_elapsed    | 1806     |\n",
            "|    total_timesteps | 671744   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=233.54 +/- 19.02\n",
            "Episode length: 510.10 +/- 51.82\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 510         |\n",
            "|    mean_reward          | 234         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 680000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003587752 |\n",
            "|    clip_fraction        | 0.0334      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.36        |\n",
            "|    n_updates            | 164         |\n",
            "|    policy_gradient_loss | -0.00047    |\n",
            "|    value_loss           | 23.1        |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 980      |\n",
            "|    ep_rew_mean     | 133      |\n",
            "| time/              |          |\n",
            "|    fps             | 374      |\n",
            "|    iterations      | 42       |\n",
            "|    time_elapsed    | 1839     |\n",
            "|    total_timesteps | 688128   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=222.65 +/- 23.74\n",
            "Episode length: 479.50 +/- 27.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 480         |\n",
            "|    mean_reward          | 223         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 690000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008515036 |\n",
            "|    clip_fraction        | 0.0402      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.02       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.82        |\n",
            "|    n_updates            | 168         |\n",
            "|    policy_gradient_loss | -0.00122    |\n",
            "|    value_loss           | 23.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=209.10 +/- 63.59\n",
            "Episode length: 484.50 +/- 76.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 484      |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 700000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 958      |\n",
            "|    ep_rew_mean     | 131      |\n",
            "| time/              |          |\n",
            "|    fps             | 375      |\n",
            "|    iterations      | 43       |\n",
            "|    time_elapsed    | 1878     |\n",
            "|    total_timesteps | 704512   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=223.60 +/- 20.80\n",
            "Episode length: 510.50 +/- 13.60\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 510          |\n",
            "|    mean_reward          | 224          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 710000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044255825 |\n",
            "|    clip_fraction        | 0.0243       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1           |\n",
            "|    explained_variance   | 0.958        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 109          |\n",
            "|    n_updates            | 172          |\n",
            "|    policy_gradient_loss | -0.000187    |\n",
            "|    value_loss           | 52           |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=225.03 +/- 20.99\n",
            "Episode length: 511.70 +/- 41.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 512      |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 720000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 943      |\n",
            "|    ep_rew_mean     | 129      |\n",
            "| time/              |          |\n",
            "|    fps             | 376      |\n",
            "|    iterations      | 44       |\n",
            "|    time_elapsed    | 1916     |\n",
            "|    total_timesteps | 720896   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=224.35 +/- 23.66\n",
            "Episode length: 497.70 +/- 33.35\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 498          |\n",
            "|    mean_reward          | 224          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 730000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045111347 |\n",
            "|    clip_fraction        | 0.0341       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.975       |\n",
            "|    explained_variance   | 0.959        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 39.8         |\n",
            "|    n_updates            | 176          |\n",
            "|    policy_gradient_loss | -0.000718    |\n",
            "|    value_loss           | 56.8         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 901      |\n",
            "|    ep_rew_mean     | 123      |\n",
            "| time/              |          |\n",
            "|    fps             | 378      |\n",
            "|    iterations      | 45       |\n",
            "|    time_elapsed    | 1946     |\n",
            "|    total_timesteps | 737280   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=223.75 +/- 20.06\n",
            "Episode length: 504.30 +/- 25.73\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 504         |\n",
            "|    mean_reward          | 224         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 740000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003975217 |\n",
            "|    clip_fraction        | 0.0284      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.964      |\n",
            "|    explained_variance   | 0.94        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 45.1        |\n",
            "|    n_updates            | 180         |\n",
            "|    policy_gradient_loss | -0.00151    |\n",
            "|    value_loss           | 83.3        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=224.90 +/- 21.97\n",
            "Episode length: 476.80 +/- 29.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 477      |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 750000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 886      |\n",
            "|    ep_rew_mean     | 121      |\n",
            "| time/              |          |\n",
            "|    fps             | 379      |\n",
            "|    iterations      | 46       |\n",
            "|    time_elapsed    | 1984     |\n",
            "|    total_timesteps | 753664   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=207.19 +/- 51.93\n",
            "Episode length: 586.90 +/- 216.47\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 587          |\n",
            "|    mean_reward          | 207          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 760000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048252796 |\n",
            "|    clip_fraction        | 0.0466       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.979       |\n",
            "|    explained_variance   | 0.954        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 13.5         |\n",
            "|    n_updates            | 184          |\n",
            "|    policy_gradient_loss | -0.000147    |\n",
            "|    value_loss           | 57.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=227.86 +/- 24.57\n",
            "Episode length: 470.80 +/- 29.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 471      |\n",
            "|    mean_reward     | 228      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 770000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 858      |\n",
            "|    ep_rew_mean     | 118      |\n",
            "| time/              |          |\n",
            "|    fps             | 380      |\n",
            "|    iterations      | 47       |\n",
            "|    time_elapsed    | 2022     |\n",
            "|    total_timesteps | 770048   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=231.22 +/- 20.60\n",
            "Episode length: 481.60 +/- 34.42\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 482         |\n",
            "|    mean_reward          | 231         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 780000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003827203 |\n",
            "|    clip_fraction        | 0.0341      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.973      |\n",
            "|    explained_variance   | 0.948       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.98        |\n",
            "|    n_updates            | 188         |\n",
            "|    policy_gradient_loss | 0.000126    |\n",
            "|    value_loss           | 73.3        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 880      |\n",
            "|    ep_rew_mean     | 121      |\n",
            "| time/              |          |\n",
            "|    fps             | 383      |\n",
            "|    iterations      | 48       |\n",
            "|    time_elapsed    | 2051     |\n",
            "|    total_timesteps | 786432   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=237.57 +/- 21.88\n",
            "Episode length: 454.90 +/- 30.15\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 455        |\n",
            "|    mean_reward          | 238        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 790000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00494847 |\n",
            "|    clip_fraction        | 0.0474     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.955     |\n",
            "|    explained_variance   | 0.98       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.82       |\n",
            "|    n_updates            | 192        |\n",
            "|    policy_gradient_loss | 0.000157   |\n",
            "|    value_loss           | 20.4       |\n",
            "----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=800000, episode_reward=249.99 +/- 15.91\n",
            "Episode length: 462.30 +/- 21.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 462      |\n",
            "|    mean_reward     | 250      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 800000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 880      |\n",
            "|    ep_rew_mean     | 122      |\n",
            "| time/              |          |\n",
            "|    fps             | 384      |\n",
            "|    iterations      | 49       |\n",
            "|    time_elapsed    | 2087     |\n",
            "|    total_timesteps | 802816   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=242.91 +/- 14.17\n",
            "Episode length: 425.30 +/- 22.05\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 425        |\n",
            "|    mean_reward          | 243        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 810000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00450238 |\n",
            "|    clip_fraction        | 0.0385     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.937     |\n",
            "|    explained_variance   | 0.971      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 2.42       |\n",
            "|    n_updates            | 196        |\n",
            "|    policy_gradient_loss | -0.000813  |\n",
            "|    value_loss           | 33.1       |\n",
            "----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 909      |\n",
            "|    ep_rew_mean     | 127      |\n",
            "| time/              |          |\n",
            "|    fps             | 387      |\n",
            "|    iterations      | 50       |\n",
            "|    time_elapsed    | 2116     |\n",
            "|    total_timesteps | 819200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=229.93 +/- 40.20\n",
            "Episode length: 473.90 +/- 177.34\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 474          |\n",
            "|    mean_reward          | 230          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 820000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032512513 |\n",
            "|    clip_fraction        | 0.0392       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.896       |\n",
            "|    explained_variance   | 0.964        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 2.13         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.000571    |\n",
            "|    value_loss           | 40.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=246.24 +/- 20.96\n",
            "Episode length: 406.60 +/- 18.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 407      |\n",
            "|    mean_reward     | 246      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 830000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 888      |\n",
            "|    ep_rew_mean     | 126      |\n",
            "| time/              |          |\n",
            "|    fps             | 388      |\n",
            "|    iterations      | 51       |\n",
            "|    time_elapsed    | 2151     |\n",
            "|    total_timesteps | 835584   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=252.46 +/- 20.53\n",
            "Episode length: 395.70 +/- 26.91\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 396          |\n",
            "|    mean_reward          | 252          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 840000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050687273 |\n",
            "|    clip_fraction        | 0.0271       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.881       |\n",
            "|    explained_variance   | 0.926        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 42.3         |\n",
            "|    n_updates            | 204          |\n",
            "|    policy_gradient_loss | -0.000896    |\n",
            "|    value_loss           | 95.3         |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=850000, episode_reward=232.93 +/- 22.77\n",
            "Episode length: 418.10 +/- 32.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 418      |\n",
            "|    mean_reward     | 233      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 850000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 890      |\n",
            "|    ep_rew_mean     | 130      |\n",
            "| time/              |          |\n",
            "|    fps             | 390      |\n",
            "|    iterations      | 52       |\n",
            "|    time_elapsed    | 2184     |\n",
            "|    total_timesteps | 851968   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=236.70 +/- 40.76\n",
            "Episode length: 479.80 +/- 176.05\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 480          |\n",
            "|    mean_reward          | 237          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 860000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031812042 |\n",
            "|    clip_fraction        | 0.0287       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.832       |\n",
            "|    explained_variance   | 0.966        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 56.1         |\n",
            "|    n_updates            | 208          |\n",
            "|    policy_gradient_loss | -0.000254    |\n",
            "|    value_loss           | 39.8         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 904      |\n",
            "|    ep_rew_mean     | 136      |\n",
            "| time/              |          |\n",
            "|    fps             | 392      |\n",
            "|    iterations      | 53       |\n",
            "|    time_elapsed    | 2214     |\n",
            "|    total_timesteps | 868352   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=239.40 +/- 26.94\n",
            "Episode length: 405.90 +/- 24.17\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 406         |\n",
            "|    mean_reward          | 239         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 870000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003457765 |\n",
            "|    clip_fraction        | 0.0365      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.871      |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21.2        |\n",
            "|    n_updates            | 212         |\n",
            "|    policy_gradient_loss | 9.66e-05    |\n",
            "|    value_loss           | 35.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=250.40 +/- 23.44\n",
            "Episode length: 413.00 +/- 28.38\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 413      |\n",
            "|    mean_reward     | 250      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 880000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 916      |\n",
            "|    ep_rew_mean     | 140      |\n",
            "| time/              |          |\n",
            "|    fps             | 393      |\n",
            "|    iterations      | 54       |\n",
            "|    time_elapsed    | 2249     |\n",
            "|    total_timesteps | 884736   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=193.45 +/- 84.06\n",
            "Episode length: 466.90 +/- 178.08\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 467         |\n",
            "|    mean_reward          | 193         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 890000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005568516 |\n",
            "|    clip_fraction        | 0.0506      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.816      |\n",
            "|    explained_variance   | 0.975       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.84        |\n",
            "|    n_updates            | 216         |\n",
            "|    policy_gradient_loss | -0.00112    |\n",
            "|    value_loss           | 32          |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=223.15 +/- 45.46\n",
            "Episode length: 467.60 +/- 178.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 468      |\n",
            "|    mean_reward     | 223      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 900000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 906      |\n",
            "|    ep_rew_mean     | 142      |\n",
            "| time/              |          |\n",
            "|    fps             | 394      |\n",
            "|    iterations      | 55       |\n",
            "|    time_elapsed    | 2285     |\n",
            "|    total_timesteps | 901120   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=244.53 +/- 23.16\n",
            "Episode length: 405.80 +/- 23.94\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 406         |\n",
            "|    mean_reward          | 245         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 910000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004251618 |\n",
            "|    clip_fraction        | 0.0368      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.807      |\n",
            "|    explained_variance   | 0.955       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.72        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.00127    |\n",
            "|    value_loss           | 56.7        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 909      |\n",
            "|    ep_rew_mean     | 146      |\n",
            "| time/              |          |\n",
            "|    fps             | 396      |\n",
            "|    iterations      | 56       |\n",
            "|    time_elapsed    | 2312     |\n",
            "|    total_timesteps | 917504   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=245.22 +/- 21.41\n",
            "Episode length: 385.90 +/- 19.96\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 386         |\n",
            "|    mean_reward          | 245         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 920000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003458595 |\n",
            "|    clip_fraction        | 0.0325      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.824      |\n",
            "|    explained_variance   | 0.972       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.33        |\n",
            "|    n_updates            | 224         |\n",
            "|    policy_gradient_loss | 0.000847    |\n",
            "|    value_loss           | 34.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=260.12 +/- 15.17\n",
            "Episode length: 384.70 +/- 35.32\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 385      |\n",
            "|    mean_reward     | 260      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 930000   |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 929      |\n",
            "|    ep_rew_mean     | 147      |\n",
            "| time/              |          |\n",
            "|    fps             | 398      |\n",
            "|    iterations      | 57       |\n",
            "|    time_elapsed    | 2344     |\n",
            "|    total_timesteps | 933888   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=252.07 +/- 9.98\n",
            "Episode length: 398.10 +/- 27.01\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 398          |\n",
            "|    mean_reward          | 252          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 940000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040178727 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.84        |\n",
            "|    explained_variance   | 0.982        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.24         |\n",
            "|    n_updates            | 228          |\n",
            "|    policy_gradient_loss | 7.89e-05     |\n",
            "|    value_loss           | 22.6         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=232.05 +/- 37.83\n",
            "Episode length: 455.20 +/- 183.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 455      |\n",
            "|    mean_reward     | 232      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 950000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 928      |\n",
            "|    ep_rew_mean     | 149      |\n",
            "| time/              |          |\n",
            "|    fps             | 399      |\n",
            "|    iterations      | 58       |\n",
            "|    time_elapsed    | 2377     |\n",
            "|    total_timesteps | 950272   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=243.57 +/- 22.61\n",
            "Episode length: 400.80 +/- 23.61\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 401          |\n",
            "|    mean_reward          | 244          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 960000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037045581 |\n",
            "|    clip_fraction        | 0.0318       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.854       |\n",
            "|    explained_variance   | 0.976        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.81         |\n",
            "|    n_updates            | 232          |\n",
            "|    policy_gradient_loss | -0.000282    |\n",
            "|    value_loss           | 37.7         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 941      |\n",
            "|    ep_rew_mean     | 150      |\n",
            "| time/              |          |\n",
            "|    fps             | 401      |\n",
            "|    iterations      | 59       |\n",
            "|    time_elapsed    | 2404     |\n",
            "|    total_timesteps | 966656   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=238.56 +/- 26.31\n",
            "Episode length: 376.10 +/- 18.79\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 376         |\n",
            "|    mean_reward          | 239         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 970000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004324169 |\n",
            "|    clip_fraction        | 0.057       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.854      |\n",
            "|    explained_variance   | 0.997       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.23        |\n",
            "|    n_updates            | 236         |\n",
            "|    policy_gradient_loss | -0.000205   |\n",
            "|    value_loss           | 3.34        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=219.52 +/- 70.29\n",
            "Episode length: 381.60 +/- 16.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 382      |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 980000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 950      |\n",
            "|    ep_rew_mean     | 152      |\n",
            "| time/              |          |\n",
            "|    fps             | 403      |\n",
            "|    iterations      | 60       |\n",
            "|    time_elapsed    | 2435     |\n",
            "|    total_timesteps | 983040   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=249.91 +/- 19.29\n",
            "Episode length: 376.10 +/- 22.92\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 376         |\n",
            "|    mean_reward          | 250         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 990000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003817182 |\n",
            "|    clip_fraction        | 0.0267      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.809      |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.85        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.000367   |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 953      |\n",
            "|    ep_rew_mean     | 150      |\n",
            "| time/              |          |\n",
            "|    fps             | 406      |\n",
            "|    iterations      | 61       |\n",
            "|    time_elapsed    | 2460     |\n",
            "|    total_timesteps | 999424   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=244.83 +/- 19.53\n",
            "Episode length: 369.90 +/- 19.35\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 370         |\n",
            "|    mean_reward          | 245         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 1000000     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003170951 |\n",
            "|    clip_fraction        | 0.0268      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.77       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.986       |\n",
            "|    n_updates            | 244         |\n",
            "|    policy_gradient_loss | 0.00046     |\n",
            "|    value_loss           | 16.8        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=1010000, episode_reward=254.75 +/- 19.78\n",
            "Episode length: 370.50 +/- 21.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 370      |\n",
            "|    mean_reward     | 255      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 1010000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 958      |\n",
            "|    ep_rew_mean     | 148      |\n",
            "| time/              |          |\n",
            "|    fps             | 407      |\n",
            "|    iterations      | 62       |\n",
            "|    time_elapsed    | 2492     |\n",
            "|    total_timesteps | 1015808  |\n",
            "---------------------------------\n",
            "Saving to logs/ppo/LunarLander-v2_9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ldSgWhU6tUa",
        "outputId": "62c91625-6459-411e-914c-966f5fd580f3"
      },
      "source": [
        "!python scripts/all_plots.py -a ppo --env LunarLander-v2 -l ppo -f logs/ppo/LunarLander-v2_9/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval not found for logs/ppo/LunarLander-v2_5\n",
            "Eval not found for logs/ppo/LunarLander-v2_8\n",
            "scripts/all_plots.py:153: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  merged_results = np.array(merged_results)\n",
            "Traceback (most recent call last):\n",
            "  File \"scripts/all_plots.py\", line 153, in <module>\n",
            "    merged_results = np.array(merged_results)\n",
            "ValueError: could not broadcast input array from shape (101,5) into shape (101)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQxj6cGJ7eKa",
        "outputId": "13c9cd29-7db0-4ef6-b7d9-24ae60551062"
      },
      "source": [
        "!python scripts/plot_train.py -a ppo -e LunarLander-v2 -f logs/ -w 500 -x steps -x steps -y reward"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"scripts/plot_train.py\", line 46, in <module>\n",
            "    for folder in os.listdir(log_path)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'logs/ppo/LunarLander-v2_9/ppo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyPOb5RUFZwK",
        "outputId": "efe029b0-536a-4040-b2d8-0acd4bfabeb5"
      },
      "source": [
        "!python train.py --algo ppo --env LunarLander-v2 -n 1000000 --eval-freq 10000 --eval-episodes 10 --n-eval-envs 1 -i logs/ppo/LunarLander-v2_1/best_model.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== LunarLander-v2 ==========\n",
            "Seed: 147388831\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('batch_size', 64),\n",
            "             ('ent_coef', 0.01),\n",
            "             ('gae_lambda', 0.98),\n",
            "             ('gamma', 0.999),\n",
            "             ('n_envs', 16),\n",
            "             ('n_epochs', 4),\n",
            "             ('n_steps', 1024),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('policy', 'MlpPolicy')])\n",
            "Using 16 environments\n",
            "Overwriting n_timesteps with n=1000000\n",
            "Creating test environment\n",
            "Loading pretrained agent\n",
            "Log path: logs/ppo/LunarLander-v2_2\n",
            "Eval num_timesteps=10000, episode_reward=254.81 +/- 21.45\n",
            "Episode length: 315.30 +/- 10.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 315      |\n",
            "|    mean_reward     | 255      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 10000    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 300      |\n",
            "|    ep_rew_mean     | 261      |\n",
            "| time/              |          |\n",
            "|    fps             | 1032     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 15       |\n",
            "|    total_timesteps | 16384    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=20000, episode_reward=269.08 +/- 26.73\n",
            "Episode length: 325.10 +/- 17.34\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 325         |\n",
            "|    mean_reward          | 269         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 20000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004276493 |\n",
            "|    clip_fraction        | 0.0452      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.772      |\n",
            "|    explained_variance   | 0.895       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 27.5        |\n",
            "|    n_updates            | 308         |\n",
            "|    policy_gradient_loss | -0.000217   |\n",
            "|    value_loss           | 160         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=30000, episode_reward=239.75 +/- 61.14\n",
            "Episode length: 319.30 +/- 24.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 319      |\n",
            "|    mean_reward     | 240      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 30000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 307      |\n",
            "|    ep_rew_mean     | 257      |\n",
            "| time/              |          |\n",
            "|    fps             | 706      |\n",
            "|    iterations      | 2        |\n",
            "|    time_elapsed    | 46       |\n",
            "|    total_timesteps | 32768    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=40000, episode_reward=261.09 +/- 17.26\n",
            "Episode length: 320.90 +/- 18.05\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 321          |\n",
            "|    mean_reward          | 261          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 40000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025709856 |\n",
            "|    clip_fraction        | 0.0147       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.739       |\n",
            "|    explained_variance   | 0.869        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 10.8         |\n",
            "|    n_updates            | 312          |\n",
            "|    policy_gradient_loss | -0.000675    |\n",
            "|    value_loss           | 227          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 311      |\n",
            "|    ep_rew_mean     | 259      |\n",
            "| time/              |          |\n",
            "|    fps             | 701      |\n",
            "|    iterations      | 3        |\n",
            "|    time_elapsed    | 70       |\n",
            "|    total_timesteps | 49152    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=50000, episode_reward=270.46 +/- 13.53\n",
            "Episode length: 317.50 +/- 14.89\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 318         |\n",
            "|    mean_reward          | 270         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 50000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003040688 |\n",
            "|    clip_fraction        | 0.0185      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.742      |\n",
            "|    explained_variance   | 0.872       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 83.4        |\n",
            "|    n_updates            | 316         |\n",
            "|    policy_gradient_loss | 4.98e-05    |\n",
            "|    value_loss           | 261         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=60000, episode_reward=265.40 +/- 17.18\n",
            "Episode length: 322.20 +/- 15.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 322      |\n",
            "|    mean_reward     | 265      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 60000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 304      |\n",
            "|    ep_rew_mean     | 258      |\n",
            "| time/              |          |\n",
            "|    fps             | 656      |\n",
            "|    iterations      | 4        |\n",
            "|    time_elapsed    | 99       |\n",
            "|    total_timesteps | 65536    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=70000, episode_reward=243.41 +/- 62.53\n",
            "Episode length: 382.50 +/- 206.58\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 382        |\n",
            "|    mean_reward          | 243        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 70000      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00437941 |\n",
            "|    clip_fraction        | 0.042      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.748     |\n",
            "|    explained_variance   | 0.915      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 72.8       |\n",
            "|    n_updates            | 320        |\n",
            "|    policy_gradient_loss | -7.83e-05  |\n",
            "|    value_loss           | 192        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=80000, episode_reward=268.49 +/- 14.25\n",
            "Episode length: 319.70 +/- 11.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 320      |\n",
            "|    mean_reward     | 268      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 80000    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 295      |\n",
            "|    ep_rew_mean     | 251      |\n",
            "| time/              |          |\n",
            "|    fps             | 622      |\n",
            "|    iterations      | 5        |\n",
            "|    time_elapsed    | 131      |\n",
            "|    total_timesteps | 81920    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90000, episode_reward=267.88 +/- 13.69\n",
            "Episode length: 324.40 +/- 20.26\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 324          |\n",
            "|    mean_reward          | 268          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 90000        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030671293 |\n",
            "|    clip_fraction        | 0.0307       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.717       |\n",
            "|    explained_variance   | 0.893        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 360          |\n",
            "|    n_updates            | 324          |\n",
            "|    policy_gradient_loss | 0.000364     |\n",
            "|    value_loss           | 210          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 304      |\n",
            "|    ep_rew_mean     | 254      |\n",
            "| time/              |          |\n",
            "|    fps             | 631      |\n",
            "|    iterations      | 6        |\n",
            "|    time_elapsed    | 155      |\n",
            "|    total_timesteps | 98304    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=100000, episode_reward=248.54 +/- 17.50\n",
            "Episode length: 317.50 +/- 15.72\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 318          |\n",
            "|    mean_reward          | 249          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 100000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044029322 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.744       |\n",
            "|    explained_variance   | 0.963        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 114          |\n",
            "|    n_updates            | 328          |\n",
            "|    policy_gradient_loss | 0.000348     |\n",
            "|    value_loss           | 71.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=110000, episode_reward=266.19 +/- 15.52\n",
            "Episode length: 314.00 +/- 16.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 314      |\n",
            "|    mean_reward     | 266      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 110000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 318      |\n",
            "|    ep_rew_mean     | 257      |\n",
            "| time/              |          |\n",
            "|    fps             | 616      |\n",
            "|    iterations      | 7        |\n",
            "|    time_elapsed    | 186      |\n",
            "|    total_timesteps | 114688   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=120000, episode_reward=269.14 +/- 15.72\n",
            "Episode length: 315.20 +/- 18.55\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 315          |\n",
            "|    mean_reward          | 269          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 120000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056701074 |\n",
            "|    clip_fraction        | 0.0404       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.732       |\n",
            "|    explained_variance   | 0.916        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.62         |\n",
            "|    n_updates            | 332          |\n",
            "|    policy_gradient_loss | -0.000611    |\n",
            "|    value_loss           | 186          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=130000, episode_reward=253.18 +/- 22.62\n",
            "Episode length: 316.00 +/- 15.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 316      |\n",
            "|    mean_reward     | 253      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 130000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 316      |\n",
            "|    ep_rew_mean     | 259      |\n",
            "| time/              |          |\n",
            "|    fps             | 605      |\n",
            "|    iterations      | 8        |\n",
            "|    time_elapsed    | 216      |\n",
            "|    total_timesteps | 131072   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=140000, episode_reward=259.04 +/- 17.77\n",
            "Episode length: 305.50 +/- 14.96\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 306          |\n",
            "|    mean_reward          | 259          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 140000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038908334 |\n",
            "|    clip_fraction        | 0.0424       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.739       |\n",
            "|    explained_variance   | 0.96         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 17.6         |\n",
            "|    n_updates            | 336          |\n",
            "|    policy_gradient_loss | -7.68e-05    |\n",
            "|    value_loss           | 80.5         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 315      |\n",
            "|    ep_rew_mean     | 265      |\n",
            "| time/              |          |\n",
            "|    fps             | 614      |\n",
            "|    iterations      | 9        |\n",
            "|    time_elapsed    | 239      |\n",
            "|    total_timesteps | 147456   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=150000, episode_reward=252.60 +/- 18.28\n",
            "Episode length: 309.70 +/- 18.21\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 310          |\n",
            "|    mean_reward          | 253          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 150000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034939004 |\n",
            "|    clip_fraction        | 0.0552       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.702       |\n",
            "|    explained_variance   | 0.969        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.51         |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.000634    |\n",
            "|    value_loss           | 69.8         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=160000, episode_reward=257.87 +/- 26.11\n",
            "Episode length: 310.70 +/- 16.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 311      |\n",
            "|    mean_reward     | 258      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 160000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 302      |\n",
            "|    ep_rew_mean     | 265      |\n",
            "| time/              |          |\n",
            "|    fps             | 607      |\n",
            "|    iterations      | 10       |\n",
            "|    time_elapsed    | 269      |\n",
            "|    total_timesteps | 163840   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=170000, episode_reward=258.97 +/- 22.14\n",
            "Episode length: 313.10 +/- 12.32\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 313          |\n",
            "|    mean_reward          | 259          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 170000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032535146 |\n",
            "|    clip_fraction        | 0.0306       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.739       |\n",
            "|    explained_variance   | 0.914        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 55.7         |\n",
            "|    n_updates            | 344          |\n",
            "|    policy_gradient_loss | -0.000482    |\n",
            "|    value_loss           | 170          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=180000, episode_reward=264.79 +/- 19.40\n",
            "Episode length: 308.30 +/- 18.44\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 308      |\n",
            "|    mean_reward     | 265      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 180000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 289      |\n",
            "|    ep_rew_mean     | 254      |\n",
            "| time/              |          |\n",
            "|    fps             | 603      |\n",
            "|    iterations      | 11       |\n",
            "|    time_elapsed    | 298      |\n",
            "|    total_timesteps | 180224   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=190000, episode_reward=259.42 +/- 18.55\n",
            "Episode length: 310.10 +/- 18.28\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 310          |\n",
            "|    mean_reward          | 259          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 190000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028087692 |\n",
            "|    clip_fraction        | 0.0291       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.756       |\n",
            "|    explained_variance   | 0.914        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 12.5         |\n",
            "|    n_updates            | 348          |\n",
            "|    policy_gradient_loss | -5.63e-05    |\n",
            "|    value_loss           | 163          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 290      |\n",
            "|    ep_rew_mean     | 256      |\n",
            "| time/              |          |\n",
            "|    fps             | 609      |\n",
            "|    iterations      | 12       |\n",
            "|    time_elapsed    | 322      |\n",
            "|    total_timesteps | 196608   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=200000, episode_reward=258.25 +/- 23.15\n",
            "Episode length: 294.20 +/- 7.57\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 294          |\n",
            "|    mean_reward          | 258          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 200000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041587777 |\n",
            "|    clip_fraction        | 0.0422       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.723       |\n",
            "|    explained_variance   | 0.949        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.58         |\n",
            "|    n_updates            | 352          |\n",
            "|    policy_gradient_loss | 0.000384     |\n",
            "|    value_loss           | 86.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=210000, episode_reward=258.98 +/- 22.39\n",
            "Episode length: 298.30 +/- 14.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 298      |\n",
            "|    mean_reward     | 259      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 210000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 286      |\n",
            "|    ep_rew_mean     | 259      |\n",
            "| time/              |          |\n",
            "|    fps             | 607      |\n",
            "|    iterations      | 13       |\n",
            "|    time_elapsed    | 350      |\n",
            "|    total_timesteps | 212992   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=220000, episode_reward=266.53 +/- 17.04\n",
            "Episode length: 287.80 +/- 13.01\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 288          |\n",
            "|    mean_reward          | 267          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 220000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029593266 |\n",
            "|    clip_fraction        | 0.0257       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.744       |\n",
            "|    explained_variance   | 0.867        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 415          |\n",
            "|    n_updates            | 356          |\n",
            "|    policy_gradient_loss | -5.71e-05    |\n",
            "|    value_loss           | 310          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 277      |\n",
            "|    ep_rew_mean     | 263      |\n",
            "| time/              |          |\n",
            "|    fps             | 615      |\n",
            "|    iterations      | 14       |\n",
            "|    time_elapsed    | 372      |\n",
            "|    total_timesteps | 229376   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=230000, episode_reward=273.77 +/- 19.05\n",
            "Episode length: 286.20 +/- 17.24\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 286          |\n",
            "|    mean_reward          | 274          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 230000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028395543 |\n",
            "|    clip_fraction        | 0.0398       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.722       |\n",
            "|    explained_variance   | 0.918        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 248          |\n",
            "|    n_updates            | 360          |\n",
            "|    policy_gradient_loss | -0.000254    |\n",
            "|    value_loss           | 199          |\n",
            "------------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=240000, episode_reward=256.42 +/- 21.63\n",
            "Episode length: 289.70 +/- 9.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 290      |\n",
            "|    mean_reward     | 256      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 240000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 276      |\n",
            "|    ep_rew_mean     | 259      |\n",
            "| time/              |          |\n",
            "|    fps             | 613      |\n",
            "|    iterations      | 15       |\n",
            "|    time_elapsed    | 400      |\n",
            "|    total_timesteps | 245760   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=250000, episode_reward=265.14 +/- 16.24\n",
            "Episode length: 283.60 +/- 13.49\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 284        |\n",
            "|    mean_reward          | 265        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 250000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00359866 |\n",
            "|    clip_fraction        | 0.0342     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.734     |\n",
            "|    explained_variance   | 0.869      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 34.8       |\n",
            "|    n_updates            | 364        |\n",
            "|    policy_gradient_loss | -4.77e-05  |\n",
            "|    value_loss           | 333        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=260000, episode_reward=267.72 +/- 15.91\n",
            "Episode length: 289.80 +/- 11.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 290      |\n",
            "|    mean_reward     | 268      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 260000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 269      |\n",
            "|    ep_rew_mean     | 247      |\n",
            "| time/              |          |\n",
            "|    fps             | 612      |\n",
            "|    iterations      | 16       |\n",
            "|    time_elapsed    | 428      |\n",
            "|    total_timesteps | 262144   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=270000, episode_reward=271.75 +/- 19.30\n",
            "Episode length: 300.80 +/- 15.28\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 301          |\n",
            "|    mean_reward          | 272          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 270000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021644444 |\n",
            "|    clip_fraction        | 0.0164       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.73        |\n",
            "|    explained_variance   | 0.833        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 200          |\n",
            "|    n_updates            | 368          |\n",
            "|    policy_gradient_loss | -0.000219    |\n",
            "|    value_loss           | 359          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 273      |\n",
            "|    ep_rew_mean     | 245      |\n",
            "| time/              |          |\n",
            "|    fps             | 617      |\n",
            "|    iterations      | 17       |\n",
            "|    time_elapsed    | 451      |\n",
            "|    total_timesteps | 278528   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=280000, episode_reward=259.15 +/- 21.23\n",
            "Episode length: 285.30 +/- 19.10\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 285          |\n",
            "|    mean_reward          | 259          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 280000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044379625 |\n",
            "|    clip_fraction        | 0.0386       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.712       |\n",
            "|    explained_variance   | 0.849        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 99.3         |\n",
            "|    n_updates            | 372          |\n",
            "|    policy_gradient_loss | -0.000512    |\n",
            "|    value_loss           | 370          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=290000, episode_reward=272.12 +/- 18.16\n",
            "Episode length: 310.10 +/- 23.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 310      |\n",
            "|    mean_reward     | 272      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 290000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 280      |\n",
            "|    ep_rew_mean     | 261      |\n",
            "| time/              |          |\n",
            "|    fps             | 614      |\n",
            "|    iterations      | 18       |\n",
            "|    time_elapsed    | 479      |\n",
            "|    total_timesteps | 294912   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=300000, episode_reward=252.34 +/- 23.46\n",
            "Episode length: 293.80 +/- 11.20\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 294          |\n",
            "|    mean_reward          | 252          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 300000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041936813 |\n",
            "|    clip_fraction        | 0.0526       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.717       |\n",
            "|    explained_variance   | 0.91         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 18.8         |\n",
            "|    n_updates            | 376          |\n",
            "|    policy_gradient_loss | 0.0005       |\n",
            "|    value_loss           | 210          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=310000, episode_reward=264.88 +/- 24.83\n",
            "Episode length: 296.70 +/- 18.29\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 297      |\n",
            "|    mean_reward     | 265      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 310000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 284      |\n",
            "|    ep_rew_mean     | 261      |\n",
            "| time/              |          |\n",
            "|    fps             | 611      |\n",
            "|    iterations      | 19       |\n",
            "|    time_elapsed    | 508      |\n",
            "|    total_timesteps | 311296   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=320000, episode_reward=258.77 +/- 18.14\n",
            "Episode length: 293.80 +/- 10.43\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 294          |\n",
            "|    mean_reward          | 259          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 320000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038807897 |\n",
            "|    clip_fraction        | 0.0435       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.684       |\n",
            "|    explained_variance   | 0.909        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 18.6         |\n",
            "|    n_updates            | 380          |\n",
            "|    policy_gradient_loss | -0.000476    |\n",
            "|    value_loss           | 193          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 284      |\n",
            "|    ep_rew_mean     | 255      |\n",
            "| time/              |          |\n",
            "|    fps             | 616      |\n",
            "|    iterations      | 20       |\n",
            "|    time_elapsed    | 531      |\n",
            "|    total_timesteps | 327680   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=330000, episode_reward=263.13 +/- 16.78\n",
            "Episode length: 293.60 +/- 17.92\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 294          |\n",
            "|    mean_reward          | 263          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 330000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040910775 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.751       |\n",
            "|    explained_variance   | 0.872        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 133          |\n",
            "|    n_updates            | 384          |\n",
            "|    policy_gradient_loss | 0.000122     |\n",
            "|    value_loss           | 241          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=340000, episode_reward=269.95 +/- 22.05\n",
            "Episode length: 302.70 +/- 22.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 303      |\n",
            "|    mean_reward     | 270      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 340000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 283      |\n",
            "|    ep_rew_mean     | 258      |\n",
            "| time/              |          |\n",
            "|    fps             | 615      |\n",
            "|    iterations      | 21       |\n",
            "|    time_elapsed    | 559      |\n",
            "|    total_timesteps | 344064   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=350000, episode_reward=279.41 +/- 14.43\n",
            "Episode length: 284.90 +/- 19.68\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 285         |\n",
            "|    mean_reward          | 279         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 350000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006177769 |\n",
            "|    clip_fraction        | 0.0489      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.709      |\n",
            "|    explained_variance   | 0.886       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.2        |\n",
            "|    n_updates            | 388         |\n",
            "|    policy_gradient_loss | 6.14e-05    |\n",
            "|    value_loss           | 245         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=360000, episode_reward=270.23 +/- 27.58\n",
            "Episode length: 291.20 +/- 15.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 291      |\n",
            "|    mean_reward     | 270      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 360000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 287      |\n",
            "|    ep_rew_mean     | 262      |\n",
            "| time/              |          |\n",
            "|    fps             | 613      |\n",
            "|    iterations      | 22       |\n",
            "|    time_elapsed    | 587      |\n",
            "|    total_timesteps | 360448   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=370000, episode_reward=277.13 +/- 23.37\n",
            "Episode length: 287.40 +/- 13.35\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 287          |\n",
            "|    mean_reward          | 277          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 370000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042843204 |\n",
            "|    clip_fraction        | 0.0452       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.71        |\n",
            "|    explained_variance   | 0.943        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 31.3         |\n",
            "|    n_updates            | 392          |\n",
            "|    policy_gradient_loss | -0.000576    |\n",
            "|    value_loss           | 133          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 304      |\n",
            "|    ep_rew_mean     | 261      |\n",
            "| time/              |          |\n",
            "|    fps             | 616      |\n",
            "|    iterations      | 23       |\n",
            "|    time_elapsed    | 611      |\n",
            "|    total_timesteps | 376832   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=380000, episode_reward=272.78 +/- 21.30\n",
            "Episode length: 276.90 +/- 14.47\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 277         |\n",
            "|    mean_reward          | 273         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 380000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002500487 |\n",
            "|    clip_fraction        | 0.036       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.701      |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.57        |\n",
            "|    n_updates            | 396         |\n",
            "|    policy_gradient_loss | 0.000643    |\n",
            "|    value_loss           | 94.5        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=390000, episode_reward=260.99 +/- 27.71\n",
            "Episode length: 281.90 +/- 18.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 282      |\n",
            "|    mean_reward     | 261      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 390000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 307      |\n",
            "|    ep_rew_mean     | 260      |\n",
            "| time/              |          |\n",
            "|    fps             | 614      |\n",
            "|    iterations      | 24       |\n",
            "|    time_elapsed    | 639      |\n",
            "|    total_timesteps | 393216   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=400000, episode_reward=266.83 +/- 24.15\n",
            "Episode length: 288.70 +/- 7.71\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 289         |\n",
            "|    mean_reward          | 267         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 400000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004203239 |\n",
            "|    clip_fraction        | 0.0347      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.696      |\n",
            "|    explained_variance   | 0.946       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.51        |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.000328   |\n",
            "|    value_loss           | 128         |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 296      |\n",
            "|    ep_rew_mean     | 262      |\n",
            "| time/              |          |\n",
            "|    fps             | 616      |\n",
            "|    iterations      | 25       |\n",
            "|    time_elapsed    | 664      |\n",
            "|    total_timesteps | 409600   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=410000, episode_reward=262.55 +/- 13.04\n",
            "Episode length: 274.50 +/- 16.82\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 274          |\n",
            "|    mean_reward          | 263          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 410000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038897037 |\n",
            "|    clip_fraction        | 0.0418       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.693       |\n",
            "|    explained_variance   | 0.961        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 154          |\n",
            "|    n_updates            | 404          |\n",
            "|    policy_gradient_loss | -0.000508    |\n",
            "|    value_loss           | 135          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=420000, episode_reward=256.42 +/- 54.75\n",
            "Episode length: 381.60 +/- 216.31\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 382      |\n",
            "|    mean_reward     | 256      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 420000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 325      |\n",
            "|    ep_rew_mean     | 248      |\n",
            "| time/              |          |\n",
            "|    fps             | 612      |\n",
            "|    iterations      | 26       |\n",
            "|    time_elapsed    | 695      |\n",
            "|    total_timesteps | 425984   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=430000, episode_reward=243.12 +/- 30.05\n",
            "Episode length: 305.20 +/- 107.43\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 305          |\n",
            "|    mean_reward          | 243          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 430000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042173313 |\n",
            "|    clip_fraction        | 0.0461       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.709       |\n",
            "|    explained_variance   | 0.947        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 158          |\n",
            "|    n_updates            | 408          |\n",
            "|    policy_gradient_loss | -0.000448    |\n",
            "|    value_loss           | 139          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=440000, episode_reward=264.55 +/- 19.98\n",
            "Episode length: 286.20 +/- 16.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 286      |\n",
            "|    mean_reward     | 265      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 440000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 328      |\n",
            "|    ep_rew_mean     | 244      |\n",
            "| time/              |          |\n",
            "|    fps             | 611      |\n",
            "|    iterations      | 27       |\n",
            "|    time_elapsed    | 723      |\n",
            "|    total_timesteps | 442368   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=450000, episode_reward=272.55 +/- 13.43\n",
            "Episode length: 276.90 +/- 14.31\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 277          |\n",
            "|    mean_reward          | 273          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 450000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040905094 |\n",
            "|    clip_fraction        | 0.0362       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.725       |\n",
            "|    explained_variance   | 0.934        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 47.3         |\n",
            "|    n_updates            | 412          |\n",
            "|    policy_gradient_loss | 0.000281     |\n",
            "|    value_loss           | 212          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 295      |\n",
            "|    ep_rew_mean     | 257      |\n",
            "| time/              |          |\n",
            "|    fps             | 614      |\n",
            "|    iterations      | 28       |\n",
            "|    time_elapsed    | 746      |\n",
            "|    total_timesteps | 458752   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=460000, episode_reward=253.00 +/- 19.43\n",
            "Episode length: 278.70 +/- 12.07\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 279          |\n",
            "|    mean_reward          | 253          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 460000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035537917 |\n",
            "|    clip_fraction        | 0.0316       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.688       |\n",
            "|    explained_variance   | 0.952        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 379          |\n",
            "|    n_updates            | 416          |\n",
            "|    policy_gradient_loss | -7.24e-05    |\n",
            "|    value_loss           | 102          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=470000, episode_reward=242.28 +/- 55.20\n",
            "Episode length: 358.20 +/- 214.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 358      |\n",
            "|    mean_reward     | 242      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 470000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 289      |\n",
            "|    ep_rew_mean     | 257      |\n",
            "| time/              |          |\n",
            "|    fps             | 612      |\n",
            "|    iterations      | 29       |\n",
            "|    time_elapsed    | 775      |\n",
            "|    total_timesteps | 475136   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=480000, episode_reward=267.03 +/- 17.46\n",
            "Episode length: 283.50 +/- 19.70\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 284          |\n",
            "|    mean_reward          | 267          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 480000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030606631 |\n",
            "|    clip_fraction        | 0.0447       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.695       |\n",
            "|    explained_variance   | 0.912        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 295          |\n",
            "|    n_updates            | 420          |\n",
            "|    policy_gradient_loss | -0.000993    |\n",
            "|    value_loss           | 206          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=490000, episode_reward=249.71 +/- 30.40\n",
            "Episode length: 279.10 +/- 12.11\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 279      |\n",
            "|    mean_reward     | 250      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 490000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 292      |\n",
            "|    ep_rew_mean     | 262      |\n",
            "| time/              |          |\n",
            "|    fps             | 611      |\n",
            "|    iterations      | 30       |\n",
            "|    time_elapsed    | 803      |\n",
            "|    total_timesteps | 491520   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=500000, episode_reward=265.98 +/- 19.66\n",
            "Episode length: 277.10 +/- 12.30\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 277         |\n",
            "|    mean_reward          | 266         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 500000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004041626 |\n",
            "|    clip_fraction        | 0.0509      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.719      |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.78        |\n",
            "|    n_updates            | 424         |\n",
            "|    policy_gradient_loss | 0.000734    |\n",
            "|    value_loss           | 41.2        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 298      |\n",
            "|    ep_rew_mean     | 257      |\n",
            "| time/              |          |\n",
            "|    fps             | 614      |\n",
            "|    iterations      | 31       |\n",
            "|    time_elapsed    | 825      |\n",
            "|    total_timesteps | 507904   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=510000, episode_reward=260.34 +/- 14.83\n",
            "Episode length: 269.80 +/- 15.71\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 270        |\n",
            "|    mean_reward          | 260        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 510000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00181395 |\n",
            "|    clip_fraction        | 0.0126     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.715     |\n",
            "|    explained_variance   | 0.889      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 253        |\n",
            "|    n_updates            | 428        |\n",
            "|    policy_gradient_loss | 0.000442   |\n",
            "|    value_loss           | 274        |\n",
            "----------------------------------------\n",
            "Eval num_timesteps=520000, episode_reward=256.28 +/- 16.99\n",
            "Episode length: 281.90 +/- 15.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 282      |\n",
            "|    mean_reward     | 256      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 520000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 296      |\n",
            "|    ep_rew_mean     | 261      |\n",
            "| time/              |          |\n",
            "|    fps             | 614      |\n",
            "|    iterations      | 32       |\n",
            "|    time_elapsed    | 853      |\n",
            "|    total_timesteps | 524288   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=530000, episode_reward=253.71 +/- 17.35\n",
            "Episode length: 282.50 +/- 17.60\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 282          |\n",
            "|    mean_reward          | 254          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 530000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046517123 |\n",
            "|    clip_fraction        | 0.0493       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.674       |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.72         |\n",
            "|    n_updates            | 432          |\n",
            "|    policy_gradient_loss | 8.74e-06     |\n",
            "|    value_loss           | 78.7         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=540000, episode_reward=279.36 +/- 19.57\n",
            "Episode length: 276.10 +/- 17.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 276      |\n",
            "|    mean_reward     | 279      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 540000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 288      |\n",
            "|    ep_rew_mean     | 259      |\n",
            "| time/              |          |\n",
            "|    fps             | 613      |\n",
            "|    iterations      | 33       |\n",
            "|    time_elapsed    | 880      |\n",
            "|    total_timesteps | 540672   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=550000, episode_reward=258.97 +/- 21.62\n",
            "Episode length: 286.80 +/- 12.38\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 287          |\n",
            "|    mean_reward          | 259          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 550000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034539436 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.707       |\n",
            "|    explained_variance   | 0.878        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 48.2         |\n",
            "|    n_updates            | 436          |\n",
            "|    policy_gradient_loss | -0.000347    |\n",
            "|    value_loss           | 297          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 284      |\n",
            "|    ep_rew_mean     | 252      |\n",
            "| time/              |          |\n",
            "|    fps             | 616      |\n",
            "|    iterations      | 34       |\n",
            "|    time_elapsed    | 903      |\n",
            "|    total_timesteps | 557056   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=560000, episode_reward=267.69 +/- 18.37\n",
            "Episode length: 286.10 +/- 21.62\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 286         |\n",
            "|    mean_reward          | 268         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 560000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005882914 |\n",
            "|    clip_fraction        | 0.0807      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.744      |\n",
            "|    explained_variance   | 0.956       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14.3        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.000393   |\n",
            "|    value_loss           | 139         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=570000, episode_reward=268.44 +/- 14.69\n",
            "Episode length: 275.20 +/- 13.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 275      |\n",
            "|    mean_reward     | 268      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 570000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 290      |\n",
            "|    ep_rew_mean     | 255      |\n",
            "| time/              |          |\n",
            "|    fps             | 615      |\n",
            "|    iterations      | 35       |\n",
            "|    time_elapsed    | 931      |\n",
            "|    total_timesteps | 573440   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=580000, episode_reward=276.00 +/- 13.31\n",
            "Episode length: 282.40 +/- 20.67\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 282          |\n",
            "|    mean_reward          | 276          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 580000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048436415 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.696       |\n",
            "|    explained_variance   | 0.925        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 105          |\n",
            "|    n_updates            | 444          |\n",
            "|    policy_gradient_loss | -0.000756    |\n",
            "|    value_loss           | 212          |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 280      |\n",
            "|    ep_rew_mean     | 256      |\n",
            "| time/              |          |\n",
            "|    fps             | 617      |\n",
            "|    iterations      | 36       |\n",
            "|    time_elapsed    | 954      |\n",
            "|    total_timesteps | 589824   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=590000, episode_reward=265.48 +/- 24.51\n",
            "Episode length: 281.80 +/- 19.07\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 282          |\n",
            "|    mean_reward          | 265          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 590000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038109978 |\n",
            "|    clip_fraction        | 0.04         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.692       |\n",
            "|    explained_variance   | 0.924        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 31           |\n",
            "|    n_updates            | 448          |\n",
            "|    policy_gradient_loss | -0.000282    |\n",
            "|    value_loss           | 202          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=600000, episode_reward=256.58 +/- 19.53\n",
            "Episode length: 275.40 +/- 11.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 275      |\n",
            "|    mean_reward     | 257      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 600000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 281      |\n",
            "|    ep_rew_mean     | 254      |\n",
            "| time/              |          |\n",
            "|    fps             | 617      |\n",
            "|    iterations      | 37       |\n",
            "|    time_elapsed    | 981      |\n",
            "|    total_timesteps | 606208   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=610000, episode_reward=265.58 +/- 23.64\n",
            "Episode length: 273.70 +/- 17.66\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 274         |\n",
            "|    mean_reward          | 266         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 610000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002614115 |\n",
            "|    clip_fraction        | 0.0283      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.729      |\n",
            "|    explained_variance   | 0.85        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 407         |\n",
            "|    n_updates            | 452         |\n",
            "|    policy_gradient_loss | -0.000372   |\n",
            "|    value_loss           | 354         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=620000, episode_reward=268.42 +/- 22.78\n",
            "Episode length: 278.20 +/- 14.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 278      |\n",
            "|    mean_reward     | 268      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 620000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 293      |\n",
            "|    ep_rew_mean     | 252      |\n",
            "| time/              |          |\n",
            "|    fps             | 616      |\n",
            "|    iterations      | 38       |\n",
            "|    time_elapsed    | 1009     |\n",
            "|    total_timesteps | 622592   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=630000, episode_reward=283.63 +/- 16.59\n",
            "Episode length: 274.00 +/- 19.33\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 274         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 630000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004219979 |\n",
            "|    clip_fraction        | 0.0483      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.687      |\n",
            "|    explained_variance   | 0.954       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21          |\n",
            "|    n_updates            | 456         |\n",
            "|    policy_gradient_loss | -6.19e-05   |\n",
            "|    value_loss           | 77          |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 292      |\n",
            "|    ep_rew_mean     | 253      |\n",
            "| time/              |          |\n",
            "|    fps             | 619      |\n",
            "|    iterations      | 39       |\n",
            "|    time_elapsed    | 1031     |\n",
            "|    total_timesteps | 638976   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=640000, episode_reward=284.01 +/- 13.91\n",
            "Episode length: 266.90 +/- 20.53\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 267         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 640000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002858338 |\n",
            "|    clip_fraction        | 0.0185      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.713      |\n",
            "|    explained_variance   | 0.874       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 131         |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.00137    |\n",
            "|    value_loss           | 290         |\n",
            "-----------------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=650000, episode_reward=262.46 +/- 25.61\n",
            "Episode length: 271.10 +/- 15.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 271      |\n",
            "|    mean_reward     | 262      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 650000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 291      |\n",
            "|    ep_rew_mean     | 248      |\n",
            "| time/              |          |\n",
            "|    fps             | 618      |\n",
            "|    iterations      | 40       |\n",
            "|    time_elapsed    | 1059     |\n",
            "|    total_timesteps | 655360   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=660000, episode_reward=262.16 +/- 25.89\n",
            "Episode length: 261.00 +/- 14.50\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 261          |\n",
            "|    mean_reward          | 262          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 660000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038154447 |\n",
            "|    clip_fraction        | 0.0335       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.715       |\n",
            "|    explained_variance   | 0.935        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 63.8         |\n",
            "|    n_updates            | 464          |\n",
            "|    policy_gradient_loss | -0.00146     |\n",
            "|    value_loss           | 266          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=670000, episode_reward=275.55 +/- 23.73\n",
            "Episode length: 264.30 +/- 17.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 276      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 670000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 308      |\n",
            "|    ep_rew_mean     | 251      |\n",
            "| time/              |          |\n",
            "|    fps             | 618      |\n",
            "|    iterations      | 41       |\n",
            "|    time_elapsed    | 1086     |\n",
            "|    total_timesteps | 671744   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=680000, episode_reward=282.97 +/- 24.74\n",
            "Episode length: 267.70 +/- 30.88\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 268         |\n",
            "|    mean_reward          | 283         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 680000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004898816 |\n",
            "|    clip_fraction        | 0.0599      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.717      |\n",
            "|    explained_variance   | 0.965       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 38.5        |\n",
            "|    n_updates            | 468         |\n",
            "|    policy_gradient_loss | 0.000471    |\n",
            "|    value_loss           | 112         |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 292      |\n",
            "|    ep_rew_mean     | 253      |\n",
            "| time/              |          |\n",
            "|    fps             | 620      |\n",
            "|    iterations      | 42       |\n",
            "|    time_elapsed    | 1108     |\n",
            "|    total_timesteps | 688128   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=690000, episode_reward=282.13 +/- 21.19\n",
            "Episode length: 261.10 +/- 24.79\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 261         |\n",
            "|    mean_reward          | 282         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 690000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002578211 |\n",
            "|    clip_fraction        | 0.032       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.709      |\n",
            "|    explained_variance   | 0.898       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 35.7        |\n",
            "|    n_updates            | 472         |\n",
            "|    policy_gradient_loss | 3.47e-05    |\n",
            "|    value_loss           | 228         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=700000, episode_reward=269.51 +/- 17.53\n",
            "Episode length: 280.00 +/- 16.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 280      |\n",
            "|    mean_reward     | 270      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 700000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 267      |\n",
            "|    ep_rew_mean     | 260      |\n",
            "| time/              |          |\n",
            "|    fps             | 621      |\n",
            "|    iterations      | 43       |\n",
            "|    time_elapsed    | 1134     |\n",
            "|    total_timesteps | 704512   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=710000, episode_reward=271.41 +/- 26.45\n",
            "Episode length: 252.10 +/- 10.05\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 252          |\n",
            "|    mean_reward          | 271          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 710000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033518113 |\n",
            "|    clip_fraction        | 0.0432       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.704       |\n",
            "|    explained_variance   | 0.948        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 16.8         |\n",
            "|    n_updates            | 476          |\n",
            "|    policy_gradient_loss | -0.000461    |\n",
            "|    value_loss           | 101          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=720000, episode_reward=262.97 +/- 21.81\n",
            "Episode length: 267.30 +/- 15.28\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 267      |\n",
            "|    mean_reward     | 263      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 720000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 263      |\n",
            "|    ep_rew_mean     | 272      |\n",
            "| time/              |          |\n",
            "|    fps             | 621      |\n",
            "|    iterations      | 44       |\n",
            "|    time_elapsed    | 1160     |\n",
            "|    total_timesteps | 720896   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=730000, episode_reward=254.93 +/- 43.76\n",
            "Episode length: 350.40 +/- 217.40\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 350          |\n",
            "|    mean_reward          | 255          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 730000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049880664 |\n",
            "|    clip_fraction        | 0.0399       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.711       |\n",
            "|    explained_variance   | 0.969        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.86         |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | 0.00086      |\n",
            "|    value_loss           | 42.3         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 289      |\n",
            "|    ep_rew_mean     | 262      |\n",
            "| time/              |          |\n",
            "|    fps             | 622      |\n",
            "|    iterations      | 45       |\n",
            "|    time_elapsed    | 1184     |\n",
            "|    total_timesteps | 737280   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=740000, episode_reward=268.34 +/- 18.31\n",
            "Episode length: 257.80 +/- 18.41\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 258         |\n",
            "|    mean_reward          | 268         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 740000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003784841 |\n",
            "|    clip_fraction        | 0.0314      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.704      |\n",
            "|    explained_variance   | 0.94        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 54.7        |\n",
            "|    n_updates            | 484         |\n",
            "|    policy_gradient_loss | -0.000138   |\n",
            "|    value_loss           | 148         |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=750000, episode_reward=274.91 +/- 17.02\n",
            "Episode length: 264.30 +/- 25.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 275      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 750000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 329      |\n",
            "|    ep_rew_mean     | 257      |\n",
            "| time/              |          |\n",
            "|    fps             | 621      |\n",
            "|    iterations      | 46       |\n",
            "|    time_elapsed    | 1212     |\n",
            "|    total_timesteps | 753664   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=760000, episode_reward=275.41 +/- 17.43\n",
            "Episode length: 270.30 +/- 23.61\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 270          |\n",
            "|    mean_reward          | 275          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 760000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043704957 |\n",
            "|    clip_fraction        | 0.0477       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.649       |\n",
            "|    explained_variance   | 0.965        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11           |\n",
            "|    n_updates            | 488          |\n",
            "|    policy_gradient_loss | -0.000165    |\n",
            "|    value_loss           | 105          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=770000, episode_reward=270.22 +/- 16.61\n",
            "Episode length: 271.20 +/- 19.26\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 271      |\n",
            "|    mean_reward     | 270      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 770000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 319      |\n",
            "|    ep_rew_mean     | 262      |\n",
            "| time/              |          |\n",
            "|    fps             | 620      |\n",
            "|    iterations      | 47       |\n",
            "|    time_elapsed    | 1240     |\n",
            "|    total_timesteps | 770048   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=780000, episode_reward=262.69 +/- 16.60\n",
            "Episode length: 273.20 +/- 17.09\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 273          |\n",
            "|    mean_reward          | 263          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 780000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052227057 |\n",
            "|    clip_fraction        | 0.0653       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.692       |\n",
            "|    explained_variance   | 0.972        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 45           |\n",
            "|    n_updates            | 492          |\n",
            "|    policy_gradient_loss | -0.0015      |\n",
            "|    value_loss           | 86.6         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 306      |\n",
            "|    ep_rew_mean     | 261      |\n",
            "| time/              |          |\n",
            "|    fps             | 622      |\n",
            "|    iterations      | 48       |\n",
            "|    time_elapsed    | 1264     |\n",
            "|    total_timesteps | 786432   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=790000, episode_reward=265.81 +/- 11.10\n",
            "Episode length: 260.80 +/- 25.16\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 261          |\n",
            "|    mean_reward          | 266          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 790000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041285357 |\n",
            "|    clip_fraction        | 0.0407       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.691       |\n",
            "|    explained_variance   | 0.97         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.24         |\n",
            "|    n_updates            | 496          |\n",
            "|    policy_gradient_loss | -0.000223    |\n",
            "|    value_loss           | 103          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=800000, episode_reward=260.31 +/- 26.60\n",
            "Episode length: 262.00 +/- 16.69\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 262      |\n",
            "|    mean_reward     | 260      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 800000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 336      |\n",
            "|    ep_rew_mean     | 259      |\n",
            "| time/              |          |\n",
            "|    fps             | 621      |\n",
            "|    iterations      | 49       |\n",
            "|    time_elapsed    | 1292     |\n",
            "|    total_timesteps | 802816   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=810000, episode_reward=266.18 +/- 23.59\n",
            "Episode length: 256.80 +/- 18.77\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 257         |\n",
            "|    mean_reward          | 266         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 810000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004924223 |\n",
            "|    clip_fraction        | 0.0654      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.706      |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.92        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.000397   |\n",
            "|    value_loss           | 29.1        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 329      |\n",
            "|    ep_rew_mean     | 260      |\n",
            "| time/              |          |\n",
            "|    fps             | 622      |\n",
            "|    iterations      | 50       |\n",
            "|    time_elapsed    | 1315     |\n",
            "|    total_timesteps | 819200   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=820000, episode_reward=273.20 +/- 21.91\n",
            "Episode length: 265.80 +/- 19.83\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 266          |\n",
            "|    mean_reward          | 273          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 820000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052564926 |\n",
            "|    clip_fraction        | 0.0495       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.683       |\n",
            "|    explained_variance   | 0.96         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 44           |\n",
            "|    n_updates            | 504          |\n",
            "|    policy_gradient_loss | -0.000562    |\n",
            "|    value_loss           | 130          |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=830000, episode_reward=252.18 +/- 43.54\n",
            "Episode length: 345.20 +/- 218.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 345      |\n",
            "|    mean_reward     | 252      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 830000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 313      |\n",
            "|    ep_rew_mean     | 256      |\n",
            "| time/              |          |\n",
            "|    fps             | 621      |\n",
            "|    iterations      | 51       |\n",
            "|    time_elapsed    | 1344     |\n",
            "|    total_timesteps | 835584   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840000, episode_reward=261.56 +/- 22.19\n",
            "Episode length: 264.40 +/- 17.48\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 264          |\n",
            "|    mean_reward          | 262          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 840000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037323923 |\n",
            "|    clip_fraction        | 0.0329       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.675       |\n",
            "|    explained_variance   | 0.973        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 22           |\n",
            "|    n_updates            | 508          |\n",
            "|    policy_gradient_loss | -0.000987    |\n",
            "|    value_loss           | 93           |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=850000, episode_reward=268.69 +/- 23.92\n",
            "Episode length: 261.50 +/- 23.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 262      |\n",
            "|    mean_reward     | 269      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 850000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 320      |\n",
            "|    ep_rew_mean     | 261      |\n",
            "| time/              |          |\n",
            "|    fps             | 620      |\n",
            "|    iterations      | 52       |\n",
            "|    time_elapsed    | 1372     |\n",
            "|    total_timesteps | 851968   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=860000, episode_reward=277.93 +/- 20.79\n",
            "Episode length: 259.10 +/- 20.34\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 259          |\n",
            "|    mean_reward          | 278          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 860000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043939883 |\n",
            "|    clip_fraction        | 0.0473       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.667       |\n",
            "|    explained_variance   | 0.993        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.15         |\n",
            "|    n_updates            | 512          |\n",
            "|    policy_gradient_loss | 0.00109      |\n",
            "|    value_loss           | 12.2         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 313      |\n",
            "|    ep_rew_mean     | 267      |\n",
            "| time/              |          |\n",
            "|    fps             | 622      |\n",
            "|    iterations      | 53       |\n",
            "|    time_elapsed    | 1395     |\n",
            "|    total_timesteps | 868352   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=870000, episode_reward=262.14 +/- 23.66\n",
            "Episode length: 278.70 +/- 15.52\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 279          |\n",
            "|    mean_reward          | 262          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 870000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035282485 |\n",
            "|    clip_fraction        | 0.0388       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.675       |\n",
            "|    explained_variance   | 0.974        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 145          |\n",
            "|    n_updates            | 516          |\n",
            "|    policy_gradient_loss | -0.000815    |\n",
            "|    value_loss           | 68.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=880000, episode_reward=277.66 +/- 21.08\n",
            "Episode length: 274.80 +/- 23.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 275      |\n",
            "|    mean_reward     | 278      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 880000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 293      |\n",
            "|    ep_rew_mean     | 265      |\n",
            "| time/              |          |\n",
            "|    fps             | 621      |\n",
            "|    iterations      | 54       |\n",
            "|    time_elapsed    | 1423     |\n",
            "|    total_timesteps | 884736   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=890000, episode_reward=257.05 +/- 17.06\n",
            "Episode length: 257.60 +/- 14.52\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 258         |\n",
            "|    mean_reward          | 257         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 890000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004543583 |\n",
            "|    clip_fraction        | 0.0529      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.694      |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.52        |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | 0.000168    |\n",
            "|    value_loss           | 35.9        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=900000, episode_reward=270.88 +/- 21.21\n",
            "Episode length: 267.00 +/- 21.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 267      |\n",
            "|    mean_reward     | 271      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 900000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 281      |\n",
            "|    ep_rew_mean     | 267      |\n",
            "| time/              |          |\n",
            "|    fps             | 621      |\n",
            "|    iterations      | 55       |\n",
            "|    time_elapsed    | 1449     |\n",
            "|    total_timesteps | 901120   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=910000, episode_reward=270.70 +/- 14.93\n",
            "Episode length: 254.30 +/- 21.49\n",
            "----------------------------------------\n",
            "| eval/                   |            |\n",
            "|    mean_ep_length       | 254        |\n",
            "|    mean_reward          | 271        |\n",
            "| time/                   |            |\n",
            "|    total timesteps      | 910000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00464834 |\n",
            "|    clip_fraction        | 0.0602     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.691     |\n",
            "|    explained_variance   | 0.968      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.49       |\n",
            "|    n_updates            | 524        |\n",
            "|    policy_gradient_loss | -7.84e-05  |\n",
            "|    value_loss           | 69.6       |\n",
            "----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 264      |\n",
            "|    ep_rew_mean     | 274      |\n",
            "| time/              |          |\n",
            "|    fps             | 623      |\n",
            "|    iterations      | 56       |\n",
            "|    time_elapsed    | 1470     |\n",
            "|    total_timesteps | 917504   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=920000, episode_reward=262.72 +/- 18.35\n",
            "Episode length: 259.90 +/- 18.57\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 260          |\n",
            "|    mean_reward          | 263          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 920000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052172677 |\n",
            "|    clip_fraction        | 0.0531       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.708       |\n",
            "|    explained_variance   | 0.997        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.72         |\n",
            "|    n_updates            | 528          |\n",
            "|    policy_gradient_loss | 0.00011      |\n",
            "|    value_loss           | 4.85         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=930000, episode_reward=260.28 +/- 19.00\n",
            "Episode length: 269.60 +/- 20.05\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 270      |\n",
            "|    mean_reward     | 260      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 930000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 263      |\n",
            "|    ep_rew_mean     | 271      |\n",
            "| time/              |          |\n",
            "|    fps             | 624      |\n",
            "|    iterations      | 57       |\n",
            "|    time_elapsed    | 1496     |\n",
            "|    total_timesteps | 933888   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=940000, episode_reward=263.68 +/- 20.47\n",
            "Episode length: 269.30 +/- 23.98\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 269          |\n",
            "|    mean_reward          | 264          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 940000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038637337 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.682       |\n",
            "|    explained_variance   | 0.988        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.65         |\n",
            "|    n_updates            | 532          |\n",
            "|    policy_gradient_loss | -0.000358    |\n",
            "|    value_loss           | 25.5         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=950000, episode_reward=263.98 +/- 23.79\n",
            "Episode length: 270.30 +/- 24.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 270      |\n",
            "|    mean_reward     | 264      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 950000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 254      |\n",
            "|    ep_rew_mean     | 272      |\n",
            "| time/              |          |\n",
            "|    fps             | 624      |\n",
            "|    iterations      | 58       |\n",
            "|    time_elapsed    | 1522     |\n",
            "|    total_timesteps | 950272   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=960000, episode_reward=264.54 +/- 22.51\n",
            "Episode length: 252.90 +/- 16.63\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 253         |\n",
            "|    mean_reward          | 265         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 960000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003769738 |\n",
            "|    clip_fraction        | 0.0372      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.719      |\n",
            "|    explained_variance   | 0.976       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.74        |\n",
            "|    n_updates            | 536         |\n",
            "|    policy_gradient_loss | -0.000131   |\n",
            "|    value_loss           | 48.4        |\n",
            "-----------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 265      |\n",
            "|    ep_rew_mean     | 270      |\n",
            "| time/              |          |\n",
            "|    fps             | 626      |\n",
            "|    iterations      | 59       |\n",
            "|    time_elapsed    | 1543     |\n",
            "|    total_timesteps | 966656   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=970000, episode_reward=283.99 +/- 15.24\n",
            "Episode length: 263.40 +/- 29.14\n",
            "-----------------------------------------\n",
            "| eval/                   |             |\n",
            "|    mean_ep_length       | 263         |\n",
            "|    mean_reward          | 284         |\n",
            "| time/                   |             |\n",
            "|    total timesteps      | 970000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003169898 |\n",
            "|    clip_fraction        | 0.0267      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.677      |\n",
            "|    explained_variance   | 0.957       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 38.6        |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.000451   |\n",
            "|    value_loss           | 99.6        |\n",
            "-----------------------------------------\n",
            "Eval num_timesteps=980000, episode_reward=272.79 +/- 21.51\n",
            "Episode length: 252.10 +/- 23.44\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 252      |\n",
            "|    mean_reward     | 273      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 980000   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 268      |\n",
            "|    ep_rew_mean     | 269      |\n",
            "| time/              |          |\n",
            "|    fps             | 626      |\n",
            "|    iterations      | 60       |\n",
            "|    time_elapsed    | 1569     |\n",
            "|    total_timesteps | 983040   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=990000, episode_reward=266.75 +/- 34.80\n",
            "Episode length: 336.40 +/- 221.86\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 336          |\n",
            "|    mean_reward          | 267          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 990000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036791698 |\n",
            "|    clip_fraction        | 0.0414       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.7         |\n",
            "|    explained_variance   | 0.968        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.15         |\n",
            "|    n_updates            | 544          |\n",
            "|    policy_gradient_loss | 0.000335     |\n",
            "|    value_loss           | 84.2         |\n",
            "------------------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 257      |\n",
            "|    ep_rew_mean     | 263      |\n",
            "| time/              |          |\n",
            "|    fps             | 627      |\n",
            "|    iterations      | 61       |\n",
            "|    time_elapsed    | 1592     |\n",
            "|    total_timesteps | 999424   |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1000000, episode_reward=280.21 +/- 24.11\n",
            "Episode length: 254.90 +/- 20.67\n",
            "------------------------------------------\n",
            "| eval/                   |              |\n",
            "|    mean_ep_length       | 255          |\n",
            "|    mean_reward          | 280          |\n",
            "| time/                   |              |\n",
            "|    total timesteps      | 1000000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034940687 |\n",
            "|    clip_fraction        | 0.0334       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.674       |\n",
            "|    explained_variance   | 0.952        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 54.8         |\n",
            "|    n_updates            | 548          |\n",
            "|    policy_gradient_loss | 0.000534     |\n",
            "|    value_loss           | 95.9         |\n",
            "------------------------------------------\n",
            "Eval num_timesteps=1010000, episode_reward=271.53 +/- 16.06\n",
            "Episode length: 272.90 +/- 25.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 273      |\n",
            "|    mean_reward     | 272      |\n",
            "| time/              |          |\n",
            "|    total timesteps | 1010000  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 247      |\n",
            "|    ep_rew_mean     | 266      |\n",
            "| time/              |          |\n",
            "|    fps             | 627      |\n",
            "|    iterations      | 62       |\n",
            "|    time_elapsed    | 1617     |\n",
            "|    total_timesteps | 1015808  |\n",
            "---------------------------------\n",
            "Saving to logs/ppo/LunarLander-v2_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx5rTbQaM4K2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}